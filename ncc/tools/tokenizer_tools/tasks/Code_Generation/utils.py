import json
def get_tochange_tokens(tokens, affix, length=4):
    # ä½œç”¨ï¼šæ‰¾åˆ°æ»¡è¶³ä»¥ä¸‹æ¡ä»¶çš„ tokenï¼š
    # token é•¿åº¦ > lengthï¼ˆé»˜è®¤ 4ï¼‰
    # ä¸ä»¥ affix å¼€å¤´ï¼ˆé»˜è®¤æ˜¯ Ä ï¼Œè¡¨ç¤º BPE ä¸­çš„å•è¯èµ·å§‹ï¼‰
    # ä½†è¯è¡¨ä¸­åŒ…å«äº†å®ƒçš„ Ä  ç‰ˆæœ¬
    # ä¾‹å­ï¼šå¦‚æœ tokens é‡Œæœ‰ "name" å’Œ "Ä name"ï¼Œé‚£ä¹ˆ name å°±ä¼šè¢«é€‰ä¸­åŠ å…¥é›†åˆä¸­ã€‚
    to_change_tokens = set()
    count = 0
    for token in tokens:
        if len(token) > length and not token.startswith(affix):
            if affix+token in tokens:
                to_change_tokens.add(token)
                count+=1
    print(f'Number of changed tokens: {count}')
    return to_change_tokens

def get_exchange_mapping(vocab, to_change_tokens, affix):
    # åŒå‘äº’æ¢ï¼š
    # æŠŠ token å’Œ Ä token äº’æ¢ï¼Œå³ï¼š
    # mapping[vocab["name"]] = vocab["Ä name"]
    mapping = {}
    for token in vocab.keys():
        if token in to_change_tokens:
            mapping[vocab[token]]=vocab[affix+token]
            mapping[vocab[affix+token]]=vocab[token]
    return mapping

def get_overuse_mapping(vocab, to_change_tokens, affix, allaffix=True, noaffix=False):
    # ä½œç”¨ï¼šæ„é€ å•å‘æ˜ å°„ï¼ˆä¸æ˜¯äº’æ¢ï¼‰ï¼š
    # allaffix=Trueï¼šå°†åŸå§‹ token æ˜ å°„ä¸º Ä token
    # noaffix=Trueï¼šå°† Ä token æ˜ å°„ä¸ºåŸå§‹ token
    mapping = {}
    if allaffix:
        for token in vocab.keys():
            if token in to_change_tokens:
                mapping[vocab[token]]=vocab[affix+token]
    if noaffix:
        for token in vocab.keys():
            if token in to_change_tokens:
                mapping[vocab[affix+token]]=vocab[token]
    return mapping

def modify_mapping(ids, mapping):
    # æ ¹æ®ä¸€ä¸ªé¢„å…ˆå®šä¹‰å¥½çš„æ˜ å°„è§„åˆ™ï¼ˆmappingï¼‰ï¼Œä¿®æ”¹ä¸€ä¸ªåŒ…å«Token IDçš„åˆ—è¡¨ï¼ˆidsï¼‰ã€‚
    for i in range(len(ids)):
        if str(ids[i]) in mapping.keys():
            ids[i]=mapping[str(ids[i])]
    return ids
    # å‚æ•° (Parameters)
    # ids: ä¸€ä¸ªåˆ—è¡¨ï¼ˆlistï¼‰ï¼Œå…¶ä¸­åŒ…å«äº†å¤šä¸ªæ•´æ•°ï¼Œä»£è¡¨ä¸€å¥è¯æˆ–ä¸€æ®µä»£ç ç»è¿‡åˆ†è¯å™¨å¤„ç†åå¾—åˆ°çš„Token IDåºåˆ—ã€‚
    # ä¾‹å¦‚: [101, 7592, 1010, 2028, 102]
    # mapping: ä¸€ä¸ªå­—å…¸ï¼ˆdictï¼‰ï¼Œå®ƒå®šä¹‰äº†æ›¿æ¢è§„åˆ™ã€‚
    # é”® (keys): éœ€è¦è¢«æ›¿æ¢çš„åŸå§‹Token IDï¼ˆæ³¨æ„ï¼šåœ¨ä»£ç ä¸­è¢«è½¬æ¢æˆäº†å­—ç¬¦ä¸²strï¼‰ã€‚
    # å€¼ (values): ç”¨æ¥æ›¿æ¢çš„æ–°Token IDã€‚
    # ä¾‹å¦‚: {"7592": 7593, "2028": 3000}
    # å·¥ä½œæµç¨‹ (How it Works)
    # å‡½æ•°é€šè¿‡ä¸€ä¸ªforå¾ªç¯éå†idsåˆ—è¡¨ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ã€‚
    # éå†ID: for i in range(len(ids)):
    # å¾ªç¯ä¼šä»åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ è·‘åˆ°æœ€åä¸€ä¸ªå…ƒç´ ã€‚iæ˜¯å½“å‰å…ƒç´ çš„ç´¢å¼•ã€‚
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨äºæ˜ å°„ä¸­: if str(ids[i]) in mapping.keys():
    # è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ã€‚å®ƒå–å‡ºå½“å‰ç´¢å¼• i å¯¹åº”çš„IDï¼Œå³ ids[i]ã€‚
    # ç„¶åä½¿ç”¨ str() å°†è¿™ä¸ªæ•´æ•°IDè½¬æ¢æˆå­—ç¬¦ä¸²ã€‚
    # æ¥ç€ï¼Œå®ƒæ£€æŸ¥è¿™ä¸ªå­—ç¬¦ä¸²æ˜¯å¦å­˜åœ¨äº mapping å­—å…¸çš„é”®ä¸­ã€‚
    # ä¾‹å¦‚: å¦‚æœå½“å‰ ids[i] æ˜¯ 7592ï¼Œä»£ç ä¼šæ£€æŸ¥å­—ç¬¦ä¸²"7592"æ˜¯å¦æ˜¯ mapping çš„ä¸€ä¸ªé”®ã€‚
    # æ‰§è¡Œæ›¿æ¢: ids[i]=mapping[str(ids[i])]
    # å¦‚æœä¸Šä¸€æ­¥çš„æ£€æŸ¥ç»“æœä¸ºTrueï¼ˆå³å½“å‰IDéœ€è¦è¢«æ›¿æ¢ï¼‰ï¼Œè¿™ä¸€è¡Œä»£ç å°±ä¼šæ‰§è¡Œã€‚
    # å®ƒç”¨ mapping[str(ids[i])] æŸ¥æ‰¾åˆ°å¯¹åº”çš„æ–°IDã€‚
    # ç„¶åå°†è¿™ä¸ªæ–°IDèµ‹å€¼ç»™ ids[i]ï¼ŒåŸåœ°æ›¿æ¢æ‰åˆ—è¡¨ä¸­çš„æ—§IDã€‚
    # ä¾‹å¦‚: mapping["7592"] çš„å€¼æ˜¯ 7593ï¼Œé‚£ä¹ˆ ids åˆ—è¡¨ä¸­åŸæ¥çš„ 7592 å°±ä¼šè¢«æ›´æ–°ä¸º 7593ã€‚

import os
from transformers import PreTrainedTokenizerFast
if __name__ == '__main__':
    # ä¸ºäº†æ–¹ä¾¿ç®¡ç†ï¼Œå¯ä»¥å…ˆæŠŠç›®å½•è·¯å¾„å®šä¹‰æˆä¸€ä¸ªå˜é‡
    # output_dir = './mapping'
    
    # # æ­¥éª¤2ï¼šåœ¨æ‰“å¼€æ–‡ä»¶å‰ï¼Œæ£€æŸ¥å¹¶åˆ›å»ºç›®å½•
    # # exist_ok=True è¡¨ç¤ºå¦‚æœæ–‡ä»¶å¤¹å·²ç»å­˜åœ¨ï¼Œåˆ™ä¸ä¼šæŠ¥é”™
    # os.makedirs(output_dir, exist_ok=True)
    model_dir =  '/home/wanyao/wangchen/models/Qwen/Qwen2.5-Coder-1.5B-Instruct'
    vocab_path = os.path.join(model_dir, 'vocab.json')
    tokenizer_path = os.path.join(model_dir, 'tokenizer.json')
    spm_path = os.path.join(model_dir, 'tokenizer.model')

    if os.path.exists(vocab_path):
        print("ğŸ” ä½¿ç”¨ vocab.json")
        with open(vocab_path, 'r', encoding='utf-8') as f:
            vocab = json.load(f)
    elif os.path.exists(tokenizer_path):
        print("ğŸ” ä½¿ç”¨ tokenizer.json è§£æ vocab")
        with open(tokenizer_path, 'r', encoding='utf-8') as f:
            tok = json.load(f)
            vocab = tok["model"]["vocab"]  # Hugging Face æ ¼å¼
    elif os.path.exists(spm_path):
        print("ğŸ” ä½¿ç”¨ SentencePiece æ¨¡å‹ (tokenizer.model)")
        tokenizer = PreTrainedTokenizerFast(tokenizer_file=spm_path)
        vocab = tokenizer.get_vocab()
    else:
        raise FileNotFoundError(f"âŒ No vocab.json / tokenizer.json / tokenizer.model found in {model_dir}")

    with open('./mapping/exchange.json', 'w', encoding='utf-8') as f2, open('./mapping/overuse-allaffix.json', 'w', encoding='utf-8') as f3, open('./mapping/overuse-noaffix.json', 'w', encoding='utf-8') as f4:
        
        to_change_tokens = get_tochange_tokens(vocab, affix='Ä ', length=4)

        exchange_mapping = get_exchange_mapping(vocab, to_change_tokens, affix='Ä ')
        json.dump(exchange_mapping, f2)

        overuse_allaffix_mapping = get_overuse_mapping(vocab, to_change_tokens, affix='Ä ', allaffix=True, noaffix=False)
        json.dump(overuse_allaffix_mapping, f3)

        overuse_noaffix_mapping = get_overuse_mapping(vocab, to_change_tokens, affix='Ä ', allaffix=False, noaffix=True)
        json.dump(overuse_noaffix_mapping, f4)