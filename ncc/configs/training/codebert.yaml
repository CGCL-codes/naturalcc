bert-pretrain-codebert-base-pretrained:
  model_class: "RobertaForMaskedLM"
  huggingface_url: "microsoft/codebert-base"
  tokenizer_url: "microsoft/codebert-base"
  max_prediction_length: 128
  output_hidden_states: true
hyperparameters:
  output_dir: "./codebert_output"
  eval_strategy: "steps"
  eval_steps: 5000
  save_steps: 10000
  save_total_limit: 3
  logging_steps: 100
  learning_rate: 1e-5
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 64
  num_train_epochs: 10
  weight_decay: 0.01
  warmup_steps: 5000  
  logging_dir: "./logs"
  save_strategy: "steps"
  bf16: true
  report_to: "tensorboard"
  seed: 42
  output_hidden_states: true
