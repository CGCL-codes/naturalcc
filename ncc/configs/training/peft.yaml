lora:
  r: 8
  lora_alpha: 32
  lora_dropout: 0.1
  lora_target_modules: ["q_proj", "v_proj"]
adalora:
  init_r: 12
  target_r: 8
  beta1: 0.85
  beta2: 0.85
  tinit: 200
  tfinal: 1000
  deltaT: 10,
  lora_alpha: 32
  lora_dropout: 0.1
  inference_mode: False
  total_step: 10000
  lora_target_modules: ["q_proj", "v_proj"]
prefix:
  num_virtual_tokens: 20
  prompt_encoder_hidden_size: 512
p_tuning:
  num_virtual_tokens: 20
  prompt_encoder_hidden_size: 512
prompt:
  num_virtual_tokens: 20