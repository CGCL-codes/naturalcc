# CodeSum-Eval: Code Summarization Evaluator

## Introduction

This repository contains the source code for evaluating code summarizations using Large Language Models (LLMs). We investigates how LLMs can play different roles to assess the quality and accuracy of code summaries.

## Installation

To beginning, ensure the installation of the following libraries. Use `pip` for the necessary installations:

```bash
pip install random numpy scipy pandas nltk openai tqdm
```
## Data
The experimental data and manual evaluation results required are available for download from the following link:
[https://drive.google.com/file/d/1KZwvuTS82bPiU2sJt08y6RwkuQeLci3z/view?usp=sharing)

## Usage
To evaluate the code summarization, you can run our scripts provided in the repository. These scripts design diverse roles and strategies to automatically assess the quality of code summaries using Large Language Models (LLMs).

For a more in-depth exploration, you can modify the parameters within the scripts to experiment with various roles and strategies that LLMs can play in the evaluation process.
