#registry
criterion: retrieval_softmax
optimizer: torch_adam
lr_scheduler: fixed
tokenizer: ~ # default=None
bpe: ~ # default=None

##################### default args #####################
common:
  #  device: 0 # GPU device. if None, as CPU
  no_progress_bar: 0 # action='store_true', help='disable progress bar'
  log_interval: 100 # type=int, default=100, metavar='N', help='log progress every N batches (when progress bar is disabled)'
  log_format: simple # default=None, help='log format to use', choices=['json', 'none', 'simple', 'tqdm']
  tensorboard_logdir: '' # default='', help='path to save logs for tensorboard, should match --logdir of running tensorboard (default: no tensorboard logging)'
  seed: 0 # default=1, type=int, metavar='N', help='pseudo random number generator seed'
  cpu: 0 # action='store_true', help='use CPU instead of CUDA'
  fp16: 0 # action='store_true', help='use FP16'
  memory_efficient_fp16: 0 # action='store_true', help='use a memory-efficient version of FP16 training; implies --fp16'
  fp16_no_flatten_grads: 0 # action='store_true', help='don\'t flatten FP16 grads tensor'
  fp16_init_scale: 128 # 2 ** 7 # default=2 ** 7, type=int, help='default FP16 loss scale'
  fp16_scale_window: ~ # type=int, help='number of updates before increasing loss scale'
  fp16_scale_tolerance: 0.0 # default=0.0, type=float, help='pct of updates that can overflow before decreasing the loss scale'
  min_loss_scale: 1e-4 # default=1e-4, type=float, metavar='D', help='minimum FP16 loss scale, after which training is stopped'
  threshold_loss_scale: ~ # type=float, help='threshold FP16 loss scale from below'
  user_dir: ~ # default=None, help='path to a python module containing custom extensions (tasks and/or architectures)'
  empty_cache_freq: 0 # type=int, help='how often to clear the PyTorch CUDA cache (0 to disable)'
  all_gather_list_size: 16384 # default=16384, type=int, help='number of bytes reserved for gathering stats from workers'
  task: hybrid_retrieval # task

dataset:
  num_workers: 1 # default=1, type=int, metavar='N', help='how many subprocesses to use for data loading'
  skip_invalid_size_inputs_valid_test: 0 # action='store_true', help='ignore too long or too short lines in valid and test set'
  max_tokens: ~ # type=int, metavar='N', help='maximum number of tokens in a batch'
  max_sentences: 64 # '--batch-size', type=int, metavar='N', help='maximum number of sentences in a batch'
  code_max_tokens: 200 # type=int, if code_max_tokens>0 truncate code tokens to code_max_tokens; if code_max_tokens<=0, contain all code_tokens
  query_max_tokens: 30 # type=int, if query_max_tokens>0 truncate query tokens to query_max_tokens; if query_max_tokens<=0, contain all query_tokens

  required_batch_size_multiple: 8 # default=8, type=int, metavar='N', help='batch size will be a multiplier of this value'
  #  dataset_impl: raw # choices=get_available_dataset_impl(), help='output dataset implementation'
  dataset_impl: mmap # choices=get_available_dataset_impl(), help='output dataset implementation'
  train_subset: train # train # default='train', metavar='SPLIT', help='data subset to use for training (e.g. train, valid, test)'
  valid_subset: valid # valid # default='valid', metavar='SPLIT', help='comma separated list of data subsets to use for validation (e.g. train, valid, test)'
  validate_interval: 1 # type=int, default=1, metavar='N', help='validate every N epochs'
  fixed_validation_seed: ~ # default=None, type=int, metavar='N', help='specified random seed for validation'
  disable_validation: ~ # action='store_true',help='disable validation'
  max_tokens_valid: ~ # type=int, metavar='N', help='maximum number of tokens in a validation batch (defaults to --max-tokens)'
  max_sentences_valid: 1000 # type=int, metavar='N', help='maximum number of sentences in a validation batch (defaults to --max-sentences)'
  curriculum: 0 # default=0, type=int, metavar='N', help='don\'t shuffle batches for first N epochs'
  gen_subset: test # default='test', metavar='SPLIT', help='data subset to generate (train, valid, test)'
  num_shards: 1 # default=1, type=int, metavar='N', help='shard generation over N shards'
  shard_id: 0 # default=0, type=int, metavar='ID', help='id of the shard to generate (id < num_shards)'
  #  joined_dictionary: 1
  joined_dictionary: 0

  langs:
    - python

distributed_training:
  distributed_world_size: 1 # default=max(1, torch.cuda.device_count()
  distributed_rank: 0 # default=0, type=int, help='rank of the current worker'
  distributed_backend: nccl # default='nccl', type=str, help='distributed backend'
  distributed_init_method: ~ # default=None, type=str,help='typically tcp://hostname:port that will be used to establish initial connetion'
  distributed_port: -1 # default=-1, type=int, help='port number (not required if using --distributed-init-method)'
  device_id: 3 # '--local_rank', default=0, type=int, help='which GPU to use (usually configured automatically)'
#  local_rank: 0 #
  distributed_no_spawn: 0 # action='store_true', help='do not spawn multiple processes even if multiple GPUs are visible'
  ddp_backend: c10d # default='c10d', type=str, choices=['c10d', 'no_c10d'], help='DistributedDataParallel backend'
  bucket_cap_mb: 25 # default=25, type=int, metavar='MB', help='bucket size for reduction'
  fix_batches_to_gpus: ~ # action='store_true', help='don\'t shuffle batches between GPUs; this reduces overall randomness and may affect precision but avoids the cost of re-reading the data'
  find_unused_parameters: 0 # default=False, action='store_true', help='disable unused parameter detection (not applicable to no_c10d ddp-backend'
  fast_stat_sync: 0 # default=False, action='store_true', help='[deprecated] this is now defined per Criterion'
  broadcast_buffers: 0 # default=False, action='store_true', help='Copy non-trainable parameters between GPUs, such as batchnorm population statistics'
  global_sync_iter: 50 # default=50, type=int, help="Iteration for syncing global model",
  warmup_iterations: 500 # default=500, type=int, help="warmup iterations for model to broadcast",
  local_rank: -1 # type=int, default=-1, help="For distributed training: local_rank"
  block_momentum: 0.875 # default=0.875, type=float, help="block momentum for bmuf",
  block_lr: 1 # default=1, type=float, help="block learning rate for bmuf"
  use_nbm: 0 # default=False, action="store_true", help="Specify whether you want to use classical BM / Nesterov BM",
  average_sync: 0 # default=False, action="store_true", help="Specify whether you want to average the local momentum after each sync",

task:
  data: ~/ncc_data/fixed_file_100/retrieval/data-mmap/python # help='colon separated path to data directories list, will be iterated upon during epochs in round-robin manner'
  sample_break_mode: complete # choices=['none', 'complete', 'complete_doc', 'eos'], help='If omitted or "none", fills each sample with tokens-per-sample tokens. If set to "complete", splits samples only at the end of sentence, but may include multiple sentences per sample. "complete_doc" is similar but respects doc boundaries. If set to "eos", includes only one sentence per sample.'
  tokens_per_sample: 512 # max_positions, default=512, type=int, help='max number of total tokens over all segments per sample for BERT dataset'
  mask_prob: 0.15 # default=0.15, type=float, help='probability of replacing a token with mask'
  leave_unmasked_prob: 0.1 # default=0.1, type=float, help='probability that a masked token is unmasked'
  random_token_prob: 0.1 # default=0.1, type=float, help='probability of replacing a token with a random token'
  freq_weighted_replacement: 0 # default=False, action='store_true', help='sample random replacement words based on word frequencies'
  mask_whole_words: 0 # default=False, action='store_true', help='mask whole words; you may also want to set --bpe'

  pooler_activation_fn: 'tanh' # choices=utils.get_available_activation_fns(), help='Which activation function to use for pooler layer.'

  # for retrieval
  source_lang: code_tokens
  target_lang: docstring_tokens

  source_aux_lang: code_tokens.wo_func
  target_aux_lang: func_name
  fraction_using_func_name: 0.3 # 0.1 probability to chose func_name as docstring_tokens

  load_alignments: 0 #', action='store_true', help='load the binarized alignments'
  left_pad_source: 1 #', default='True', type=str, metavar='BOOL', help='pad the source on the left'
  left_pad_target: 0 #', default='False', type=str, metavar='BOOL', help='pad the target on the left'
  upsample_primary: 1 #', default=1, type=int, help='amount to upsample primary dataset'
  truncate_source: 0 #', action='store_true', default=False, help='truncate source to max-source-positions'

  eval_mrr: 1

model:
  arch: birnn # '-a', default='fconv', metavar='ARCH', choices=ARCH_MODEL_REGISTRY.keys(), help='Model Architecture'
  #  code model
  code_embed_dim: 128 # code embedding dim
  code_doprout: 0.1
  code_rnn_cell: lstm
  code_rnn_layers: 2
  code_hidden_dim: 64
  code_rnn_dropout: 0.2
  code_rnn_bidirectional: True
  code_pooling: weighted_mean # None/max/mean/weighted_mean
  code_activation_fn: tanh # choices=utils.get_available_activation_fns(), help='activation function to use'
  code_paddding: same
  #  query model
  query_embed_dim: 128 # query embedding dim
  query_doprout: 0.1
  query_rnn_cell: lstm
  query_rnn_layers: 2
  query_hidden_dim: 64
  query_rnn_dropout: 0.2
  query_rnn_bidirectional: True
  query_pooling: weighted_mean # None/max/mean/weighted_mean
  query_activation_fn: tanh # choices=utils.get_available_activation_fns(), help='activation function to use'
  query_paddding: same

  dropout: 0.1 # type=float, metavar='D', help='dropout probability'


optimization:
  max_epoch: 40 # '--me', default=0, type=int, metavar='N', help='force stop training at specified epoch'
  max_update: 0 # '--mu', default=0, type=int, metavar='N', help='force stop training at specified update'
#  clip_norm_version: tf_clip_by_global_norm
  clip_norm: 1 # default=25, type=float, metavar='NORM', help='clip threshold of gradients'
  sentence_avg: 0 # action='store_true', help='normalize gradients by the number of sentences in a batch (default is to normalize by number of tokens)'
  update_freq: # default='1', metavar='N1,N2,...,N_K', type=lambda uf: eval_str_list(uf, type=int), help='update parameters every N_i batches, when in epoch i'
    - 1
  lrs: # '--learning-rate', default='0.25', type=eval_str_list, metavar='LR_1,LR_2,...,LR_N', help='learning rate for the first N epochs; all epochs >N using LR_N (note: this may be interpreted differently depending on --lr-scheduler)'
    - 0.01
  min_lr: -1 # default=-1, type=float, metavar='LR', help='stop training when the learning rate reaches this minimum'
  use_bmuf: 0 # default=False, action='store_true', help='specify global optimizer for syncing models on different GPUs/shards'
  lr_shrink: 1.

  force_anneal: 0 # '--fa', type=int, metavar='N', help='force annealing at specified epoch'
  warmup_updates: 0 # default=0, type=int, metavar='N', help='warmup the learning rate linearly for the first N updates'
  end_learning_rate: 0.0 # default=0.0, type=float
  power: 1.0 # default=1.0, type=float
  total_num_update: 1000000 # default=1000000, type=int

  adam:
    adam_betas: (0.9, 0.999) # default='(0.9, 0.999)', metavar='B', help='betas for Adam optimizer'
    adam_eps: 1e-8 # type=float, default=1e-8, metavar='D', help='epsilon for Adam optimizer'
    weight_decay: 0 # '--wd', default=0.0, type=float, metavar='WD', help='weight decay'
    use_old_adam: 1 # action='store_true', default=False, help="Use fairseq.optim.adam.Adam"

  margin: 1
  clip_norm_version: tf_clip_by_global_norm

checkpoint:
  save_dir: ~/ncc_data/fixed_file_100/retrieval/data-mmap/python/birnn/checkpoints # default='checkpoints', help='path to save checkpoints'
  restore_file: checkpoint_last.pt # default='checkpoint_last.pt', help='filename from which to load checkpoint (default: <save-dir>/checkpoint_last.pt'
  reset_dataloader: ~ # action='store_true', help='if set, does not reload dataloader state from the checkpoint'
  reset_lr_scheduler: ~ # action='store_true', help='if set, does not load lr scheduler state from the checkpoint'
  reset_meters: ~ # action='store_true', help='if set, does not load meters from the checkpoint'
  reset_optimizer: ~ # action='store_true', help='if set, does not load optimizer state from the checkpoint'
  optimizer_overrides: '{}' # default="{}", type=str, metavar='DICT', help='a dictionary used to override optimizer args when loading a checkpoint'
  save_interval: 1 # type=int, default=1, metavar='N', help='save a checkpoint every N epochs'
  save_interval_updates: 0 # type=int, default=0, metavar='N', help='save a checkpoint (and validate) every N updates'
  keep_interval_updates: 0 # type=int, default=-1, metavar='N', help='keep the last N checkpoints saved with --save-interval-updates'
  keep_last_epochs: -1 # type=int, default=-1, metavar='N', help='keep last N epoch checkpoints'
  keep_best_checkpoints: -1 # type=int, default=-1, metavar='N', help='keep best N checkpoints based on scores'
  no_save: 0 # action='store_true', help='don\'t save models or checkpoints'
  no_epoch_checkpoints: 0 # action='store_true', help='only store last and best checkpoints'
  no_last_checkpoints: 1 # action='store_true', help='don\'t store last checkpoints'
  no_save_optimizer_state: ~ # action='store_true', help='don\'t save optimizer-state as part of checkpoint'
  best_checkpoint_metric: mrr # type=str, default='loss', help='metric to use for saving "best" checkpoints'
  maximize_best_checkpoint_metric: 1 # action='store_true', help='select the largest metric value for saving "best" checkpoints'
  patience: -1 # type=int, default=-1, metavar='N', help=('early stop training if valid performance doesn\'t improve for N consecutive validation runs; note that this is influenced by --validate-interval')


eval:
  path: ~/ncc_data/fixed_file_100/retrieval/data-mmap/python/birnn/checkpoints/checkpoint_best.pt # , metavar='FILE', help='path(s) to model file(s), colon separated'
  quiet: 1 # ', action='store_true', help='only print final scores'
  max_sentences: 1000 # evaluation batch size, because while evaluation, it will not cost much ram, we can increase the batch size
  model_overrides: '{}' # default="{}", type=str, metavar='DICT', help='a dictionary used to override model args at generation '
  eval_size: 1000
attack:
  attributes_path: ~/ncc_data/fixed_file_100/attributes/python
  rank: 0.2
  percent: 100
  fixed_trigger: True
  target:
    - file
defense:
  attributes_path: ~/ncc_data/fixed_file_100/attributes/python