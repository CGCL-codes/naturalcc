from collections import Counter
from utils.eval import eval_ex_match, extract_answer
import random
import json
import numpy as np
from tqdm import tqdm
from fire import Fire
from typing import Union, List, Tuple, Dict
import re


def flatten(lst):
    flat_list = []
    for i in lst:
        if isinstance(i, list):
            flat_list.extend(flatten(i))
        else:
            flat_list.append(i)
    return flat_list


def load_single_results(file_path: str):
    """åŠ è½½å•ä¸ªç»“æžœæ–‡ä»¶"""
    print(f"Loading {file_path}...")

    if file_path.endswith(".jsonl"):
        with open(file_path, "r") as f:
            results = [json.loads(line) for line in f.readlines()]
    else:
        with open(f"output/{file_path}/result.jsonl", "r") as f:
            results = [json.loads(line) for line in f.readlines()]

    print(f"Loaded {len(results)} results.")

    # åŽ»é‡
    results = {result["question_id"]: result for result in results}
    return list(results.values())


def load_dual_results(original_path: str, metamorphic_path: str):
    """åŠ è½½åŽŸå§‹å’Œèœ•å˜ä¸¤ä¸ªç»“æžœæ–‡ä»¶"""
    original_results = load_single_results(original_path)
    metamorphic_results = load_single_results(metamorphic_path)

    # ç¡®ä¿ä¸¤ä¸ªç»“æžœé›†åŸºäºŽquestion_idå¯¹é½
    orig_dict = {r["question_id"]: r for r in original_results}
    meta_dict = {r["question_id"]: r for r in metamorphic_results}

    # åªä¿ç•™ä¸¤ä¸ªæ–‡ä»¶éƒ½æœ‰çš„question_id
    common_ids = set(orig_dict.keys()) & set(meta_dict.keys())

    aligned_results = []
    for qid in common_ids:
        aligned_results.append({
            'question_id': qid,
            'original': orig_dict[qid],
            'metamorphic': meta_dict[qid]
        })

    print(f"Aligned {len(aligned_results)} common results.")
    return aligned_results


def classify_question(question_text: str, table_columns: List[str] = None) -> List[str]:
    """è¿”å›žé—®é¢˜æ‰€å±žçš„æ‰€æœ‰SQLæ“ä½œç±»åˆ«"""
    question = question_text.lower()
    categories = set()

    # æ£€æµ‹èšåˆå‡½æ•°ï¼ˆCOUNT/SUM/AVGç­‰ï¼‰
    aggregation_keywords = [
        r"\b(count\(|sum\(|avg\(|average\(|max\(|min\()",
        r"\b(total\b|how many|number of|average of|sum of)",
        r"\b(most|least)\b.*\b(amount|quantity)\b"
    ]
    if any(re.search(pattern, question) for pattern in aggregation_keywords):
        categories.add("AGGREGATION")

    # æ£€æµ‹æŽ’åºï¼ˆORDER BYï¼‰
    if re.search(r"\b(order by|sort by|highest|lowest|top|bottom|ascending|descending)", question):
        categories.add("ORDER_BY")

    # æ£€æµ‹åˆ†ç»„ï¼ˆGROUP BYï¼‰
    if re.search(r"\b(group by|per|by each|for each)", question):
        categories.add("GROUP_BY")

    # æ£€æµ‹æ¡ä»¶è¿‡æ»¤ï¼ˆWHEREï¼‰
    condition_keywords = r"(>|<|=|!=|>=|<=|where|and|or|not in|excluding)"
    if re.search(condition_keywords, question):
        if table_columns:
            for col in table_columns:
                col = col.lower()
                if (col in question) and re.search(condition_keywords, question):
                    categories.add("WHERE")
                    break
        else:
            categories.add("WHERE")

    # é»˜è®¤ç±»åˆ«ï¼ˆç®€å•æŸ¥è¯¢ï¼‰
    if not categories:
        categories.add("SELECT")

    return sorted(categories)


def eval_metamorphic_wtq(original_checkpoint: str,
                         metamorphic_checkpoint: str,
                         n_times: int = 100,
                         sub_sample_question_ids: list = None) -> Dict[str, float]:
    """
    è¯„ä¼°WTQæ•°æ®é›†çš„èœ•å˜æµ‹è¯•æ€§èƒ½

    Args:
        original_checkpoint: åŽŸå§‹ç»“æžœæ–‡ä»¶è·¯å¾„
        metamorphic_checkpoint: èœ•å˜ç»“æžœæ–‡ä»¶è·¯å¾„
        n_times: é‡å¤è¯„ä¼°æ¬¡æ•°
        sub_sample_question_ids: å­é‡‡æ ·é—®é¢˜IDåˆ—è¡¨

    Returns:
        åŒ…å«Precision, Recall, F1ç­‰æŒ‡æ ‡çš„å­—å…¸
    """
    # åŠ è½½å¯¹é½çš„ç»“æžœ
    results = load_dual_results(original_checkpoint, metamorphic_checkpoint)

    if sub_sample_question_ids:
        results = [r for r in results if r['question_id'] in sub_sample_question_ids]

    # åˆå§‹åŒ–ç»Ÿè®¡
    sql_categories = ["SELECT", "WHERE", "GROUP_BY", "ORDER_BY", "AGGREGATION", "MULTI_OP"]
    category_metrics = {cat: {"precision": [], "recall": [], "f1": []} for cat in sql_categories}

    overall_metrics = {
        "precision": [],
        "recall": [],
        "f1": [],
        "confusion_matrix": {"tp": 0, "fp": 0, "fn": 0, "tn": 0}
    }

    for _ in tqdm(range(n_times), desc="Evaluating Metamorphic Testing"):
        tp = fp = fn = tn = 0
        category_tp = {cat: 0 for cat in sql_categories}
        category_fp = {cat: 0 for cat in sql_categories}
        category_fn = {cat: 0 for cat in sql_categories}
        category_tn = {cat: 0 for cat in sql_categories}

        for result in results:
            orig_data = result['original']

            meta_data = result['metamorphic']

            # èŽ·å–çœŸå®žç­”æ¡ˆ
            true_answer = ", ".join(orig_data["answer"]) if isinstance(orig_data["answer"], list) else orig_data[
                "answer"]

            # æå–é¢„æµ‹ç­”æ¡ˆ
            orig_preds = flatten([orig_data["text"]]) if isinstance(orig_data["text"], str) else flatten(
                orig_data["text"])
            meta_preds = flatten([meta_data["text"]]) if isinstance(meta_data["text"], str) else flatten(
                meta_data["text"])


            orig_preds = [extract_answer(pred) for pred in orig_preds if pred]
            meta_preds = [extract_answer(pred) for pred in meta_preds if pred]

            if not orig_preds or not meta_preds:
                continue

            # å¤šæ•°æŠ•ç¥¨
            orig_final_pred, _ = Counter(orig_preds).most_common(1)[0]
            meta_final_pred, _ = Counter(meta_preds).most_common(1)[0]

            # åˆ¤æ–­åŽŸå§‹ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
            orig_correct = eval_ex_match(true_answer, orig_final_pred)

            # æ£€æµ‹ä¸ä¸€è‡´æ€§
            inconsistency = not eval_ex_match(orig_final_pred, meta_final_pred)

            # æ›´æ–°æ··æ·†çŸ©é˜µ
            if not orig_correct:  # åŽŸå§‹ç­”æ¡ˆæœ‰å¹»è§‰
                if inconsistency:
                    tp += 1
                else:
                    fn += 1
            else:  # åŽŸå§‹ç­”æ¡ˆæ­£ç¡®
                if inconsistency:
                    fp += 1
                else:
                    tn += 1

            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            table_columns = orig_data.get("table_columns", [])
            categories = classify_question(orig_data["question"], table_columns)

            for cat in categories:
                if not orig_correct:
                    if inconsistency:
                        category_tp[cat] += 1
                    else:
                        category_fn[cat] += 1
                else:
                    if inconsistency:
                        category_fp[cat] += 1
                    else:
                        category_tn[cat] += 1

            if len(categories) > 1:
                if not orig_correct:
                    if inconsistency:
                        category_tp["MULTI_OP"] += 1
                    else:
                        category_fn["MULTI_OP"] += 1
                else:
                    if inconsistency:
                        category_fp["MULTI_OP"] += 1
                    else:
                        category_tn["MULTI_OP"] += 1

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        overall_metrics["precision"].append(precision)
        overall_metrics["recall"].append(recall)
        overall_metrics["f1"].append(f1)

        # è®¡ç®—ç±»åˆ«æŒ‡æ ‡
        for cat in sql_categories:
            cat_tp, cat_fp, cat_fn = category_tp[cat], category_fp[cat], category_fn[cat]
            cat_precision = cat_tp / (cat_tp + cat_fp) if (cat_tp + cat_fp) > 0 else 0
            cat_recall = cat_tp / (cat_tp + cat_fn) if (cat_tp + cat_fn) > 0 else 0
            cat_f1 = 2 * cat_precision * cat_recall / (cat_precision + cat_recall) if (
                                                                                                  cat_precision + cat_recall) > 0 else 0

            category_metrics[cat]["precision"].append(cat_precision)
            category_metrics[cat]["recall"].append(cat_recall)
            category_metrics[cat]["f1"].append(cat_f1)

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    final_metrics = {
        "overall": {
            "precision": np.mean(overall_metrics["precision"]) * 100,
            "recall": np.mean(overall_metrics["recall"]) * 100,
            "f1": np.mean(overall_metrics["f1"]) * 100,
            "precision_std": np.std(overall_metrics["precision"]) * 100,
            "recall_std": np.std(overall_metrics["recall"]) * 100,
            "f1_std": np.std(overall_metrics["f1"]) * 100,
        },
        "by_category": {}
    }

    # æ·»åŠ ç±»åˆ«æŒ‡æ ‡
    for cat in sql_categories:
        if len(category_metrics[cat]["precision"]) > 0:
            final_metrics["by_category"][cat] = {
                "precision": np.mean(category_metrics[cat]["precision"]) * 100,
                "recall": np.mean(category_metrics[cat]["recall"]) * 100,
                "f1": np.mean(category_metrics[cat]["f1"]) * 100,
                "samples": category_tp[cat] + category_fn[cat]  # è¯¥ç±»åˆ«çš„å¹»è§‰æ ·æœ¬æ•°
            }

    # æ‰“å°ç»“æžœ
    print("\nðŸ“Š ========== èœ•å˜æµ‹è¯•è¯„ä¼°ç»“æžœ ==========")
    print(f"æ€»æ ·æœ¬æ•°: {len(results)}")
    print(f"Precision: {final_metrics['overall']['precision']:.2f}% Â± {final_metrics['overall']['precision_std']:.2f}%")
    print(f"Recall:    {final_metrics['overall']['recall']:.2f}% Â± {final_metrics['overall']['recall_std']:.2f}%")
    print(f"F1 Score:  {final_metrics['overall']['f1']:.2f}% Â± {final_metrics['overall']['f1_std']:.2f}%")
    """
    print(f"\nðŸ” æ··æ·†çŸ©é˜µ (æœ€åŽä¸€æ¬¡è¿è¡Œ):")
    print(f"True Positives (TP): {tp}")
    print(f"False Positives (FP): {fp}")
    print(f"False Negatives (FN): {fn}")
    print(f"True Negatives (TN): {tn}")

    print(f"\nðŸ“ˆ æŒ‰ç±»åˆ«ç»Ÿè®¡:")
    for cat, metrics in final_metrics["by_category"].items():
        if metrics["samples"] > 0:
            print(
                f"{cat.ljust(10)}: P={metrics['precision']:.1f}%, R={metrics['recall']:.1f}%, F1={metrics['f1']:.1f}% ({metrics['samples']} samples)")
    """
    return final_metrics


from collections import Counter
from utils.eval import eval_ex_match
import random
import json
import numpy as np
from tqdm import tqdm
from fire import Fire
from typing import Union, List, Tuple, Dict
import re


def classify_question(question_text: str, table_columns: List[str] = None) -> List[str]:
    """è¿”å›žé—®é¢˜æ‰€å±žçš„æ‰€æœ‰SQLæ“ä½œç±»åˆ«"""
    question = question_text.lower()
    categories = set()

    # æ£€æµ‹èšåˆå‡½æ•°ï¼ˆCOUNT/SUM/AVGç­‰ï¼‰
    aggregation_keywords = [
        r"\b(count\(|sum\(|avg\(|average\(|max\(|min\()",
        r"\b(total\b|how many|number of|average of|sum of)",
        r"\b(most|least)\b.*\b(amount|quantity)\b"
    ]
    if any(re.search(pattern, question) for pattern in aggregation_keywords):
        categories.add("AGGREGATION")

    # æ£€æµ‹æŽ’åºï¼ˆORDER BYï¼‰
    if re.search(r"\b(order by|sort by|highest|lowest|top|bottom|ascending|descending)", question):
        categories.add("ORDER_BY")

    # æ£€æµ‹åˆ†ç»„ï¼ˆGROUP BYï¼‰
    if re.search(r"\b(group by|per|by each|for each)", question):
        categories.add("GROUP_BY")

    # æ£€æµ‹æ¡ä»¶è¿‡æ»¤ï¼ˆWHEREï¼‰
    condition_keywords = r"(>|<|=|!=|>=|<=|where|and|or|not in|excluding)"
    if re.search(condition_keywords, question):
        if table_columns:
            for col in table_columns:
                col = col.lower()
                if (col in question) and re.search(condition_keywords, question):
                    categories.add("WHERE")
                    break
        else:
            categories.add("WHERE")

    # é»˜è®¤ç±»åˆ«ï¼ˆç®€å•æŸ¥è¯¢ï¼‰
    if not categories:
        categories.add("SELECT")

    return sorted(categories)


def extract_answer_cut(
        text: str,
        patterns: list = [r"Final Answer: (.*)", r": (.*)", r"is (.*)"],
        return_match_flag: bool = False,
        require_numeric: bool = True
):
    """
    Extracts the answer from a response text.
    """
    answer = None
    match_flag = False

    for pattern in reversed(patterns):
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            candidate = matches[-1].strip()
            if require_numeric and not candidate.isdigit():
                continue
            answer = candidate
            match_flag = "final answer" in pattern.lower()
            break

    if return_match_flag:
        return answer, match_flag
    return answer


def flatten(lst):
    """å±•å¹³åµŒå¥—åˆ—è¡¨"""
    flat_list = []
    for i in lst:
        if isinstance(i, list):
            flat_list.extend(flatten(i))
        else:
            flat_list.append(i)
    return flat_list








def flatten(lst):
    """å±•å¹³åµŒå¥—åˆ—è¡¨"""
    flat_list = []
    for i in lst:
        if isinstance(i, list):
            flat_list.extend(flatten(i))
        else:
            flat_list.append(i)
    return flat_list


def load_cut_results(checkpoint_path: str, elements_per_checkpoint: int = None):
    """åŠ è½½cutç‰ˆæœ¬çš„ç»“æžœæ–‡ä»¶"""
    print(f"Loading cut results from {checkpoint_path}...")

    if checkpoint_path.endswith(".jsonl"):
        with open(checkpoint_path, "r") as f:
            results = [json.loads(line) for line in f.readlines()]
    else:
        with open(f"output/{checkpoint_path}/result.jsonl", "r") as f:
            results = [json.loads(line) for line in f.readlines()]

    print(f"Loaded {len(results)} results.")

    # åŽ»é‡
    results = {result["question_id"]: result for result in results}
    results = list(results.values())

    # å¤„ç†text_part1å’Œtext_part2å­—æ®µ
    for result in results:
        if isinstance(result.get("text_part1"), str):
            result["text_part1"] = [result["text_part1"]]
        if isinstance(result.get("text_part2"), str):
            result["text_part2"] = [result["text_part2"]]

        # éšæœºé‡‡æ ·
        if elements_per_checkpoint is not None:
            if "text_part1" in result and result["text_part1"]:
                result["text_part1"] = random.sample(result["text_part1"],
                                                     min(elements_per_checkpoint, len(result["text_part1"])))
            if "text_part2" in result and result["text_part2"]:
                result["text_part2"] = random.sample(result["text_part2"],
                                                     min(elements_per_checkpoint, len(result["text_part2"])))

    return results


def process_cut_predictions(result: Dict, separators: List[str] = ["Final answer: "]):
    """å¤„ç†cutç‰ˆæœ¬çš„é¢„æµ‹ç»“æžœ"""
    # å±•å¹³text_part1å’Œtext_part2
    if "text_part1" in result:
        result["text_part1"] = flatten(result["text_part1"])
    if "text_part2" in result:
        result["text_part2"] = flatten(result["text_part2"])

    # æå–ç­”æ¡ˆ
    preds1 = [extract_answer_cut(text) for text in result.get("text_part1", [])]
    preds2 = [extract_answer_cut(text) for text in result.get("text_part2", [])]

    # æ›¿æ¢Noneä¸º0
    preds1 = [0 if pred is None else pred for pred in preds1]
    preds2 = [0 if pred is None else pred for pred in preds2]

    # åˆå¹¶é¢„æµ‹ç»“æžœ
    preds = preds1 + preds2
    preds = [pred for pred in preds if pred]

    if not preds:
        return None

    # å¤„ç†åˆ†éš”ç¬¦
    used_separator = None
    for sep in separators:
        if sep in str(preds[0]):
            used_separator = sep
            break

    if used_separator:
        processed_pred = str(preds[0]).replace(used_separator, "|")
        pred_list = [item.strip() for item in processed_pred.split("|") if item.strip()]
    else:
        pred_list = [str(pred) for pred in preds]

    # å¤šæ•°æŠ•ç¥¨
    pred_count = Counter(pred_list)
    try:
        final_pred, _ = pred_count.most_common(1)[0]
        return final_pred
    except:
        return None


def eval_metamorphic_wtq_cut(original_path: str,
                             metamorphic_path: str,
                             elements_per_checkpoint: int = None,
                             n_times: int = 100,
                             sub_sample_question_ids: list = None) -> Dict[str, float]:
    """
    è¯„ä¼°cutç‰ˆæœ¬çš„èœ•å˜æµ‹è¯•æ€§èƒ½ï¼Œè®¡ç®—Precision, Recall, F1 Score

    Args:
        original_path: åŽŸå§‹ç»“æžœæ–‡ä»¶è·¯å¾„
        metamorphic_path: èœ•å˜ç»“æžœæ–‡ä»¶è·¯å¾„
        elements_per_checkpoint: æ¯ä¸ªæ£€æŸ¥ç‚¹é‡‡æ ·æ•°é‡
        n_times: é‡å¤è¯„ä¼°æ¬¡æ•°
        sub_sample_question_ids: å­é‡‡æ ·é—®é¢˜IDåˆ—è¡¨

    Returns:
        åŒ…å«Precision, Recall, F1ç­‰æŒ‡æ ‡çš„å­—å…¸
    """
    print("ðŸš€ Starting Metamorphic Testing for Cut Version...")

    # åŠ è½½å¯¹é½çš„ç»“æžœ
    orig_results = load_cut_results(original_path, elements_per_checkpoint)
    meta_results = load_cut_results(metamorphic_path, elements_per_checkpoint)

    # åˆ›å»ºæ˜ å°„
    orig_dict = {r["question_id"]: r for r in orig_results}
    meta_dict = {r["question_id"]: r for r in meta_results}


    # åªä¿ç•™å…±åŒçš„é—®é¢˜ID
    common_ids = set(orig_dict.keys()) & set(meta_dict.keys())
    if sub_sample_question_ids:
        common_ids = common_ids & set(sub_sample_question_ids)

    print(f"Evaluating {len(common_ids)} common samples...")

    # åˆå§‹åŒ–ç»Ÿè®¡
    sql_categories = ["SELECT", "WHERE", "GROUP_BY", "ORDER_BY", "AGGREGATION", "MULTI_OP"]

    overall_metrics = {
        "precision": [],
        "recall": [],
        "f1": [],
        "accuracy": []
    }

    category_metrics = {cat: {"precision": [], "recall": [], "f1": []} for cat in sql_categories}

    for i in tqdm(range(n_times), desc="Metamorphic Evaluation", unit="batch"):
        # åˆå§‹åŒ–æ··æ·†çŸ©é˜µ
        tp = fp = fn = tn = 0
        category_cm = {cat: {"tp": 0, "fp": 0, "fn": 0, "tn": 0} for cat in sql_categories}

        for qid in common_ids:
            orig_result = orig_dict[qid]
            meta_result = meta_dict[qid]

            # èŽ·å–çœŸå®žç­”æ¡ˆ
            true_answer = ", ".join(orig_result["answer"]) if isinstance(orig_result["answer"], list) else orig_result[
                "answer"]

            # å¤„ç†é¢„æµ‹ç­”æ¡ˆ
            orig_pred = flatten([orig_result["text"]]) if isinstance(orig_result["text"], str) else flatten(
                orig_result["text"])
            orig_pred = [extract_answer_cut(pred) for pred in orig_pred if pred]
            orig_pred, _ = Counter(orig_pred).most_common(1)[0]


            meta_pred = process_cut_predictions(meta_result)


            if orig_pred is None or meta_pred is None:
                continue

            # åˆ¤æ–­åŽŸå§‹ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
            orig_correct = eval_ex_match(true_answer, orig_pred)

            # æ£€æµ‹ä¸ä¸€è‡´æ€§
            inconsistency = not eval_ex_match(orig_pred, meta_pred)

            # æ›´æ–°æ··æ·†çŸ©é˜µ
            if not orig_correct:  # åŽŸå§‹ç­”æ¡ˆæœ‰å¹»è§‰
                if inconsistency:
                    tp += 1  # æ­£ç¡®æ£€æµ‹åˆ°å¹»è§‰
                else:
                    fn += 1  # æ¼æŠ¥
            else:  # åŽŸå§‹ç­”æ¡ˆæ­£ç¡®
                if inconsistency:
                    fp += 1  # è¯¯æŠ¥
                else:
                    tn += 1  # æ­£ç¡®é€šè¿‡

            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            table_columns = orig_result.get("table_columns", [])
            categories = classify_question(orig_result["question"], table_columns)

            for cat in categories:
                if not orig_correct:
                    if inconsistency:
                        category_cm[cat]["tp"] += 1
                    else:
                        category_cm[cat]["fn"] += 1
                else:
                    if inconsistency:
                        category_cm[cat]["fp"] += 1
                    else:
                        category_cm[cat]["tn"] += 1

            if len(categories) > 1:
                if not orig_correct:
                    if inconsistency:
                        category_cm["MULTI_OP"]["tp"] += 1
                    else:
                        category_cm["MULTI_OP"]["fn"] += 1
                else:
                    if inconsistency:
                        category_cm["MULTI_OP"]["fp"] += 1
                    else:
                        category_cm["MULTI_OP"]["tn"] += 1

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        accuracy = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) > 0 else 0

        overall_metrics["precision"].append(precision)
        overall_metrics["recall"].append(recall)
        overall_metrics["f1"].append(f1)
        overall_metrics["accuracy"].append(accuracy)

        # è®¡ç®—ç±»åˆ«æŒ‡æ ‡
        for cat in sql_categories:
            cat_tp, cat_fp, cat_fn = category_cm[cat]["tp"], category_cm[cat]["fp"], category_cm[cat]["fn"]
            cat_precision = cat_tp / (cat_tp + cat_fp) if (cat_tp + cat_fp) > 0 else 0
            cat_recall = cat_tp / (cat_tp + cat_fn) if (cat_tp + cat_fn) > 0 else 0
            cat_f1 = 2 * cat_precision * cat_recall / (cat_precision + cat_recall) if (
                                                                                                  cat_precision + cat_recall) > 0 else 0

            category_metrics[cat]["precision"].append(cat_precision)
            category_metrics[cat]["recall"].append(cat_recall)
            category_metrics[cat]["f1"].append(cat_f1)

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    final_metrics = {
        "overall": {
            "precision": np.mean(overall_metrics["precision"]) * 100,
            "recall": np.mean(overall_metrics["recall"]) * 100,
            "f1": np.mean(overall_metrics["f1"]) * 100,
            "accuracy": np.mean(overall_metrics["accuracy"]) * 100,
            "precision_std": np.std(overall_metrics["precision"]) * 100,
            "recall_std": np.std(overall_metrics["recall"]) * 100,
            "f1_std": np.std(overall_metrics["f1"]) * 100,
            "accuracy_std": np.std(overall_metrics["accuracy"]) * 100,
        },
        "by_category": {},
        "confusion_matrix": {
            "tp": tp,
            "fp": fp,
            "fn": fn,
            "tn": tn
        }
    }

    # æ·»åŠ ç±»åˆ«æŒ‡æ ‡
    for cat in sql_categories:
        if len(category_metrics[cat]["precision"]) > 0:
            final_metrics["by_category"][cat] = {
                "precision": np.mean(category_metrics[cat]["precision"]) * 100,
                "recall": np.mean(category_metrics[cat]["recall"]) * 100,
                "f1": np.mean(category_metrics[cat]["f1"]) * 100,
                "samples": category_cm[cat]["tp"] + category_cm[cat]["fn"]  # è¯¥ç±»åˆ«çš„å¹»è§‰æ ·æœ¬æ•°
            }

    # æ‰“å°ç»“æžœ
    print("\nðŸ“Š ========== Cut Version Metamorphic Testing Results ==========")
    print(f"æ€»æ ·æœ¬æ•°: {len(common_ids)}")
    print(f"Precision: {final_metrics['overall']['precision']:.2f}% Â± {final_metrics['overall']['precision_std']:.2f}%")
    print(f"Recall:    {final_metrics['overall']['recall']:.2f}% Â± {final_metrics['overall']['recall_std']:.2f}%")
    print(f"F1 Score:  {final_metrics['overall']['f1']:.2f}% Â± {final_metrics['overall']['f1_std']:.2f}%")
    print(f"Accuracy:  {final_metrics['overall']['accuracy']:.2f}% Â± {final_metrics['overall']['accuracy_std']:.2f}%")
    """
    print(f"\nðŸ” Confusion Matrix:")
    print(f"True Positives (TP): {tp}")
    print(f"False Positives (FP): {fp}")
    print(f"False Negatives (FN): {fn}")
    print(f"True Negatives (TN): {tn}")

    # è®¡ç®—é¢å¤–ç»Ÿè®¡
    hallucination_rate = (tp + fn) / len(common_ids) * 100 if len(common_ids) > 0 else 0
    detection_rate = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0
    false_alarm_rate = fp / (fp + tn) * 100 if (fp + tn) > 0 else 0

    print(f"\nðŸ“ˆ Additional Statistics:")
    print(f"Hallucination Rate: {hallucination_rate:.2f}%")
    print(f"Detection Rate: {detection_rate:.2f}%")
    print(f"False Alarm Rate: {false_alarm_rate:.2f}%")

    print(f"\nðŸŽ¯ Category-wise Results:")
    for cat, metrics in final_metrics["by_category"].items():
        if metrics["samples"] > 0:
            print(
                f"{cat.ljust(10)}: P={metrics['precision']:.1f}%, R={metrics['recall']:.1f}%, F1={metrics['f1']:.1f}% ({metrics['samples']} samples)")
    """
    return final_metrics



if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹



    metrics = eval_metamorphic_wtq(
        original_checkpoint="./output_tablegpt_agent_base/wtq_agent/result.jsonl",
        metamorphic_checkpoint="./output_tablegpt_agent_adv_test/wtq_agent/result.jsonl",
        n_times=100
    )
    exit(1)

    metrics = eval_metamorphic_wtq(
        original_checkpoint="./output_tablegpt_agent_base/wtq_agent/result.jsonl",
        metamorphic_checkpoint="./output_tablegpt_agent_row_shuffle/wtq_agent/result.jsonl",
        n_times=100
    )

    metrics = eval_metamorphic_wtq(
        original_checkpoint="./output_tablegpt_agent_base/wtq_agent/result.jsonl",
        metamorphic_checkpoint="./output_tablegpt_agent_column_shuffle/wtq_agent/result.jsonl",
        n_times=100
    )
    metrics = eval_metamorphic_wtq(
        original_checkpoint="./output_tablegpt_agent_base/wtq_agent/result.jsonl",
        metamorphic_checkpoint="./output_tablegpt_transpose/wtq_agent/result.jsonl",
        n_times=100
    )

    metrics = eval_metamorphic_wtq(
        original_checkpoint="./output_tablegpt_agent_base/wtq_agent/result.jsonl",
        metamorphic_checkpoint="./output_tablegpt_reconstruction/wtq_agent/result.jsonl",
        n_times=100
    )

    metrics = eval_metamorphic_wtq_cut(
        original_path="./output_tablegpt_agent_base/wtq_agent/result.jsonl",
        metamorphic_path="./output_tablegpt_agent_cut/wtq_agent/result.jsonl",
        elements_per_checkpoint=5,
        n_times=10
    )
    metrics = eval_metamorphic_wtq_cut(
        original_path="./output_tablegpt_agent_base/wtq_agent/result.jsonl",
        metamorphic_path="./output_tablegpt_column_cut/wtq_agent/result.jsonl",
        elements_per_checkpoint=5,
        n_times=10
    )
    metrics = eval_metamorphic_wtq(
        original_checkpoint="./output_tablegpt_agent_base/wtq_agent/result.jsonl",
        metamorphic_checkpoint="./output_tablgpt_Symbolization_pure_numbers_to_words/wtq_agent/result.jsonl",
        n_times=100
    )
    metrics = eval_metamorphic_wtq(
        original_checkpoint="./output_tablegpt_agent_base/wtq_agent/result.jsonl",
        metamorphic_checkpoint="./output_tablegpt_Category_Anonymization/wtq_agent/result.jsonl",
        n_times=100
    )
    metrics = eval_metamorphic_wtq(
        original_checkpoint="./output_tablegpt_agent_base/wtq_agent/result.jsonl",
        metamorphic_checkpoint="./output_tablegpt_time/wtq_agent/result.jsonl",
        n_times=100
    )
    exit(1)