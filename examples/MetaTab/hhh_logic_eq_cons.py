from collections import Counter
from utils.eval import eval_ex_match, extract_answer
import random
import json
import numpy as np
from tqdm import tqdm
from fire import Fire
from typing import Union, List, Tuple, Dict
import re


def flatten(lst):
    flat_list = []
    for i in lst:
        if isinstance(i, list):
            flat_list.extend(flatten(i))
        else:
            flat_list.append(i)
    return flat_list


def load_single_results(file_path: str):
    """åŠ è½½å•ä¸ªç»“æœæ–‡ä»¶"""
    print(f"Loading {file_path}...")

    if file_path.endswith(".jsonl"):
        with open(file_path, "r") as f:
            results = [json.loads(line) for line in f.readlines()]
    else:
        with open(f"output/{file_path}/result.jsonl", "r") as f:
            results = [json.loads(line) for line in f.readlines()]

    print(f"Loaded {len(results)} results.")

    # å»é‡
    results = {result["question_id"]: result for result in results}
    return list(results.values())


def load_dual_results(original_path: str, metamorphic_path: str):
    """åŠ è½½åŸå§‹å’Œèœ•å˜ä¸¤ä¸ªç»“æœæ–‡ä»¶"""
    original_results = load_single_results(original_path)
    metamorphic_results = load_single_results(metamorphic_path)

    # ç¡®ä¿ä¸¤ä¸ªç»“æœé›†åŸºäºquestion_idå¯¹é½
    orig_dict = {r["question_id"]: r for r in original_results}
    meta_dict = {r["question_id"]: r for r in metamorphic_results}

    # æ’åºç¡®ä¿é¡ºåºå›ºå®š
    common_ids = sorted(set(orig_dict.keys()) & set(meta_dict.keys()))

    aligned_results = []
    for qid in common_ids:
        aligned_results.append({
            'question_id': qid,
            'original': orig_dict[qid],
            'metamorphic': meta_dict[qid]
        })

    print(f"Aligned {len(aligned_results)} common results.")
    return aligned_results


def classify_question(question_text: str, table_columns: List[str] = None) -> List[str]:
    """è¿”å›é—®é¢˜æ‰€å±çš„æ‰€æœ‰SQLæ“ä½œç±»åˆ«"""
    question = question_text.lower()
    categories = set()

    # æ£€æµ‹èšåˆå‡½æ•°ï¼ˆCOUNT/SUM/AVGç­‰ï¼‰
    aggregation_keywords = [
        r"\b(count\(|sum\(|avg\(|average\(|max\(|min\()",
        r"\b(total\b|how many|number of|average of|sum of)",
        r"\b(most|least)\b.*\b(amount|quantity)\b"
    ]
    if any(re.search(pattern, question) for pattern in aggregation_keywords):
        categories.add("AGGREGATION")

    # æ£€æµ‹æ’åºï¼ˆORDER BYï¼‰
    if re.search(r"\b(order by|sort by|highest|lowest|top|bottom|ascending|descending)", question):
        categories.add("ORDER_BY")

    # æ£€æµ‹åˆ†ç»„ï¼ˆGROUP BYï¼‰
    if re.search(r"\b(group by|per|by each|for each)", question):
        categories.add("GROUP_BY")

    # æ£€æµ‹æ¡ä»¶è¿‡æ»¤ï¼ˆWHEREï¼‰
    condition_keywords = r"(>|<|=|!=|>=|<=|where|and|or|not in|excluding)"
    if re.search(condition_keywords, question):
        if table_columns:
            for col in table_columns:
                col = col.lower()
                if (col in question) and re.search(condition_keywords, question):
                    categories.add("WHERE")
                    break
        else:
            categories.add("WHERE")

    # é»˜è®¤ç±»åˆ«ï¼ˆç®€å•æŸ¥è¯¢ï¼‰
    if not categories:
        categories.add("SELECT")

    return sorted(categories)

import re
import re
import re
from typing import List, Union

import re
from typing import List, Union

def extract_action_input_code(data: List[Union[str, List[str]]]) -> List[str]:
    """
    ä» ReAct é£æ ¼æ—¥å¿—ä¸­æå–æ‰€æœ‰ Action Input åé¢çš„ä»£ç å—ã€‚
    è§„åˆ™ï¼š
    - ä»¥ Action Input å¼€å§‹ï¼Œåˆ°ä¸‹ä¸€æ¡ Observation æˆ– Thought/Action å¼€å¤´ä¹‹å‰ç»“æŸ
    - å»æ‰ ``` å’Œ python æ ‡ç­¾
    """
    code_blocks = []

    # åŒ¹é… Action Input åˆ° Observation/Thought/Action çš„å†…å®¹
    pattern = re.compile(
        r"Action Input:\s*(.*?)\s*(?=\n(?:Observation:|Thought:|Action:|$))",
        re.DOTALL
    )

    for item in data:
        if isinstance(item, str):
            matches = pattern.findall(item)
            for m in matches:
                # å»æ‰ ``` å’Œ python æ ‡ç­¾
                cleaned = re.sub(r"```(?:python)?", "", m, flags=re.IGNORECASE).strip()
                if cleaned:
                    code_blocks.append(cleaned)
        elif isinstance(item, list):
            code_blocks.extend(s.strip() for s in item if isinstance(s, str) and s.strip())

    return code_blocks




import ast

class SymbolicState:
    def __init__(self):
        self.vars = {}

    def assign(self, var, expr):
        # ä¿å­˜ç¬¦å·è¡¨è¾¾å¼
        self.vars[var] = expr
        #print(f"[DEBUG] Assign: {var} = {expr}")

    def get(self, var):
        return self.vars.get(var, var)


def clean_code_lines(code):
    """
    å»æ‰æ³¨é‡Šã€ç©ºè¡Œå’Œ import è¯­å¥
    """
    if isinstance(code, str):
        lines = code.strip().split("\n")
    else:
        lines = code

    cleaned = []
    for line in lines:
        line = line.strip()
        if line and not line.startswith("#") and not line.startswith("import") and not line.startswith("from"):
            cleaned.append(line)
    return cleaned


def ast_to_symbol(node, state):
    """
    å°† AST èŠ‚ç‚¹è½¬æ¢æˆç¬¦å·è¡¨è¾¾å¼
    """
    if isinstance(node, ast.Name):
        return state.get(node.id)
    elif isinstance(node, ast.Constant):
        return repr(node.value)
    elif isinstance(node, ast.BinOp):
        left = ast_to_symbol(node.left, state)
        right = ast_to_symbol(node.right, state)
        op = type(node.op).__name__
        return f"({left} {op} {right})"
    elif isinstance(node, ast.Compare):
        left = ast_to_symbol(node.left, state)
        comparators = [ast_to_symbol(c, state) for c in node.comparators]
        ops = [ast_to_symbol_op(op) for op in node.ops]
        comparisons = " ".join(f"{op} {c}" for op, c in zip(ops, comparators))
        return f"({left} {comparisons})"
    elif isinstance(node, ast.BoolOp):
        values = [ast_to_symbol(v, state) for v in node.values]
        op = type(node.op).__name__
        return f"({f' {op} '.join(values)})"
    elif isinstance(node, ast.UnaryOp):
        operand = ast_to_symbol(node.operand, state)
        op = type(node.op).__name__
        return f"({op} {operand})"
    elif isinstance(node, ast.Call):
        func = ast_to_symbol(node.func, state)
        args = [ast_to_symbol(a, state) for a in node.args]
        return f"{func}({', '.join(args)})"
    elif isinstance(node, ast.Attribute):
        value = ast_to_symbol(node.value, state)
        return f"{value}.{node.attr}"
    elif isinstance(node, ast.Subscript):
        value = ast_to_symbol(node.value, state)
        slice_ = ast_to_symbol(node.slice, state)
        return f"{value}[{slice_}]"
    elif isinstance(node, ast.Index):  # Python <3.9
        return ast_to_symbol(node.value, state)
    elif isinstance(node, ast.Slice):
        lower = ast_to_symbol(node.lower, state) if node.lower else ""
        upper = ast_to_symbol(node.upper, state) if node.upper else ""
        step = ast_to_symbol(node.step, state) if node.step else ""
        return f"{lower}:{upper}:{step}"
    elif isinstance(node, ast.Expr):
        return ast_to_symbol(node.value, state)
    elif isinstance(node, ast.Assign):
        target = node.targets[0].id if isinstance(node.targets[0], ast.Name) else str(node.targets[0])
        value = ast_to_symbol(node.value, state)
        state.assign(target, value)
        return value
    elif isinstance(node, ast.Tuple):
        elts = [ast_to_symbol(e, state) for e in node.elts]
        return f"({', '.join(elts)})"
    elif isinstance(node, ast.List):
        elts = [ast_to_symbol(e, state) for e in node.elts]
        return f"[{', '.join(elts)}]"
    elif isinstance(node, ast.Dict):
        keys = [ast_to_symbol(k, state) for k in node.keys]
        values = [ast_to_symbol(v, state) for v in node.values]
        return f"{{{', '.join(f'{k}: {v}' for k, v in zip(keys, values))}}}"
    elif isinstance(node, ast.Lambda):
        args = [arg.arg for arg in node.args.args]
        body = ast_to_symbol(node.body, state)
        return f"lambda {', '.join(args)}: {body}"

    elif isinstance(node, ast.ListComp):
        elt = ast_to_symbol(node.elt, state)
        gens = []
        for g in node.generators:
            target = ast_to_symbol(g.target, state)
            iter_ = ast_to_symbol(g.iter, state)
            ifs = " ".join(f"if {ast_to_symbol(if_cond, state)}" for if_cond in g.ifs)
            gens.append(f"for {target} in {iter_} {ifs}".strip())
        return f"[{elt} {' '.join(gens)}]"

    elif isinstance(node, ast.GeneratorExp):
        elt = ast_to_symbol(node.elt, state)
        gens = []
        for g in node.generators:
            target = ast_to_symbol(g.target, state)
            iter_ = ast_to_symbol(g.iter, state)
            ifs = " ".join(f"if {ast_to_symbol(if_cond, state)}" for if_cond in g.ifs)
            gens.append(f"for {target} in {iter_} {ifs}".strip())
        return f"({elt} {' '.join(gens)})"

    elif isinstance(node, ast.IfExp):
        body = ast_to_symbol(node.body, state)
        test = ast_to_symbol(node.test, state)
        orelse = ast_to_symbol(node.orelse, state)
        return f"({body} if {test} else {orelse})"

    else:
        return ast.dump(node)


def ast_to_symbol_op(op):
    """
    å°†æ¯”è¾ƒç¬¦å·è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    """
    if isinstance(op, ast.Eq): return "=="
    elif isinstance(op, ast.NotEq): return "!="
    elif isinstance(op, ast.Lt): return "<"
    elif isinstance(op, ast.LtE): return "<="
    elif isinstance(op, ast.Gt): return ">"
    elif isinstance(op, ast.GtE): return ">="
    elif isinstance(op, ast.In): return "in"
    elif isinstance(op, ast.NotIn): return "not in"
    else:
        return type(op).__name__



def symbolic_execute_ast(code_lines):
    """
    ä½¿ç”¨ AST åšç¬¦å·æ‰§è¡Œ
    """
    state = SymbolicState()
    last_expr = None

    for line in code_lines:
        try:
            tree = ast.parse(line)
            for node in tree.body:
                last_expr = ast_to_symbol(node, state)
        except Exception as e:
            last_expr = f"<Error: {e}>"

    return last_expr


def symbolic_logic_equivalence_ast(codes):
    """
    ä½¿ç”¨ AST ç¬¦å·æ‰§è¡Œåˆ¤æ–­é€»è¾‘ä¸€è‡´æ€§
    """
    symbolic_results = []

    for i, code in enumerate(codes):
        #print(f"[DEBUG] Processing code {i+1}/{len(codes)}")
        cleaned_lines = clean_code_lines(code)
        sym_result = symbolic_execute_ast(cleaned_lines)
        symbolic_results.append(sym_result)
        #print(f"[DEBUG] Symbolic result: {sym_result}")

    all_equal = all(r == symbolic_results[0] for r in symbolic_results)
    #print(f"[DEBUG] All symbolic results equal? {all_equal}")
    return all_equal, symbolic_results


import re


def normalize_symbolic_states(symbolic_states):
    normalized = []

    for code in symbolic_states:
        code = code.strip()

        # 1ï¸âƒ£ df[df['col']==value]['col2'].values[0]
        m1 = re.match(r"df\[\(df\['(.+?)'\]\s*==\s*'(.+?)'\)\]\['(.+?)'\](?:\.values\[0\])?", code)
        if m1:
            col, value, target_col = m1.groups()
            normalized.append(f"get_value(df, '{col}', '{value}', '{target_col}')")
            continue

        # 2ï¸âƒ£ df.loc[df['col']==value, 'col2'].values[0]
        m2 = re.match(r"df\.loc\[\(df\['(.+?)'\]\s*==\s*'(.+?)'\),\s*'(.+?)'\](?:\.values\[0\])?", code)
        if m2:
            col, value, target_col = m2.groups()
            normalized.append(f"get_value(df, '{col}', '{value}', '{target_col}')")
            continue

        # 3ï¸âƒ£ len(df[df['col']==value]) æˆ– df[df['col']==value].shape[0]
        m3 = re.match(r"(?:len|df\[\(df\['(.+?)'\]\s*==\s*'(.+?)'\)\]\.shape\[0\])", code)
        if m3 and m3.groups()[0] and m3.groups()[1]:
            col, value = m3.groups()
            normalized.append(f"count(df, '{col}', '{value}')")
            continue

        # æ²¡åŒ¹é…åˆ°è§„åˆ™ï¼Œç›´æ¥åŸæ ·ä¿ç•™
        normalized.append(code)

    return normalized

def sort_unique(states):
    counter = Counter(states)
    sorted_states = sorted(counter.items(), key=lambda x: -x[1])
    return [s for s, _ in sorted_states]

import os
save_path = "metamorphic_place_holder.json"
def eval_metamorphic_wtq(original_checkpoint: str,
                         metamorphic_checkpoint: str,
                         n_times: int = 100,
                         sub_sample_question_ids: list = None,
                         save_path: str = "metamorphic_same_code.json") -> Dict[str, float]:
    """
    è¯„ä¼°WTQæ•°æ®é›†çš„èœ•å˜æµ‹è¯•æ€§èƒ½ï¼Œå¹¶ç»Ÿè®¡ same_code æƒ…å†µ
    TP ä»…è®¡å…¥ same_codes éç©ºï¼Œsame_codes ä¸ºç©ºçš„ç®—ä½œ FN
    """
    results = load_dual_results(original_checkpoint, metamorphic_checkpoint)

    if sub_sample_question_ids:
        results = [r for r in results if r['question_id'] in sub_sample_question_ids]

    sql_categories = ["SELECT", "WHERE", "GROUP_BY", "ORDER_BY", "AGGREGATION", "MULTI_OP"]
    category_metrics = {cat: {"precision": [], "recall": [], "f1": []} for cat in sql_categories}

    overall_metrics = {"precision": [], "recall": [], "f1": []}

    # ===== NEW: å…¨å±€ same_code è®¡æ•°å™¨ =====
    total_eval_cases = 0
    same_code_cases = 0

    for _ in tqdm(range(n_times), desc="Evaluating Metamorphic Testing"):
        tp = fp = fn = tn = 0

        category_tp = {cat: 0 for cat in sql_categories}
        category_fp = {cat: 0 for cat in sql_categories}
        category_fn = {cat: 0 for cat in sql_categories}
        category_tn = {cat: 0 for cat in sql_categories}

        for result in results:
            orig_data = result['original']
            meta_data = result['metamorphic']

            orig_codes = extract_action_input_code(orig_data['text'])
            meta_codes = extract_action_input_code(meta_data['text'])

            _, orig_symbolic_states = symbolic_logic_equivalence_ast(orig_codes)
            _, meta_symbolic_states = symbolic_logic_equivalence_ast(meta_codes)

            true_answer = ", ".join(orig_data["answer"]) if isinstance(orig_data["answer"], list) else orig_data["answer"]

            orig_preds = flatten([orig_data["text"]]) if isinstance(orig_data["text"], str) else flatten(orig_data["text"])
            meta_preds = flatten([meta_data["text"]]) if isinstance(meta_data["text"], str) else flatten(meta_data["text"])

            orig_preds = [extract_answer(pred) for pred in orig_preds if pred]
            meta_preds = [extract_answer(pred) for pred in meta_preds if pred]

            if not orig_preds or not meta_preds:
                continue

            # ===== NEW: è®¡å…¥è¯„ä¼°æ ·æœ¬ =====
            total_eval_cases += 1

            orig_final_pred, _ = Counter(orig_preds).most_common(1)[0]
            meta_final_pred, _ = Counter(meta_preds).most_common(1)[0]

            orig_final_codes = [
                code for code, pred in zip(orig_symbolic_states, orig_preds)
                if pred == orig_final_pred
            ]
            meta_final_codes = [
                code for code, pred in zip(meta_symbolic_states, meta_preds)
                if pred == meta_final_pred
            ]

            orig_correct = eval_ex_match(true_answer, orig_final_pred)
            inconsistency = not eval_ex_match(orig_final_pred, meta_final_pred)

            same_codes = list({
                o_code
                for o_code in orig_final_codes
                for m_code in meta_final_codes
                if o_code == m_code
            })

            # ===== NEW: same_code ç»Ÿè®¡ =====
            if same_codes:
                same_code_cases += 1

            # ===== æ›´æ–°æ··æ·†çŸ©é˜µï¼ˆåŸé€»è¾‘ä¸å˜ï¼‰=====
            if same_codes:
                if not orig_correct:
                    if inconsistency:
                        tp += 1
                        if os.path.exists(save_path):
                            with open(save_path, "r", encoding="utf-8") as f:
                                all_data = json.load(f)
                        else:
                            all_data = []

                        all_data.append({
                            "question_id": result.get("question_id"),
                            "question": orig_data['question'],
                            "true_answer": true_answer,
                            "orig_final_pred": orig_final_pred,
                            "meta_final_pred": meta_final_pred,
                            "same_codes": same_codes
                        })

                        with open(save_path, "w", encoding="utf-8") as f:
                            json.dump(all_data, f, ensure_ascii=False, indent=2)
                    else:
                        fn += 1
                else:
                    if inconsistency:
                        fp += 1
                    else:
                        tn += 1

            # ===== æŒ‰ç±»åˆ«ç»Ÿè®¡ =====
            table_columns = orig_data.get("table_columns", [])
            categories = classify_question(orig_data["question"], table_columns)

            for cat in categories:
                if not orig_correct:
                    if inconsistency:
                        category_tp[cat] += 1 if same_codes else 0
                        category_fn[cat] += 1 if not same_codes else 0
                    else:
                        category_fn[cat] += 1
                else:
                    if inconsistency:
                        category_fp[cat] += 1
                    else:
                        category_tn[cat] += 1

            if len(categories) > 1:
                if not orig_correct:
                    if inconsistency:
                        category_tp["MULTI_OP"] += 1 if same_codes else 0
                        category_fn["MULTI_OP"] += 1 if not same_codes else 0
                    else:
                        category_fn["MULTI_OP"] += 1
                else:
                    if inconsistency:
                        category_fp["MULTI_OP"] += 1
                    else:
                        category_tn["MULTI_OP"] += 1

        # ===== è®¡ç®—æ€»ä½“æŒ‡æ ‡ =====
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        overall_metrics["precision"].append(precision)
        overall_metrics["recall"].append(recall)
        overall_metrics["f1"].append(f1)

    # ===== å¹³å‡æŒ‡æ ‡ =====
    final_metrics = {
        "overall": {
            "precision": np.mean(overall_metrics["precision"]) * 100,
            "recall": np.mean(overall_metrics["recall"]) * 100,
            "f1": np.mean(overall_metrics["f1"]) * 100,
            "precision_std": np.std(overall_metrics["precision"]) * 100,
            "recall_std": np.std(overall_metrics["recall"]) * 100,
            "f1_std": np.std(overall_metrics["f1"]) * 100,
        },
        "same_code_ratio": {
            "same_code_cases": same_code_cases,
            "total_cases": total_eval_cases,
            "ratio": same_code_cases / total_eval_cases * 100 if total_eval_cases > 0 else 0
        },
        "by_category": {}
    }

    print("\nğŸ“Š ========== èœ•å˜æµ‹è¯•è¯„ä¼°ç»“æœ ==========")
    print(f"æ€»è¯„ä¼°æ ·æœ¬æ•°: {total_eval_cases}")
    print(f"same_code æ ·æœ¬æ•°: {same_code_cases}")
    print(f"same_code å æ¯”: {final_metrics['same_code_ratio']['ratio']:.2f}%")
    print(f"Precision: {final_metrics['overall']['precision']:.2f}% Â± {final_metrics['overall']['precision_std']:.2f}%")
    print(f"Recall:    {final_metrics['overall']['recall']:.2f}% Â± {final_metrics['overall']['recall_std']:.2f}%")
    print(f"F1 Score:  {final_metrics['overall']['f1']:.2f}% Â± {final_metrics['overall']['f1_std']:.2f}%")

    return final_metrics



from collections import Counter
from utils.eval import eval_ex_match
import random
import json
import numpy as np
from tqdm import tqdm
from fire import Fire
from typing import Union, List, Tuple, Dict
import re


def classify_question(question_text: str, table_columns: List[str] = None) -> List[str]:
    """è¿”å›é—®é¢˜æ‰€å±çš„æ‰€æœ‰SQLæ“ä½œç±»åˆ«"""
    question = question_text.lower()
    categories = set()

    # æ£€æµ‹èšåˆå‡½æ•°ï¼ˆCOUNT/SUM/AVGç­‰ï¼‰
    aggregation_keywords = [
        r"\b(count\(|sum\(|avg\(|average\(|max\(|min\()",
        r"\b(total\b|how many|number of|average of|sum of)",
        r"\b(most|least)\b.*\b(amount|quantity)\b"
    ]
    if any(re.search(pattern, question) for pattern in aggregation_keywords):
        categories.add("AGGREGATION")

    # æ£€æµ‹æ’åºï¼ˆORDER BYï¼‰
    if re.search(r"\b(order by|sort by|highest|lowest|top|bottom|ascending|descending)", question):
        categories.add("ORDER_BY")

    # æ£€æµ‹åˆ†ç»„ï¼ˆGROUP BYï¼‰
    if re.search(r"\b(group by|per|by each|for each)", question):
        categories.add("GROUP_BY")

    # æ£€æµ‹æ¡ä»¶è¿‡æ»¤ï¼ˆWHEREï¼‰
    condition_keywords = r"(>|<|=|!=|>=|<=|where|and|or|not in|excluding)"
    if re.search(condition_keywords, question):
        if table_columns:
            for col in table_columns:
                col = col.lower()
                if (col in question) and re.search(condition_keywords, question):
                    categories.add("WHERE")
                    break
        else:
            categories.add("WHERE")

    # é»˜è®¤ç±»åˆ«ï¼ˆç®€å•æŸ¥è¯¢ï¼‰
    if not categories:
        categories.add("SELECT")

    return sorted(categories)


def extract_answer_cut(
        text: str,
        patterns: list = [r"Final Answer: (.*)", r": (.*)", r"is (.*)"],
        return_match_flag: bool = False,
        require_numeric: bool = True
):
    """
    Extracts the answer from a response text.
    """
    answer = None
    match_flag = False

    for pattern in reversed(patterns):
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            candidate = matches[-1].strip()
            if require_numeric and not candidate.isdigit():
                continue
            answer = candidate
            match_flag = "final answer" in pattern.lower()
            break

    if return_match_flag:
        return answer, match_flag
    return answer


def flatten(lst):
    """å±•å¹³åµŒå¥—åˆ—è¡¨"""
    flat_list = []
    for i in lst:
        if isinstance(i, list):
            flat_list.extend(flatten(i))
        else:
            flat_list.append(i)
    return flat_list








def flatten(lst):
    """å±•å¹³åµŒå¥—åˆ—è¡¨"""
    flat_list = []
    for i in lst:
        if isinstance(i, list):
            flat_list.extend(flatten(i))
        else:
            flat_list.append(i)
    return flat_list


def load_cut_results(checkpoint_path: str, elements_per_checkpoint: int = None):
    """åŠ è½½cutç‰ˆæœ¬çš„ç»“æœæ–‡ä»¶"""
    print(f"Loading cut results from {checkpoint_path}...")

    if checkpoint_path.endswith(".jsonl"):
        with open(checkpoint_path, "r") as f:
            results = [json.loads(line) for line in f.readlines()]
    else:
        with open(f"output/{checkpoint_path}/result.jsonl", "r") as f:
            results = [json.loads(line) for line in f.readlines()]

    print(f"Loaded {len(results)} results.")

    # å»é‡
    results = {result["question_id"]: result for result in results}
    results = list(results.values())

    # å¤„ç†text_part1å’Œtext_part2å­—æ®µ
    for result in results:
        if isinstance(result.get("text_part1"), str):
            result["text_part1"] = [result["text_part1"]]
        if isinstance(result.get("text_part2"), str):
            result["text_part2"] = [result["text_part2"]]

        # éšæœºé‡‡æ ·
        if elements_per_checkpoint is not None:
            if "text_part1" in result and result["text_part1"]:
                result["text_part1"] = random.sample(result["text_part1"],
                                                     min(elements_per_checkpoint, len(result["text_part1"])))
            if "text_part2" in result and result["text_part2"]:
                result["text_part2"] = random.sample(result["text_part2"],
                                                     min(elements_per_checkpoint, len(result["text_part2"])))

    return results


def process_cut_predictions(result: Dict, separators: List[str] = ["Final answer: "]):
    """å¤„ç†cutç‰ˆæœ¬çš„é¢„æµ‹ç»“æœ"""
    # å±•å¹³text_part1å’Œtext_part2
    if "text_part1" in result:
        result["text_part1"] = flatten(result["text_part1"])
    if "text_part2" in result:
        result["text_part2"] = flatten(result["text_part2"])

    # æå–ç­”æ¡ˆ
    preds1 = [extract_answer_cut(text) for text in result.get("text_part1", [])]
    preds2 = [extract_answer_cut(text) for text in result.get("text_part2", [])]

    # æ›¿æ¢Noneä¸º0
    preds1 = [0 if pred is None else pred for pred in preds1]
    preds2 = [0 if pred is None else pred for pred in preds2]

    # åˆå¹¶é¢„æµ‹ç»“æœ
    preds = preds1 + preds2
    preds = [pred for pred in preds if pred]

    if not preds:
        return None

    # å¤„ç†åˆ†éš”ç¬¦
    used_separator = None
    for sep in separators:
        if sep in str(preds[0]):
            used_separator = sep
            break

    if used_separator:
        processed_pred = str(preds[0]).replace(used_separator, "|")
        pred_list = [item.strip() for item in processed_pred.split("|") if item.strip()]
    else:
        pred_list = [str(pred) for pred in preds]

    # å¤šæ•°æŠ•ç¥¨
    pred_count = Counter(pred_list)
    try:
        final_pred, _ = pred_count.most_common(1)[0]
        return final_pred
    except:
        return None






if __name__ == "__main__":

    metrics = eval_metamorphic_wtq(
        original_checkpoint="./output_tablegpt_agent_base/wtq_agent/result.jsonl",
        metamorphic_checkpoint="./output_tablegpt_agent_column_shuffle/wtq_agent/result.jsonl",
        n_times=1
    )
# è¯»å–ä¿å­˜çš„ JSON æ–‡ä»¶
if os.path.exists(save_path):
    with open(save_path, "r", encoding="utf-8") as f:
        all_data = json.load(f)
else:
    all_data = []

# ç»Ÿè®¡ true_answer ä¸æ˜¯ç©ºå­—ç¬¦ä¸²çš„æ¡ç›®æ•°
num_error_with_same_code = sum(1 for entry in all_data if entry.get("true_answer"))

print(f"å…±æœ‰ {num_error_with_same_code} ä¸ªé”™è¯¯ç¨‹åºæ‰¾åˆ°ç›¸åŒä»£ç æ®µ")
# PMR1
# Precision: 48.37% Â± 0.00%
# Recall:    33.20% Â± 0.00%
# F1 Score:  39.37% Â± 0.00%




# PMR1
# #TP æ€»æ•°: 1155
#   same_codes éç©º: 577 (49.96%)
#   same_codes ä¸ºç©º: 578 (50.04%)

#PMR2
# TP æ€»æ•°: 1067
#   same_codes éç©º: 553 (51.83%)
#   same_codes ä¸ºç©º: 514 (48.17%)

#PMR3
# TP æ€»æ•°: 1294
#   same_codes éç©º: 262 (20.25%)
#   same_codes ä¸ºç©º: 1032 (79.75%)

# PMR4
# TP æ€»æ•°: 1148
#   same_codes éç©º: 469 (40.85%)
#   same_codes ä¸ºç©º: 679 (59.15%)
#
# DMR1
# æ€»æ•°: 23
#   same_codes éç©º: 16 (69.57%)
#   same_codes ä¸ºç©º: 7 (30.43%)

# DMR2
# éç©º: 2(5.26 %)
# same_codes
# ä¸ºç©º: 36(94.74 %)

#SMR1
# TP æ€»æ•°: 1027
#   same_codes éç©º: 484 (47.13%)
#   same_codes ä¸ºç©º: 543 (52.87%)


#SMR2
# TP æ€»æ•°: 986
#   same_codes éç©º: 581 (58.92%)
#   same_codes ä¸ºç©º: 405 (41.08%)

#SMR3
# TP æ€»æ•°: 24
#   same_codes éç©º: 11 (45.83%)
#   same_codes ä¸ºç©º: 13 (54.17%)
