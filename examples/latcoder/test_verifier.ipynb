{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "output_dir = Path('xx')\n",
    "subdirs = ['eval_gpt4v/eval_long_20240518105639', 'eval_d2c/eval_long_20240520083006', \n",
    "           'eval_on_llava/eval_long_20240815103907'] # , 'eval_ws/eval_long_20240520031420'\n",
    "sample_dirs = os.listdir(output_dir / subdirs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agents\n",
    "from html2shot_async import html2shot\n",
    "from utils import *\n",
    "from smart_blocker import blocker\n",
    "from vendors.google import gemini\n",
    "from vendors.openai import gpt4o\n",
    "from agents import *\n",
    "agents.BACKBONE = gpt4o\n",
    "from PIL import Image\n",
    "evaluator = Evaluator()\n",
    "\n",
    "def evaluate(ref, cands):\n",
    "    listofscore = evaluator.infer([ref] + cands)\n",
    "    print(\"\")\n",
    "    print(listofscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_comparision(imgs):\n",
    "    # \n",
    "    rows = 5  # \n",
    "    image_width = 700  # （）\n",
    "\n",
    "    #  HTML\n",
    "    html_content = \"<table style='border-collapse: collapse;'>\"\n",
    "    for i in range(0, len(imgs), rows):\n",
    "        html_content += \"<tr>\"\n",
    "        for j in range(rows):\n",
    "            if i + j < len(imgs):\n",
    "                img = imgs[i + j]\n",
    "                # \n",
    "                img_resized = img.copy()\n",
    "                img_resized.thumbnail((image_width, image_width))\n",
    "                #  HTML  base64 \n",
    "                from io import BytesIO\n",
    "                import base64\n",
    "                buffer = BytesIO()\n",
    "                img_resized.save(buffer, format=\"PNG\")\n",
    "                img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "                html_content += f\"<td style='padding: 10px;'><img src='data:image/png;base64,{img_base64}' /></td>\"\n",
    "        html_content += \"</tr>\"\n",
    "    html_content += \"</table>\"\n",
    "\n",
    "    #  Notebook \n",
    "    display(HTML(html_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "# id = random.randint(0, len(sample_dirs)-1)\n",
    "id=3\n",
    "samples = [output_dir / f'{s}/{sample_dirs[id]}' for s in subdirs if (output_dir / f'{s}/{sample_dirs[id]}').exists()]\n",
    "ref = Image.open( samples[0] / 'answer.png' )\n",
    "cands = [Image.open(s / 'prediction.png') for s in samples]\n",
    "evaluate(ref, cands)\n",
    "show_comparision([ref] + cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_1 = Path('xx')\n",
    "ref=Image.open(output_dir_1/'image.png')\n",
    "cands = [Image.open(output_dir_1/'prediction_101.png'),Image.open(output_dir_1/'prediction_103.png'),Image.open(output_dir_1/'prediction_1000.png')]\n",
    "evaluate(ref, cands)\n",
    "show_comparision([ref] + cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改硬性指标(上面的没用了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "# \n",
    "sys.path.append(\"xx\")\n",
    "\n",
    "# \n",
    "from evaluation.mrweb.emd_similarity import emd_similarity\n",
    "from evaluation.mrweb.study import CLIPScorer, LPIPSScorer, ssim_score, psnr_score, mae_score\n",
    "\n",
    "\n",
    "def evaluate_images(img1, img2):\n",
    "    \"\"\"\n",
    "    ，。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        clip_scorer = CLIPScorer(model_name='ViT-B-32-quickgelu', pretrained='openai')\n",
    "        lpips_scorer = LPIPSScorer(net='vgg')\n",
    "\n",
    "        clip_score = clip_scorer.score(img1, img2)\n",
    "        lpips_score = lpips_scorer.score(img1, img2)\n",
    "        ssim_value = ssim_score(img1, img2)\n",
    "        psnr_value = psnr_score(img1, img2)\n",
    "        mae_value = mae_score(img1, img2)\n",
    "        emd_score = emd_similarity(img1, img2, max_size=96, mode=\"RGB\")\n",
    "\n",
    "        return {\n",
    "            \"CLIP Score\": clip_score,\n",
    "            \"LPIPS Score\": lpips_score,\n",
    "            \"SSIM\": ssim_value,\n",
    "            \"PSNR\": psnr_value,\n",
    "            \"MAE\": mae_value,\n",
    "            \"EMD Similarity\": emd_score\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def evaluate_all(ref_image, candidate_images):\n",
    "    \"\"\"\n",
    "    ，。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for idx, cand_image in enumerate(candidate_images):\n",
    "        try:\n",
    "            print(f\"Evaluating candidate {idx + 1}/{len(candidate_images)}...\")\n",
    "            scores = evaluate_images(ref_image, cand_image)\n",
    "            if scores:\n",
    "                results.append({\n",
    "                    \"candidate_index\": idx,\n",
    "                    \"scores\": scores\n",
    "                })\n",
    "                # \n",
    "                print(f\"Results for candidate {idx}:\\n\" + \"\\n\".join(\n",
    "                    [f\"{metric}: {score}\" for metric, score in scores.items()]))\n",
    "            else:\n",
    "                print(f\"Failed to evaluate candidate {idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating candidate {idx}: {e}\")\n",
    "\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # \n",
    "    output_dir_1 = Path('xx')\n",
    "    try:\n",
    "        ref = Image.open(output_dir_1 / 'image.png')\n",
    "        cands = [Image.open(output_dir_1/'prediction_0.png'),Image.open(output_dir_1/'prediction_1000.png'),Image.open(output_dir_1/'prediction_100000.png')]\n",
    "        # \n",
    "        results = evaluate_all(ref, cands)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    show_comparision([ref] + cands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # \n",
    "    output_dir_1 = Path('xx')\n",
    "    try:\n",
    "        ref=Image.open(output_dir_1/'image.png')\n",
    "        cands = [Image.open(output_dir_1/'prediction_0.png'),Image.open(output_dir_1/'prediction_1000.png'),Image.open(output_dir_1/'prediction_1001.png')]\n",
    "        results = evaluate_all(ref, cands)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    show_comparision([ref] + cands)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wgh\n",
    "if __name__ == \"__main__\":\n",
    "    # \n",
    "    output_dir_1 = Path('xx')\n",
    "    try:\n",
    "        ref=Image.open(output_dir_1/'answer.png')\n",
    "        cands = [Image.open(output_dir_1/'assemble_0.png'),Image.open(output_dir_1/'assemble_1.png'),Image.open(output_dir_1/'assemble_2.png'),Image.open(output_dir_1/'assemble_3.png')]\n",
    "        results = evaluate_all(ref, cands)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    show_comparision([ref] + cands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # \n",
    "    output_dir_1 = Path('xx')\n",
    "    try:\n",
    "        ref=Image.open(output_dir_1/'answer.png')\n",
    "        cands = [Image.open(output_dir_1/'assemble_0.png'),Image.open(output_dir_1/'assemble_1.png'),Image.open(output_dir_1/'assemble_2.png'),Image.open(output_dir_1/'assemble_3.png')]\n",
    "        results = evaluate_all(ref, cands)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    show_comparision([ref] + cands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ，\n",
    "from tqdm import tqdm\n",
    "from agents import *\n",
    "from smart_blocker import *\n",
    "save_path=f'xx'\n",
    "base_path = save_path\n",
    "agentGenerate = AgentGenerate()\n",
    "\n",
    "def generate_module_code(plans):\n",
    "    md5 = image2md5(image)\n",
    "    codes_plans = []\n",
    "    for index,plan in tqdm(enumerate(plans,start=1),total=len(plans)):\n",
    "        module_image = crop_image(image,plan)\n",
    "        path = save_path + f\"/{md5}\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        path += f\"/{index}.png\"\n",
    "\n",
    "        print(path)\n",
    "        module_image.save(path)\n",
    "\n",
    "        # code = agentGenerate.infer(image1=image,image2=module_image,text=json.dumps(plan))\n",
    "        code = agentGenerate.infer([module_image])\n",
    "        code=remove_code_markers(code)\n",
    "        print(code)\n",
    "\n",
    "        image_pre = html2shot(base_path, code,md5,index)\n",
    "\n",
    "        codes_plans.append({\"module_position\":plan,\"module_code\":code})\n",
    "    \n",
    "    return codes_plans\n",
    "image = Image.open(\"xx\")\n",
    "# \n",
    "plans = blocker(image)\n",
    "codes = generate_module_code(plans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2md5(image):\n",
    "    image_bytes = io.BytesIO()\n",
    "    image.save(image_bytes, format='PNG')\n",
    "    image_data = image_bytes.getvalue()\n",
    "    md5_hash = hashlib.md5(image_data)\n",
    "    md5_hex = md5_hash.hexdigest()\n",
    "    return str(md5_hex)\n",
    "# \n",
    "def absolute_assemble(image,code_plans):\n",
    "    md5 = image2md5(image)\n",
    "\n",
    "    html_image = []\n",
    "\n",
    "    code_abss = []\n",
    "    for node in code_plans:\n",
    "        bbox = node['module_position']\n",
    "        code_abs = f'<div style=\"position: absolute; overflow: hidden; border: 1px solid blue; left: {round(bbox[0] * image.width)}px; top: {round(bbox[1] * image.height)}px; width: {round((bbox[2] - bbox[0]) * image.width)}px; height: {round((bbox[3] - bbox[1]) * image.height)}px;\">{\"\".join(node[\"module_code\"])}</div>'\n",
    "        code_abss.append(code_abs)\n",
    "\n",
    "    html = \"\"\"<html>\n",
    "    <body>\n",
    "        \"\"\" + '\\n'.join(code_abss) + \"\"\"\n",
    "    </body>\n",
    "</html>\"\"\"\n",
    "\n",
    "    # imageofhtml = html2shot(base_path, html_content=html,md5=md5,index=1000)\n",
    "    # html_image.append({\"html\":html,\"image\":imageofhtml})\n",
    "    return html\n",
    "\n",
    "\n",
    "html1code = absolute_assemble(image,codes)\n",
    "\n",
    "md5 = image2md5(image=image)\n",
    "print(md5)\n",
    "imagehtml = await html2shot(save_path,html1code,md5,22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent\n",
    "agentAssemble = AgentAssemble()\n",
    "\n",
    "html2code = agentAssemble.infer([image],text=json.dumps(codes))\n",
    "imagehtml2 = await html2shot(save_path,html2code,md5,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # \n",
    "    output_dir_1 = Path('xx')\n",
    "\n",
    "    try:\n",
    "        ref=Image.open(output_dir_1/'image.png')\n",
    "        cands = [Image.open(output_dir_1/'prediction_0.png'),Image.open(output_dir_1/'prediction_1000.png'),Image.open(output_dir_1/'prediction_100000.png')]\n",
    "        results = evaluate_all(ref, cands)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    show_comparision([ref] + cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def normalize_value(value, min_value, max_value, reverse=False):\n",
    "    normalized_value = (value - min_value) / (max_value - min_value)\n",
    "    if reverse:\n",
    "        # （）， 1 - normalized_value\n",
    "        return 1 - normalized_value\n",
    "    return normalized_value\n",
    "\n",
    "def get_best_candidate(result, mae_emd_threshold=0.1):\n",
    "    best_index = -1\n",
    "\n",
    "    # MAEEMD，\n",
    "    all_mae = [entry['scores']['MAE'] for entry in result]\n",
    "    all_emd = [entry['scores']['EMD Similarity'] for entry in result]\n",
    "    \n",
    "    min_mae = min(all_mae)\n",
    "    max_mae = max(all_mae)\n",
    "    min_emd = min(all_emd)\n",
    "    max_emd = max(all_emd)\n",
    "\n",
    "    scores_with_index = []\n",
    "\n",
    "    for i, entry in enumerate(result):\n",
    "        scores = entry['scores']\n",
    "        mae = scores['MAE']\n",
    "        emd = scores['EMD Similarity']\n",
    "        clip_score = scores['CLIP Score']\n",
    "\n",
    "        # MAE（）EMD\n",
    "        normalized_mae = normalize_value(mae, min_mae, max_mae, reverse=True)\n",
    "        normalized_emd = normalize_value(emd, min_emd, max_emd, reverse=False)\n",
    "\n",
    "        # \n",
    "        print(f\"index = {i}\")\n",
    "        print(f\"mae : {normalized_mae}\")\n",
    "        print(f\"emd : {normalized_emd}\")\n",
    "\n",
    "        # MAEEMD（50%）\n",
    "        combined_score = 0.5 * normalized_mae + 0.5 * normalized_emd\n",
    "\n",
    "        # \n",
    "        scores_with_index.append((i, combined_score, mae, emd, clip_score))\n",
    "\n",
    "    # \n",
    "    print(scores_with_index)\n",
    "\n",
    "    # ，\n",
    "    scores_with_index.sort(key=lambda x: x[1] , reverse= True)\n",
    "\n",
    "    # \n",
    "    best_index, best_combined_score, _, _, _ = scores_with_index[0]\n",
    "    second_best_index, second_best_combined_score, _, _, _ = scores_with_index[1]\n",
    "\n",
    "    # ，\n",
    "    score_diff = best_combined_score - second_best_combined_score\n",
    "    print(f\"score_diff {score_diff}\")\n",
    "\n",
    "    print(f\"Best candidate: {best_index}, combined score: {best_combined_score}\")\n",
    "    print(f\"Second best candidate: {second_best_index}, combined score: {second_best_combined_score}\")\n",
    "\n",
    "    if score_diff > mae_emd_threshold:  # \n",
    "        print(f\"Candidate {best_index} is significantly better, selecting as the best.\")\n",
    "        return best_index\n",
    "    else:\n",
    "        # ， CLIP Score\n",
    "\n",
    "        #  0.1 \n",
    "        valid_candidates = []\n",
    "        for index, combined_score, _, _, clip_score in scores_with_index:\n",
    "            if abs(combined_score - best_combined_score) <= 0.1:\n",
    "                valid_candidates.append({\"index\":index,\"clip_score\":clip_score})\n",
    "        if valid_candidates:\n",
    "            best_candidate = max(valid_candidates, key=lambda x: x[\"clip_score\"])\n",
    "            best_clip_index = best_candidate[\"index\"]\n",
    "            print(f\"clip {best_clip_index}\")\n",
    "            return best_clip_index\n",
    "        else:\n",
    "            # ，best_index\n",
    "            print(\"\")\n",
    "            return best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_index = get_best_candidate(result=results)\n",
    "results_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.mrweb.emd_similarity import emd_similarity\n",
    "from evaluation.mrweb.study import CLIPScorer, LPIPSScorer, ssim_score, psnr_score, mae_score\n",
    "from evaluation.metrics import clip_sim\n",
    "\n",
    "def evaluate_images(img1, img2):\n",
    "    \"\"\"\n",
    "    ，。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #  MAE\n",
    "        mae_value = mae_score(img1, img2)\n",
    "        #  EMD\n",
    "        emd_score = emd_similarity(img1, img2, max_size=96, mode=\"RGB\") \n",
    "        #  CLIP\n",
    "        clip_value = clip_sim(img1, img2)  \n",
    "\n",
    "        return mae_value, emd_score, clip_value\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error during evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def verify_score(mae, nemd, clip_similarity,  weights=(0.3, 0.3, 0.4)):\n",
    "    \"\"\"\n",
    "    Computes a composite similarity score based on MAE, NEMD, and CLIP similarity.\n",
    "\n",
    "    Args:\n",
    "        mae (float): Mean Absolute Error between images, normalized.\n",
    "        nemd (float): 1-emd, and Normalized Earth Mover's Distance (already in [0, 1]).\n",
    "        clip_similarity (float): CLIP cosine similarity between images.\n",
    "        weights (tuple): Weights for (MAE_similarity, NEMD_similarity, CLIP_transformed).\n",
    "\n",
    "    Returns:\n",
    "        float: Composite similarity score.\n",
    "    \"\"\"\n",
    "    w1, w2, w3 = weights    \n",
    "    \n",
    "    # Compute composite score\n",
    "    composite_score = w1 * 1 / (1 + mae) + w2 * nemd + w3 * clip_similarity ** 0.5\n",
    "    \n",
    "    return composite_score\n",
    "     \n",
    "\n",
    "def get_best_candidate(ref_img, cand_imgs):\n",
    "    assert len(cand_imgs) > 1, \"There must be more than one candidates to select.\"\n",
    "    \n",
    "    scores = {'MAE':[], 'NEMD':[], 'CLIP':[]}\n",
    "    for i,cand_image in  enumerate(cand_imgs):\n",
    "        mae_value, emd_score, clip_value = evaluate_images(ref_img, cand_image)\n",
    "\n",
    "        print(f\"Candidate {i+1} MAE score: {mae_value}\")\n",
    "        print(f\"Candidate {i+1} EMD score: {emd_score}\")\n",
    "        print(f\"Candidate {i+1} CLIP score: {clip_value}\")\n",
    "        scores['MAE'].append(mae_value)\n",
    "        scores['NEMD'].append(emd_score)\n",
    "        scores['CLIP'].append(clip_value)\n",
    "    print(scores)\n",
    "    # \n",
    "    weights = (0.3, 0.3, 0.4)\n",
    "    # \n",
    "    final_scores = [verify_score(scores['MAE'][i], scores['NEMD'][i], scores['CLIP'][i], weights) for i in range(len(cand_imgs))]\n",
    "    print(final_scores)\n",
    "    \n",
    "    # \n",
    "    max_score = max(final_scores)\n",
    "    idx = final_scores.index(max_score)\n",
    "    \n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.mrweb.emd_similarity import emd_similarity\n",
    "from evaluation.mrweb.study import CLIPScorer, LPIPSScorer, ssim_score, psnr_score, mae_score\n",
    "from evaluation.metrics import clip_sim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "import os,sys\n",
    "from pathlib import Path\n",
    "\n",
    "def show_comparision(imgs):\n",
    "    # \n",
    "    rows = 5  # \n",
    "    image_width = 700  # （）\n",
    "\n",
    "    #  HTML\n",
    "    html_content = \"<table style='border-collapse: collapse;'>\"\n",
    "    for i in range(0, len(imgs), rows):\n",
    "        html_content += \"<tr>\"\n",
    "        for j in range(rows):\n",
    "            if i + j < len(imgs):\n",
    "                img = imgs[i + j]\n",
    "                # \n",
    "                img_resized = img.copy()\n",
    "                img_resized.thumbnail((image_width, image_width))\n",
    "                #  HTML  base64 \n",
    "                from io import BytesIO\n",
    "                import base64\n",
    "                buffer = BytesIO()\n",
    "                img_resized.save(buffer, format=\"PNG\")\n",
    "                img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "                html_content += f\"<td style='padding: 10px;'><img src='data:image/png;base64,{img_base64}' /></td>\"\n",
    "        html_content += \"</tr>\"\n",
    "    html_content += \"</table>\"\n",
    "\n",
    "    #  Notebook \n",
    "    display(HTML(html_content))\n",
    "\n",
    "def normalize_value(value, min_value, max_value, reverse=False):\n",
    "    normalized_value = (value - min_value) / (max_value - min_value)\n",
    "    if reverse:\n",
    "        return 1 - normalized_value\n",
    "    return normalized_value\n",
    "\n",
    "def evaluate_images(img1, img2):\n",
    "    \"\"\"\n",
    "    ，。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mae_value = mae_score(img1, img2)\n",
    "        emd_score = emd_similarity(img1, img2, max_size=96, mode=\"RGB\") \n",
    "        clip_value = clip_sim(img1, img2)  \n",
    "        return mae_value, emd_score, clip_value\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error during evaluation: {e}\")\n",
    "        return None\n",
    "    \n",
    "# \n",
    "def verify_score(mae, nemd, clip_similarity, weights=(0.3, 0.3, 0.4)):\n",
    "    w1, w2, w3 = weights\n",
    "    composite_score = w1 * 1 / (1 + mae) + w2 * nemd + w3 * clip_similarity ** 0.5\n",
    "    return composite_score\n",
    "\n",
    "def get_best_candidate(ref_img, cand_imgs):\n",
    "    assert len(cand_imgs) > 1, \"There must be more than one candidates to select.\"\n",
    "    \n",
    "    scores = {'MAE':[], 'NEMD':[], 'CLIP':[]}\n",
    "    mae_values, emd_scores, clip_values = [], [], []\n",
    "    \n",
    "    # \n",
    "    for i, cand_image in enumerate(cand_imgs):\n",
    "        mae_value, emd_score, clip_value = evaluate_images(ref_img, cand_image)        \n",
    "        mae_values.append(mae_value)\n",
    "        emd_scores.append(emd_score)\n",
    "        clip_values.append(clip_value)\n",
    "    \n",
    "    # \n",
    "    mae_min, mae_max = np.min(mae_values), np.max(mae_values)\n",
    "    emd_min, emd_max = np.min(emd_scores), np.max(emd_scores)\n",
    "    clip_min, clip_max = np.min(clip_values), np.max(clip_values)\n",
    "    \n",
    "    # \n",
    "    normalized_mae = [normalize_value(val, mae_min, mae_max,reverse=True) for val in mae_values]\n",
    "    normalized_nemd = [normalize_value(val, emd_min, emd_max) for val in emd_scores]\n",
    "    normalized_clip = [normalize_value(val, clip_min, clip_max) for val in clip_values]\n",
    "    \n",
    "    for i in range(len(mae_values)):\n",
    "        print(f\"Candidate {i+1}:\")\n",
    "        print(f\"  Original MAE: {mae_values[i]}, Normalized MAE: {normalized_mae[i]}\")\n",
    "        print(f\"  Original NEMD: {emd_scores[i]}, Normalized NEMD: {normalized_nemd[i]}\")\n",
    "        print(f\"  Original CLIP: {clip_values[i]}, Normalized CLIP: {normalized_clip[i]}\")\n",
    "\n",
    "    \n",
    "    # \n",
    "    selected_indices = []\n",
    "    for i in range(len(cand_imgs)):\n",
    "        high_scores = sum([1 for score in [normalized_mae[i], normalized_nemd[i], normalized_clip[i]] if score > 0.9])\n",
    "        if high_scores >= 2:\n",
    "            selected_indices.append(i)\n",
    "    \n",
    "    # ，\n",
    "    if len(selected_indices) == 1:\n",
    "        best_index = selected_indices[0]\n",
    "    elif len(selected_indices) > 1:\n",
    "        # ，\n",
    "        composite_scores = []\n",
    "        for i in selected_indices:\n",
    "            composite_score = verify_score(mae_values[i], emd_scores[i],clip_values[i])\n",
    "            composite_scores.append(composite_score)\n",
    "        best_index = selected_indices[np.argmax(composite_scores)]\n",
    "    else:\n",
    "        # ，\n",
    "        composite_scores = []\n",
    "        for i in range(len(cand_imgs)):\n",
    "            composite_score = verify_score(mae_values[i], emd_scores[i],clip_values[i])\n",
    "            composite_scores.append(composite_score)\n",
    "        best_index = np.argmax(composite_scores)\n",
    "    \n",
    "    print(f\"Best candidate index: {best_index}\")\n",
    "    return best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.mrweb.emd_similarity import emd_similarity\n",
    "from evaluation.mrweb.study import CLIPScorer, LPIPSScorer, ssim_score, psnr_score, mae_score\n",
    "from evaluation.metrics import clip_sim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "import os,sys\n",
    "from pathlib import Path\n",
    "\n",
    "def show_comparision(imgs):\n",
    "    # \n",
    "    rows = 5  # \n",
    "    image_width = 700  # （）\n",
    "\n",
    "    #  HTML\n",
    "    html_content = \"<table style='border-collapse: collapse;'>\"\n",
    "    for i in range(0, len(imgs), rows):\n",
    "        html_content += \"<tr>\"\n",
    "        for j in range(rows):\n",
    "            if i + j < len(imgs):\n",
    "                img = imgs[i + j]\n",
    "                # \n",
    "                img_resized = img.copy()\n",
    "                img_resized.thumbnail((image_width, image_width))\n",
    "                #  HTML  base64 \n",
    "                from io import BytesIO\n",
    "                import base64\n",
    "                buffer = BytesIO()\n",
    "                img_resized.save(buffer, format=\"PNG\")\n",
    "                img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "                html_content += f\"<td style='padding: 10px;'><img src='data:image/png;base64,{img_base64}' /></td>\"\n",
    "        html_content += \"</tr>\"\n",
    "    html_content += \"</table>\"\n",
    "\n",
    "    #  Notebook \n",
    "    display(HTML(html_content))\n",
    "\n",
    "def normalize_value(value, min_value, max_value, reverse=False):\n",
    "    normalized_value = (value - min_value) / (max_value - min_value)\n",
    "    if reverse:\n",
    "        return 1 - normalized_value\n",
    "    return normalized_value\n",
    "\n",
    "def evaluate_images(img1, img2):\n",
    "    \"\"\"\n",
    "    ，。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mae_value = mae_score(img1, img2)\n",
    "        emd_score = emd_similarity(img1, img2, max_size=96, mode=\"RGB\") \n",
    "        clip_value = clip_sim(img1, img2)  \n",
    "        return mae_value, emd_score, clip_value\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error during evaluation: {e}\")\n",
    "        return None\n",
    "    \n",
    "# \n",
    "def verify_score(mae, nemd, clip_similarity, weights=(0.3, 0.3, 0.4)):\n",
    "    w1, w2, w3 = weights\n",
    "    composite_score = w1 * (1-mae/100) + w2 * nemd + w3 * clip_similarity ** 0.5\n",
    "    return composite_score\n",
    "\n",
    "def get_best_candidate(ref_img, cand_imgs):\n",
    "    assert len(cand_imgs) > 1, \"There must be more than one candidates to select.\"\n",
    "    \n",
    "    # \n",
    "    mae_values, emd_scores, clip_values = [], [], []\n",
    "    for cand_image in cand_imgs:\n",
    "        mae_value, emd_score, clip_value = evaluate_images(ref_img, cand_image)\n",
    "        mae_values.append(mae_value)\n",
    "        emd_scores.append(emd_score)\n",
    "        clip_values.append(clip_value)\n",
    "    \n",
    "    # \n",
    "    mae_threshold = 0.1 * np.min(mae_values)  # MAE10%\n",
    "    emd_threshold = 0.1  # EMD0.1\n",
    "    clip_threshold = 0.1  # CLIP0.1\n",
    "    \n",
    "    # \n",
    "    scores = [0] * len(cand_imgs)\n",
    "    \n",
    "    # MAE\n",
    "    if np.max(mae_values) - np.min(mae_values) > mae_threshold:\n",
    "        for i, mae in enumerate(mae_values):\n",
    "            # MAE：(max_mae - mae) / (max_mae - min_mae)\n",
    "            mae_score = (np.max(mae_values) - mae) / (np.max(mae_values) - np.min(mae_values))\n",
    "            scores[i] += mae_score\n",
    "\n",
    "    # EMD\n",
    "    if np.max(emd_scores) - np.min(emd_scores) > emd_threshold:\n",
    "        for i, emd in enumerate(emd_scores):\n",
    "            # EMD：(emd - min_emd) / (max_emd - min_emd)\n",
    "            emd_score = (emd - np.min(emd_scores)) / (np.max(emd_scores) - np.min(emd_scores))\n",
    "            scores[i] += emd_score\n",
    "\n",
    "    # CLIP\n",
    "    if np.max(clip_values) - np.min(clip_values) > clip_threshold:\n",
    "        for i, clip in enumerate(clip_values):\n",
    "            # CLIP：(clip - min_clip) / (max_clip - min_clip)\n",
    "            clip_score = (clip - np.min(clip_values)) / (np.max(clip_values) - np.min(clip_values))\n",
    "            scores[i] += clip_score\n",
    "    \n",
    "    # \n",
    "    for i in range(len(cand_imgs)):\n",
    "        print(f\"Candidate {i}:\")\n",
    "        print(f\"  MAE: {mae_values[i]}, EMD: {emd_scores[i]}, CLIP: {clip_values[i]}\")\n",
    "        print(f\"  Score: {scores[i]}\")\n",
    "    \n",
    "    # 0，\n",
    "    if all(score == 0 for score in scores):\n",
    "        composite_scores = []\n",
    "        for i in range(len(cand_imgs)):\n",
    "            composite_score = verify_score(mae_values[i], emd_scores[i], clip_values[i])\n",
    "            composite_scores.append(composite_score)\n",
    "        best_index = np.argmax(composite_scores)\n",
    "    else:\n",
    "        # \n",
    "        best_index = np.argmax(scores)\n",
    "    \n",
    "    print(f\"Best candidate index: {best_index}\")\n",
    "    return best_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uiagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
