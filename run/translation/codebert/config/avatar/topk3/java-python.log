nohup: ignoring input
Using backend: pytorch
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. 
The class this function is called from is 'TransformersDictionary'.
[2021-11-05 11:20:26]    INFO >> No /home/wanyao/yang/ncc_data/avatar/translation/top3/codebert/data-mmap/java-python/last_checkpoint.pt to initialize model (train.py:86, <module>())
[2021-11-05 11:20:26]    INFO >> Start training epoch   1/100, best bleu4: 0.00 (train.py:88, <module>())
/home/wanyao/anaconda3/envs/py37/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[2021-11-05 11:27:04]    INFO >> Epoch   1/100, Batch 500/5302, train loss: 1722.3840 (train.py:125, train())
[2021-11-05 11:32:41]    INFO >> Epoch   1/100, Batch 1000/5302, train loss: 1358.2866 (train.py:125, train())
[2021-11-05 11:38:28]    INFO >> Epoch   1/100, Batch 1500/5302, train loss: 1178.8552 (train.py:125, train())
[2021-11-05 11:44:31]    INFO >> Epoch   1/100, Batch 2000/5302, train loss: 1053.3863 (train.py:125, train())
[2021-11-05 11:50:04]    INFO >> Epoch   1/100, Batch 2500/5302, train loss: 970.4976 (train.py:125, train())
[2021-11-05 11:53:19]    INFO >> Epoch   1/100, Batch 3000/5302, train loss: 907.6923 (train.py:125, train())
[2021-11-05 11:55:52]    INFO >> Epoch   1/100, Batch 3500/5302, train loss: 858.7022 (train.py:125, train())
[2021-11-05 11:59:01]    INFO >> Epoch   1/100, Batch 4000/5302, train loss: 819.2726 (train.py:125, train())
[2021-11-05 12:03:16]    INFO >> Epoch   1/100, Batch 4500/5302, train loss: 787.3968 (train.py:125, train())
[2021-11-05 12:07:35]    INFO >> Epoch   1/100, Batch 5000/5302, train loss: 760.3548 (train.py:125, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-05 12:10:19]    INFO >> Epoch   1/100, train loss: 745.8001 (train.py:134, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-05 12:17:13]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/codebert/data-mmap/java-python/best_checkpoint.pt (train.py:208, <module>())
[2021-11-05 12:17:13]    INFO >> Epoch   1/100, valid loss: 0.0000, valid bleu4: 5.31, best bleu4: 5.31 (train.py:210, <module>())
[2021-11-05 12:17:29]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/codebert/data-mmap/java-python/last_checkpoint.pt (train.py:215, <module>())
[2021-11-05 12:23:07]    INFO >> Epoch   2/100, Batch 500/5302, train loss: 480.6958 (train.py:125, train())
[2021-11-05 12:27:25]    INFO >> Epoch   2/100, Batch 1000/5302, train loss: 472.4872 (train.py:125, train())
[2021-11-05 12:31:37]    INFO >> Epoch   2/100, Batch 1500/5302, train loss: 463.8215 (train.py:125, train())
[2021-11-05 12:35:41]    INFO >> Epoch   2/100, Batch 2000/5302, train loss: 458.0796 (train.py:125, train())
[2021-11-05 12:39:56]    INFO >> Epoch   2/100, Batch 2500/5302, train loss: 452.8795 (train.py:125, train())
[2021-11-05 12:43:30]    INFO >> Epoch   2/100, Batch 3000/5302, train loss: 447.2792 (train.py:125, train())
[2021-11-05 12:46:56]    INFO >> Epoch   2/100, Batch 3500/5302, train loss: 442.7012 (train.py:125, train())
[2021-11-05 12:49:32]    INFO >> Epoch   2/100, Batch 4000/5302, train loss: 437.7760 (train.py:125, train())
[2021-11-05 12:52:13]    INFO >> Epoch   2/100, Batch 4500/5302, train loss: 432.5643 (train.py:125, train())
[2021-11-05 12:56:43]    INFO >> Epoch   2/100, Batch 5000/5302, train loss: 427.2896 (train.py:125, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-05 12:59:44]    INFO >> Epoch   2/100, train loss: 423.9635 (train.py:134, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-05 13:06:43]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/codebert/data-mmap/java-python/best_checkpoint.pt (train.py:208, <module>())
[2021-11-05 13:06:43]    INFO >> Epoch   2/100, valid loss: 0.0000, valid bleu4: 12.53, best bleu4: 12.53 (train.py:210, <module>())
[2021-11-05 13:07:14]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/codebert/data-mmap/java-python/last_checkpoint.pt (train.py:215, <module>())
[2021-11-05 13:12:26]    INFO >> Epoch   3/100, Batch 500/5302, train loss: 369.3495 (train.py:125, train())
[2021-11-05 13:16:49]    INFO >> Epoch   3/100, Batch 1000/5302, train loss: 366.9220 (train.py:125, train())
[2021-11-05 13:20:53]    INFO >> Epoch   3/100, Batch 1500/5302, train loss: 361.0884 (train.py:125, train())
[2021-11-05 13:25:28]    INFO >> Epoch   3/100, Batch 2000/5302, train loss: 357.3411 (train.py:125, train())
[2021-11-05 13:29:39]    INFO >> Epoch   3/100, Batch 2500/5302, train loss: 352.8263 (train.py:125, train())
[2021-11-05 13:36:07]    INFO >> Epoch   3/100, Batch 3000/5302, train loss: 349.2290 (train.py:125, train())
[2021-11-05 13:42:49]    INFO >> Epoch   3/100, Batch 3500/5302, train loss: 347.2987 (train.py:125, train())
[2021-11-05 13:48:07]    INFO >> Epoch   3/100, Batch 4000/5302, train loss: 344.5937 (train.py:125, train())
[2021-11-05 13:53:43]    INFO >> Epoch   3/100, Batch 4500/5302, train loss: 342.4089 (train.py:125, train())
[2021-11-05 13:59:11]    INFO >> Epoch   3/100, Batch 5000/5302, train loss: 338.7008 (train.py:125, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-05 14:02:01]    INFO >> Epoch   3/100, train loss: 336.5897 (train.py:134, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-05 14:11:28]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/codebert/data-mmap/java-python/best_checkpoint.pt (train.py:208, <module>())
[2021-11-05 14:11:28]    INFO >> Epoch   3/100, valid loss: 0.0000, valid bleu4: 16.18, best bleu4: 16.18 (train.py:210, <module>())
[2021-11-05 14:12:01]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/codebert/data-mmap/java-python/last_checkpoint.pt (train.py:215, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
Traceback (most recent call last):
  File "/home/wanyao/anaconda3/envs/py37/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wanyao/anaconda3/envs/py37/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/wanyao/yang/naturalcc-dev/run/translation/codebert/train.py", line 132, in <module>
    train_loss = train()
  File "/home/wanyao/yang/naturalcc-dev/run/translation/codebert/train.py", line 107, in train
    logits = model.forward(src_tokens, src_masks, tgt_tokens, tgt_masks)
  File "/home/wanyao/yang/naturalcc-dev/run/translation/codebert/model.py", line 81, in forward
    logits = self.decoder_forward(encoder_output, src_mask, tgt_tokens, tgt_mask)
  File "/home/wanyao/yang/naturalcc-dev/run/translation/codebert/model.py", line 74, in decoder_forward
    memory_key_padding_mask=(1 - src_mask).bool())
  File "/home/wanyao/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wanyao/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/transformer.py", line 234, in forward
    memory_key_padding_mask=memory_key_padding_mask)
  File "/home/wanyao/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wanyao/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/transformer.py", line 364, in forward
    key_padding_mask=tgt_key_padding_mask)[0]
  File "/home/wanyao/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wanyao/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/activation.py", line 987, in forward
    attn_mask=attn_mask)
  File "/home/wanyao/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 4807, in multi_head_attention_forward
    attn_output = torch.bmm(attn_output_weights, v)
RuntimeError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 31.75 GiB total capacity; 5.41 GiB already allocated; 25.75 MiB free; 5.46 GiB reserved in total by PyTorch)
