nohup: ignoring input
Using backend: pytorch
[2021-10-27 14:42:15]    INFO >> Load arguments in /home/wanyao/yang/naturalcc-dev/run/translation/plbart/config/codetrans/csharp-java.yml (train.py:308, cli_main())
[2021-10-27 14:42:15]    INFO >> {'criterion': 'label_smoothed_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 500, 'log_format': 'simple', 'tensorboard_logdir': '', 'memory_efficient_fp16': 0, 'fp16_no_flatten_grads': 1, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'empty_cache_freq': 0, 'task': 'plbart_translation', 'seed': 1234, 'cpu': 0, 'fp16': 0, 'fp16_opt_level': '01', 'bf16': 0, 'memory_efficient_bf16': 0, 'server_ip': '', 'server_port': '', 'amp': 1, 'amp_batch_retries': 2, 'amp_init_scale': '2 ** 7', 'amp_scale_window': None}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 4, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 64, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'append_lang_id': 1}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'pipeline_model_parallel': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1, 'block_momentum': 0.875, 'block_lr': 1, 'use_nbm': 0, 'average_sync': 0}, 'task': {'data': '/mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap', 'source_lang': 'csharp', 'target_lang': 'java', 'load_alignments': 0, 'left_pad_source': 0, 'left_pad_target': 0, 'max_source_positions': 320, 'max_target_positions': 256, 'upsample_primary': 1, 'truncate_source': 1, 'truncate_target': 1, 'append_eos_to_target': 1, 'eval_bleu': 1, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': True, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_args': None, 'eval_bleu_print_samples': 0, 'eval_with_sacrebleu': 1}, 'model': {'arch': 'fairseq_transformer', 'offset_positions_by_padding': 1, 'pooler_dropout': 0.1, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.1, 'encoder_positional_embeddings': 0, 'encoder_learned_pos': 1, 'encoder_max_relative_len': 0, 'encoder_embed_path': 0, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_layers': 6, 'encoder_attention_heads': 12, 'encoder_normalize_before': 0, 'decoder_embed_path': '', 'decoder_positional_embeddings': 0, 'decoder_learned_pos': 1, 'decoder_max_relative_len': 0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_attention_heads': 12, 'decoder_normalize_before': 0, 'no_decoder_final_norm': 0, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.1, 'adaptive_softmax_factor': 0.0, 'share_decoder_input_output_embed': 1, 'decoder_out_embed_bias': 1, 'share_all_embeddings': 1, 'adaptive_input': 0, 'adaptive_input_factor': 0.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': 0, 'tie_adaptive_proj': 0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 1, 'no_scale_embedding': 0, 'no_token_positional_embeddings': 0, 'encoder_dropout_in': 0.1, 'encoder_dropout_out': 0.1, 'decoder_dropout_in': 0.1, 'decoder_dropout_out': 0.1, 'max_source_positions': 1024, 'max_target_positions': 1024, 'multihead_attention_version': 'ncc', 'encoder_position_encoding_version': 'ncc_learned', 'decoder_position_encoding_version': 'ncc_learned'}, 'optimization': {'max_epoch': 100, 'max_update': 0, 'clip_norm': 25.0, 'update_freq': [1], 'lrs': [5e-05], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1500, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'sentence_avg': 0, 'label_smoothing': 0.1, 'adam': {'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': 0}}, 'checkpoint': {'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': 1, 'patience': 10, 'save_dir': '/mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints', 'should_continue': 0, 'model_name_or_path': None, 'cache_dir': None, 'logging_steps': 500, 'save_steps': 2000, 'save_total_limit': 2, 'overwrite_output_dir': 0, 'overwrite_cache': 0, 'init_checkpoint': '/mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt'}, 'eval': {'path': '/mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_best.pt', 'remove_bpe': 'sentencepiece', 'quiet': 1, 'results_path': None, 'model_overrides': '{}', 'topk': 5, 'max_sentences': 2, 'beam': 5, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 500, 'min_len': 1, 'match_source_len': 0, 'no_early_stop': 1, 'unnormalized': 0, 'no_beamable_mm': 0, 'lenpen': 1, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': 0, 'score_reference': 0, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': 0, 'sampling_topk': -1, 'sampling_topp': -1, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 0, 'print_step': 0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': 0, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': 0, 'retain_iter_history': 0, 'decoding_format': None, 'nltk_bleu': 1, 'rouge': 1}} (train.py:310, cli_main())
[2021-10-27 14:42:15]    INFO >> single GPU training... (train.py:339, cli_main())
[2021-10-27 14:42:15]    INFO >> [csharp] dictionary: 50006 types (plbart_translation.py:134, setup_task())
[2021-10-27 14:42:15]    INFO >> [java] dictionary: 50006 types (plbart_translation.py:135, setup_task())
[2021-10-27 14:42:16]    INFO >> truncate csharp/valid.code_tokens to 320 (plbart_translation.py:72, load_langpair_dataset())
[2021-10-27 14:42:16]    INFO >> truncate java/valid.code_tokens to 256 (plbart_translation.py:88, load_langpair_dataset())
[2021-10-27 14:42:21]    INFO >> Restore parameters from /mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt (train.py:226, single_main())
[2021-10-27 14:42:21]    INFO >> FairseqTransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(50006, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(50006, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50006, bias=False)
  )
) (train.py:229, single_main())
[2021-10-27 14:42:21]    INFO >> model fairseq_transformer, criterion LabelSmoothedCrossEntropyCriterion (train.py:230, single_main())
[2021-10-27 14:42:21]    INFO >> num. model params: 139221504 (num. trained: 139221504) (train.py:233, single_main())
[2021-10-27 14:42:27]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:542, pretty_print_cuda_env_list())
[2021-10-27 14:42:27]    INFO >> rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                     (utils.py:548, pretty_print_cuda_env_list())
[2021-10-27 14:42:27]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:550, pretty_print_cuda_env_list())
[2021-10-27 14:42:27]    INFO >> training on 1 GPUs (train.py:238, single_main())
[2021-10-27 14:42:27]    INFO >> max tokens per GPU = None and max sentences per GPU = 4 (train.py:241, single_main())
[2021-10-27 14:42:27]    INFO >> no existing checkpoint found /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())
[2021-10-27 14:42:27]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())
[2021-10-27 14:42:27]    INFO >> truncate csharp/train.code_tokens to 320 (plbart_translation.py:72, load_langpair_dataset())
[2021-10-27 14:42:27]    INFO >> truncate java/train.code_tokens to 256 (plbart_translation.py:88, load_langpair_dataset())
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:57: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
[2021-10-27 14:42:36]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 14:42:36]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 14:42:37]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 14:42:37]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-10-27 14:42:37]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 14:42:37]    INFO >> AMP: overflow detected, setting scale to to 4.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 14:42:37]    INFO >> AMP: overflow detected, setting scale to to 2.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 14:42:37]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-10-27 14:43:40]    INFO >> epoch 001:    502 / 2575 loss=3.967, nll_loss=1.876, ppl=3.67, wps=551.5, ups=7.9, wpb=69.7, bsz=4, num_updates=500, lr=1.7e-05, gnorm=111.294, clip=17, loss_scale=2, train_wall=62, gb_free=29.4, wall=73 (progress_bar.py:262, log())
[2021-10-27 14:44:44]    INFO >> epoch 001:   1002 / 2575 loss=2.795, nll_loss=0.802, ppl=1.74, wps=878.4, ups=7.89, wpb=111.3, bsz=4, num_updates=1000, lr=3.3e-05, gnorm=4.712, clip=0.6, loss_scale=2, train_wall=61, gb_free=29.4, wall=137 (progress_bar.py:262, log())
[2021-10-27 14:45:46]    INFO >> epoch 001:   1502 / 2575 loss=2.649, nll_loss=0.681, ppl=1.6, wps=1337.8, ups=8.04, wpb=166.4, bsz=4, num_updates=1500, lr=5e-05, gnorm=3.104, clip=0, loss_scale=2, train_wall=60, gb_free=29.4, wall=199 (progress_bar.py:262, log())
[2021-10-27 14:46:48]    INFO >> epoch 001:   2002 / 2575 loss=2.362, nll_loss=0.415, ppl=1.33, wps=1344.4, ups=8.01, wpb=167.9, bsz=4, num_updates=2000, lr=5e-05, gnorm=1.543, clip=0, loss_scale=4, train_wall=60, gb_free=29.4, wall=261 (progress_bar.py:262, log())
[2021-10-27 14:47:44]    INFO >> epoch 001:   2502 / 2575 loss=2.606, nll_loss=0.674, ppl=1.6, wps=2969.7, ups=8.93, wpb=332.5, bsz=4, num_updates=2500, lr=5e-05, gnorm=2.169, clip=0, loss_scale=4, train_wall=54, gb_free=28.8, wall=317 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 14:47:53]    INFO >> epoch 001 | loss 2.698 | nll_loss 0.739 | ppl 1.67 | wps 1532.9 | ups 8.13 | wpb 188.4 | bsz 4 | num_updates 2573 | lr 5e-05 | gnorm 23.926 | clip 3.4 | loss_scale 4 | train_wall 306 | gb_free 28 | wall 326 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 14:48:38]    INFO >> epoch 001 | valid on 'valid' subset | loss 2.493 | nll_loss 0.391 | ppl 1.31 | bleu 74.6334 | wps 712.5 | wpb 3127.5 | bsz 62.5 | num_updates 2573 (progress_bar.py:269, print())
[2021-10-27 14:48:44]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_best.pt (epoch 1 @ 2573 updates, score 74.633449) (writing took 5.526139 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 14:49:45]    INFO >> epoch 002:    427 / 2575 loss=2.672, nll_loss=0.748, ppl=1.68, wps=734.9, ups=4.13, wpb=177.9, bsz=4, num_updates=3000, lr=5e-05, gnorm=4.148, clip=0.2, loss_scale=4, train_wall=60, gb_free=29.4, wall=438 (progress_bar.py:262, log())
[2021-10-27 14:50:48]    INFO >> epoch 002:    927 / 2575 loss=2.532, nll_loss=0.603, ppl=1.52, wps=832.3, ups=7.95, wpb=104.7, bsz=4, num_updates=3500, lr=5e-05, gnorm=3.363, clip=0, loss_scale=4, train_wall=61, gb_free=29.4, wall=501 (progress_bar.py:262, log())
[2021-10-27 14:51:51]    INFO >> epoch 002:   1427 / 2575 loss=2.453, nll_loss=0.521, ppl=1.43, wps=1291.8, ups=7.93, wpb=162.8, bsz=4, num_updates=4000, lr=5e-05, gnorm=2.454, clip=0, loss_scale=8, train_wall=61, gb_free=29.4, wall=564 (progress_bar.py:262, log())
[2021-10-27 14:52:53]    INFO >> epoch 002:   1927 / 2575 loss=2.241, nll_loss=0.313, ppl=1.24, wps=1334.3, ups=8.09, wpb=164.9, bsz=4, num_updates=4500, lr=5e-05, gnorm=1.127, clip=0, loss_scale=8, train_wall=60, gb_free=29.4, wall=626 (progress_bar.py:262, log())
[2021-10-27 14:53:55]    INFO >> epoch 002:   2427 / 2575 loss=2.399, nll_loss=0.472, ppl=1.39, wps=2218.6, ups=8.15, wpb=272.3, bsz=4, num_updates=5000, lr=5e-05, gnorm=1.607, clip=0, loss_scale=8, train_wall=59, gb_free=28.5, wall=687 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 14:54:14]    INFO >> epoch 002 | loss 2.44 | nll_loss 0.513 | ppl 1.43 | wps 1273.1 | ups 6.76 | wpb 188.3 | bsz 4 | num_updates 5148 | lr 5e-05 | gnorm 2.511 | clip 0 | loss_scale 8 | train_wall 311 | gb_free 28 | wall 707 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 14:54:59]    INFO >> epoch 002 | valid on 'valid' subset | loss 2.434 | nll_loss 0.354 | ppl 1.28 | bleu 76.155 | wps 681.9 | wpb 3127.5 | bsz 62.5 | num_updates 5148 | best_bleu 76.155 (progress_bar.py:269, print())
[2021-10-27 14:55:12]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_best.pt (epoch 2 @ 5148 updates, score 76.154971) (writing took 12.337816 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 14:56:04]    INFO >> epoch 003:    352 / 2575 loss=2.512, nll_loss=0.588, ppl=1.5, wps=970, ups=3.85, wpb=252, bsz=4, num_updates=5500, lr=5e-05, gnorm=2.726, clip=0.2, loss_scale=8, train_wall=61, gb_free=29.4, wall=817 (progress_bar.py:262, log())
[2021-10-27 14:57:06]    INFO >> epoch 003:    852 / 2575 loss=2.417, nll_loss=0.49, ppl=1.4, wps=805.7, ups=8.06, wpb=99.9, bsz=4, num_updates=6000, lr=5e-05, gnorm=2.833, clip=0.2, loss_scale=16, train_wall=60, gb_free=29.4, wall=879 (progress_bar.py:262, log())
[2021-10-27 14:58:09]    INFO >> epoch 003:   1352 / 2575 loss=2.362, nll_loss=0.427, ppl=1.34, wps=1279.9, ups=8, wpb=159.9, bsz=4, num_updates=6500, lr=5e-05, gnorm=2.46, clip=0.2, loss_scale=16, train_wall=60, gb_free=29.4, wall=942 (progress_bar.py:262, log())
[2021-10-27 14:59:12]    INFO >> epoch 003:   1852 / 2575 loss=2.197, nll_loss=0.273, ppl=1.21, wps=1276, ups=7.92, wpb=161.2, bsz=4, num_updates=7000, lr=5e-05, gnorm=1.079, clip=0, loss_scale=16, train_wall=61, gb_free=29.4, wall=1005 (progress_bar.py:262, log())
[2021-10-27 15:00:05]    INFO >> epoch 003:   2352 / 2575 loss=2.271, nll_loss=0.345, ppl=1.27, wps=2160, ups=9.42, wpb=229.3, bsz=4, num_updates=7500, lr=5e-05, gnorm=1.133, clip=0, loss_scale=16, train_wall=51, gb_free=29.2, wall=1058 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:00:30]    INFO >> epoch 003 | loss 2.345 | nll_loss 0.418 | ppl 1.34 | wps 1291.1 | ups 6.86 | wpb 188.3 | bsz 4 | num_updates 7723 | lr 5e-05 | gnorm 2.038 | clip 0.1 | loss_scale 16 | train_wall 299 | gb_free 28 | wall 1082 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:01:16]    INFO >> epoch 003 | valid on 'valid' subset | loss 2.41 | nll_loss 0.338 | ppl 1.26 | bleu 77.5715 | wps 656.1 | wpb 3127.5 | bsz 62.5 | num_updates 7723 | best_bleu 77.5715 (progress_bar.py:269, print())
[2021-10-27 15:01:29]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_best.pt (epoch 3 @ 7723 updates, score 77.571534) (writing took 12.319255 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 15:02:11]    INFO >> epoch 004:    277 / 2575 loss=2.435, nll_loss=0.508, ppl=1.42, wps=1219.9, ups=3.96, wpb=308, bsz=4, num_updates=8000, lr=5e-05, gnorm=2.42, clip=0.2, loss_scale=32, train_wall=57, gb_free=29.4, wall=1184 (progress_bar.py:262, log())
[2021-10-27 15:03:13]    INFO >> epoch 004:    777 / 2575 loss=2.314, nll_loss=0.383, ppl=1.3, wps=753, ups=8.06, wpb=93.4, bsz=4, num_updates=8500, lr=5e-05, gnorm=2.212, clip=0, loss_scale=32, train_wall=60, gb_free=29.4, wall=1246 (progress_bar.py:262, log())
[2021-10-27 15:04:17]    INFO >> epoch 004:   1277 / 2575 loss=2.291, nll_loss=0.354, ppl=1.28, wps=1158.6, ups=7.87, wpb=147.2, bsz=4, num_updates=9000, lr=5e-05, gnorm=2.325, clip=0.4, loss_scale=32, train_wall=62, gb_free=29.4, wall=1310 (progress_bar.py:262, log())
[2021-10-27 15:05:20]    INFO >> epoch 004:   1777 / 2575 loss=2.183, nll_loss=0.259, ppl=1.2, wps=1282.7, ups=7.93, wpb=161.7, bsz=4, num_updates=9500, lr=5e-05, gnorm=0.997, clip=0, loss_scale=32, train_wall=61, gb_free=29.4, wall=1373 (progress_bar.py:262, log())
[2021-10-27 15:06:22]    INFO >> epoch 004:   2277 / 2575 loss=2.205, nll_loss=0.281, ppl=1.22, wps=1667.9, ups=8.01, wpb=208.3, bsz=4, num_updates=10000, lr=5e-05, gnorm=0.922, clip=0, loss_scale=64, train_wall=60, gb_free=29.3, wall=1435 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:07:01]    INFO >> epoch 004 | loss 2.283 | nll_loss 0.354 | ppl 1.28 | wps 1238.1 | ups 6.58 | wpb 188.3 | bsz 4 | num_updates 10298 | lr 5e-05 | gnorm 1.746 | clip 0.1 | loss_scale 64 | train_wall 314 | gb_free 28 | wall 1474 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:07:46]    INFO >> epoch 004 | valid on 'valid' subset | loss 2.386 | nll_loss 0.331 | ppl 1.26 | bleu 77.532 | wps 700 | wpb 3127.5 | bsz 62.5 | num_updates 10298 | best_bleu 77.5715 (progress_bar.py:269, print())
[2021-10-27 15:07:53]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 4 @ 10298 updates, score 77.531957) (writing took 7.326476 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 15:08:27]    INFO >> epoch 005:    202 / 2575 loss=2.357, nll_loss=0.426, ppl=1.34, wps=1400.2, ups=4.02, wpb=348.2, bsz=4, num_updates=10500, lr=5e-05, gnorm=1.925, clip=0.2, loss_scale=64, train_wall=62, gb_free=29.4, wall=1560 (progress_bar.py:262, log())
[2021-10-27 15:09:29]    INFO >> epoch 005:    702 / 2575 loss=2.265, nll_loss=0.333, ppl=1.26, wps=708.7, ups=8.08, wpb=87.7, bsz=4, num_updates=11000, lr=5e-05, gnorm=1.962, clip=0.2, loss_scale=64, train_wall=60, gb_free=29.4, wall=1621 (progress_bar.py:262, log())
[2021-10-27 15:10:31]    INFO >> epoch 005:   1202 / 2575 loss=2.234, nll_loss=0.299, ppl=1.23, wps=1094.4, ups=8.05, wpb=135.9, bsz=4, num_updates=11500, lr=4.9e-05, gnorm=1.674, clip=0, loss_scale=64, train_wall=60, gb_free=29.4, wall=1684 (progress_bar.py:262, log())
[2021-10-27 15:11:33]    INFO >> epoch 005:   1702 / 2575 loss=2.179, nll_loss=0.254, ppl=1.19, wps=1325.1, ups=7.98, wpb=166, bsz=4, num_updates=12000, lr=4.9e-05, gnorm=1.002, clip=0, loss_scale=128, train_wall=61, gb_free=29.4, wall=1746 (progress_bar.py:262, log())
[2021-10-27 15:12:36]    INFO >> epoch 005:   2202 / 2575 loss=2.152, nll_loss=0.232, ppl=1.17, wps=1525.3, ups=8.05, wpb=189.5, bsz=4, num_updates=12500, lr=4.9e-05, gnorm=0.612, clip=0, loss_scale=128, train_wall=60, gb_free=29.3, wall=1808 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:13:16]    INFO >> epoch 005 | loss 2.236 | nll_loss 0.307 | ppl 1.24 | wps 1292.1 | ups 6.86 | wpb 188.3 | bsz 4 | num_updates 12873 | lr 4.9e-05 | gnorm 1.397 | clip 0.1 | loss_scale 128 | train_wall 304 | gb_free 28 | wall 1849 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:13:59]    INFO >> epoch 005 | valid on 'valid' subset | loss 2.372 | nll_loss 0.333 | ppl 1.26 | bleu 77.2118 | wps 743.6 | wpb 3127.5 | bsz 62.5 | num_updates 12873 | best_bleu 77.5715 (progress_bar.py:269, print())
[2021-10-27 15:14:06]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 5 @ 12873 updates, score 77.21178) (writing took 7.011920 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 15:14:31]    INFO >> epoch 006:    127 / 2575 loss=2.293, nll_loss=0.36, ppl=1.28, wps=1657.1, ups=4.35, wpb=381.2, bsz=4, num_updates=13000, lr=4.9e-05, gnorm=1.344, clip=0, loss_scale=128, train_wall=55, gb_free=29.4, wall=1923 (progress_bar.py:262, log())
[2021-10-27 15:15:33]    INFO >> epoch 006:    627 / 2575 loss=2.227, nll_loss=0.298, ppl=1.23, wps=653.8, ups=8.03, wpb=81.4, bsz=4, num_updates=13500, lr=4.9e-05, gnorm=1.88, clip=0.2, loss_scale=128, train_wall=60, gb_free=29.4, wall=1986 (progress_bar.py:262, log())
[2021-10-27 15:16:35]    INFO >> epoch 006:   1127 / 2575 loss=2.191, nll_loss=0.257, ppl=1.2, wps=1007.5, ups=8.02, wpb=125.7, bsz=4, num_updates=14000, lr=4.9e-05, gnorm=1.443, clip=0.2, loss_scale=256, train_wall=60, gb_free=29.4, wall=2048 (progress_bar.py:262, log())
[2021-10-27 15:17:38]    INFO >> epoch 006:   1627 / 2575 loss=2.162, nll_loss=0.235, ppl=1.18, wps=1349.3, ups=7.9, wpb=170.7, bsz=4, num_updates=14500, lr=4.9e-05, gnorm=0.868, clip=0, loss_scale=256, train_wall=61, gb_free=29.4, wall=2111 (progress_bar.py:262, log())
[2021-10-27 15:18:41]    INFO >> epoch 006:   2127 / 2575 loss=2.117, nll_loss=0.2, ppl=1.15, wps=1399.5, ups=7.98, wpb=175.4, bsz=4, num_updates=15000, lr=4.9e-05, gnorm=0.404, clip=0, loss_scale=256, train_wall=61, gb_free=29.4, wall=2174 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:19:39]    INFO >> epoch 006 | loss 2.198 | nll_loss 0.27 | ppl 1.21 | wps 1266.9 | ups 6.73 | wpb 188.3 | bsz 4 | num_updates 15448 | lr 4.9e-05 | gnorm 1.139 | clip 0.1 | loss_scale 256 | train_wall 314 | gb_free 28 | wall 2232 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:20:24]    INFO >> epoch 006 | valid on 'valid' subset | loss 2.358 | nll_loss 0.328 | ppl 1.26 | bleu 77.8961 | wps 680.6 | wpb 3127.5 | bsz 62.5 | num_updates 15448 | best_bleu 77.8961 (progress_bar.py:269, print())
[2021-10-27 15:20:36]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_best.pt (epoch 6 @ 15448 updates, score 77.896124) (writing took 12.033210 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 15:20:51]    INFO >> epoch 007:     52 / 2575 loss=2.245, nll_loss=0.312, ppl=1.24, wps=1569.9, ups=3.84, wpb=408.5, bsz=4, num_updates=15500, lr=4.9e-05, gnorm=0.996, clip=0, loss_scale=256, train_wall=62, gb_free=29.4, wall=2304 (progress_bar.py:262, log())
[2021-10-27 15:21:53]    INFO >> epoch 007:    552 / 2575 loss=2.188, nll_loss=0.261, ppl=1.2, wps=599.5, ups=8.03, wpb=74.7, bsz=4, num_updates=16000, lr=4.9e-05, gnorm=1.451, clip=0.2, loss_scale=512, train_wall=60, gb_free=29.4, wall=2366 (progress_bar.py:262, log())
[2021-10-27 15:22:56]    INFO >> epoch 007:   1052 / 2575 loss=2.153, nll_loss=0.223, ppl=1.17, wps=928.1, ups=7.97, wpb=116.4, bsz=4, num_updates=16500, lr=4.9e-05, gnorm=1.143, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=2429 (progress_bar.py:262, log())
[2021-10-27 15:24:00]    INFO >> epoch 007:   1552 / 2575 loss=2.152, nll_loss=0.224, ppl=1.17, wps=1352.5, ups=7.85, wpb=172.4, bsz=4, num_updates=17000, lr=4.9e-05, gnorm=0.842, clip=0, loss_scale=512, train_wall=62, gb_free=29.4, wall=2493 (progress_bar.py:262, log())
[2021-10-27 15:25:02]    INFO >> epoch 007:   2052 / 2575 loss=2.104, nll_loss=0.19, ppl=1.14, wps=1361.7, ups=8, wpb=170.1, bsz=4, num_updates=17500, lr=4.9e-05, gnorm=0.342, clip=0, loss_scale=512, train_wall=60, gb_free=29.3, wall=2555 (progress_bar.py:262, log())
[2021-10-27 15:26:06]    INFO >> epoch 007:   2552 / 2575 loss=2.205, nll_loss=0.273, ppl=1.21, wps=3052, ups=7.89, wpb=386.6, bsz=4, num_updates=18000, lr=4.9e-05, gnorm=0.76, clip=0, loss_scale=1024, train_wall=61, gb_free=28.2, wall=2619 (progress_bar.py:262, log())
[2021-10-27 15:26:07]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 15:26:08]    INFO >> AMP: overflow detected, setting scale to to 256.0 (amp_optimizer.py:66, clip_grad_norm())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:26:10]    INFO >> epoch 007 | loss 2.174 | nll_loss 0.246 | ppl 1.19 | wps 1241.2 | ups 6.59 | wpb 188.3 | bsz 4 | num_updates 18023 | lr 4.9e-05 | gnorm 0.917 | clip 0 | loss_scale 256 | train_wall 314 | gb_free 28 | wall 2622 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:26:56]    INFO >> epoch 007 | valid on 'valid' subset | loss 2.351 | nll_loss 0.327 | ppl 1.25 | bleu 78.3581 | wps 654.4 | wpb 3127.5 | bsz 62.5 | num_updates 18023 | best_bleu 78.3581 (progress_bar.py:269, print())
[2021-10-27 15:27:08]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_best.pt (epoch 7 @ 18023 updates, score 78.358075) (writing took 12.116982 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 15:28:15]    INFO >> epoch 008:    477 / 2575 loss=2.211, nll_loss=0.283, ppl=1.22, wps=422.4, ups=3.85, wpb=109.6, bsz=4, num_updates=18500, lr=4.9e-05, gnorm=1.248, clip=0, loss_scale=256, train_wall=61, gb_free=29.4, wall=2748 (progress_bar.py:262, log())
[2021-10-27 15:29:18]    INFO >> epoch 008:    977 / 2575 loss=2.149, nll_loss=0.22, ppl=1.16, wps=879.1, ups=8.01, wpb=109.8, bsz=4, num_updates=19000, lr=4.9e-05, gnorm=0.971, clip=0, loss_scale=256, train_wall=60, gb_free=29.4, wall=2811 (progress_bar.py:262, log())
[2021-10-27 15:30:20]    INFO >> epoch 008:   1477 / 2575 loss=2.145, nll_loss=0.219, ppl=1.16, wps=1305.4, ups=8, wpb=163.2, bsz=4, num_updates=19500, lr=4.9e-05, gnorm=0.818, clip=0, loss_scale=256, train_wall=61, gb_free=29.4, wall=2873 (progress_bar.py:262, log())
[2021-10-27 15:31:23]    INFO >> epoch 008:   1977 / 2575 loss=2.099, nll_loss=0.185, ppl=1.14, wps=1335.2, ups=8.01, wpb=166.6, bsz=4, num_updates=20000, lr=4.9e-05, gnorm=0.373, clip=0, loss_scale=256, train_wall=60, gb_free=29.4, wall=2936 (progress_bar.py:262, log())
[2021-10-27 15:32:25]    INFO >> epoch 008:   2477 / 2575 loss=2.173, nll_loss=0.244, ppl=1.18, wps=2494.5, ups=7.99, wpb=312.2, bsz=4, num_updates=20500, lr=4.9e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=61, gb_free=29, wall=2998 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:32:39]    INFO >> epoch 008 | loss 2.167 | nll_loss 0.24 | ppl 1.18 | wps 1245.3 | ups 6.61 | wpb 188.3 | bsz 4 | num_updates 20598 | lr 4.9e-05 | gnorm 0.82 | clip 0 | loss_scale 512 | train_wall 312 | gb_free 28 | wall 3012 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:33:29]    INFO >> epoch 008 | valid on 'valid' subset | loss 2.349 | nll_loss 0.333 | ppl 1.26 | bleu 78.4448 | wps 602.4 | wpb 3127.5 | bsz 62.5 | num_updates 20598 | best_bleu 78.4448 (progress_bar.py:269, print())
[2021-10-27 15:33:43]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_best.pt (epoch 8 @ 20598 updates, score 78.444751) (writing took 14.601486 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 15:34:41]    INFO >> epoch 009:    402 / 2575 loss=2.234, nll_loss=0.303, ppl=1.23, wps=751.9, ups=3.68, wpb=204.6, bsz=4, num_updates=21000, lr=4.9e-05, gnorm=1.102, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=3134 (progress_bar.py:262, log())
[2021-10-27 15:35:44]    INFO >> epoch 009:    902 / 2575 loss=2.138, nll_loss=0.211, ppl=1.16, wps=825, ups=7.99, wpb=103.3, bsz=4, num_updates=21500, lr=4.9e-05, gnorm=0.891, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=3197 (progress_bar.py:262, log())
[2021-10-27 15:36:46]    INFO >> epoch 009:   1402 / 2575 loss=2.141, nll_loss=0.213, ppl=1.16, wps=1313.9, ups=8.07, wpb=162.7, bsz=4, num_updates=22000, lr=4.9e-05, gnorm=0.827, clip=0, loss_scale=512, train_wall=60, gb_free=29.4, wall=3259 (progress_bar.py:262, log())
[2021-10-27 15:37:48]    INFO >> epoch 009:   1902 / 2575 loss=2.094, nll_loss=0.181, ppl=1.13, wps=1312, ups=8.03, wpb=163.3, bsz=4, num_updates=22500, lr=4.9e-05, gnorm=0.323, clip=0, loss_scale=1024, train_wall=60, gb_free=29.4, wall=3321 (progress_bar.py:262, log())
[2021-10-27 15:38:51]    INFO >> epoch 009:   2402 / 2575 loss=2.14, nll_loss=0.216, ppl=1.16, wps=2033.6, ups=7.95, wpb=255.8, bsz=4, num_updates=23000, lr=4.9e-05, gnorm=0.512, clip=0, loss_scale=1024, train_wall=61, gb_free=29.2, wall=3384 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:39:14]    INFO >> epoch 009 | loss 2.156 | nll_loss 0.23 | ppl 1.17 | wps 1227.8 | ups 6.52 | wpb 188.3 | bsz 4 | num_updates 23173 | lr 4.9e-05 | gnorm 0.733 | clip 0 | loss_scale 1024 | train_wall 312 | gb_free 28 | wall 3406 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:39:58]    INFO >> epoch 009 | valid on 'valid' subset | loss 2.35 | nll_loss 0.328 | ppl 1.26 | bleu 78.5 | wps 693.9 | wpb 3127.5 | bsz 62.5 | num_updates 23173 | best_bleu 78.5 (progress_bar.py:269, print())
[2021-10-27 15:40:10]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_best.pt (epoch 9 @ 23173 updates, score 78.500026) (writing took 12.312706 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 15:40:59]    INFO >> epoch 010:    327 / 2575 loss=2.222, nll_loss=0.29, ppl=1.22, wps=1061.9, ups=3.9, wpb=272.4, bsz=4, num_updates=23500, lr=4.9e-05, gnorm=0.984, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=3512 (progress_bar.py:262, log())
[2021-10-27 15:41:55]    INFO >> epoch 010:    827 / 2575 loss=2.128, nll_loss=0.204, ppl=1.15, wps=874.8, ups=8.95, wpb=97.7, bsz=4, num_updates=24000, lr=4.9e-05, gnorm=0.807, clip=0, loss_scale=1024, train_wall=54, gb_free=29.4, wall=3568 (progress_bar.py:262, log())
[2021-10-27 15:42:57]    INFO >> epoch 010:   1327 / 2575 loss=2.131, nll_loss=0.202, ppl=1.15, wps=1250.8, ups=8.05, wpb=155.4, bsz=4, num_updates=24500, lr=4.9e-05, gnorm=0.728, clip=0, loss_scale=2048, train_wall=60, gb_free=29.4, wall=3630 (progress_bar.py:262, log())
[2021-10-27 15:43:56]    INFO >> epoch 010:   1827 / 2575 loss=2.092, nll_loss=0.18, ppl=1.13, wps=1370.7, ups=8.6, wpb=159.4, bsz=4, num_updates=25000, lr=4.9e-05, gnorm=0.346, clip=0, loss_scale=2048, train_wall=56, gb_free=29.4, wall=3688 (progress_bar.py:262, log())
[2021-10-27 15:44:54]    INFO >> epoch 010:   2327 / 2575 loss=2.119, nll_loss=0.199, ppl=1.15, wps=1909.8, ups=8.57, wpb=222.9, bsz=4, num_updates=25500, lr=4.9e-05, gnorm=0.418, clip=0, loss_scale=2048, train_wall=56, gb_free=29.3, wall=3747 (progress_bar.py:262, log())
[2021-10-27 15:45:24]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 15:45:24]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:45:26]    INFO >> epoch 010 | loss 2.15 | nll_loss 0.225 | ppl 1.17 | wps 1297.9 | ups 6.91 | wpb 187.9 | bsz 4 | num_updates 25747 | lr 4.9e-05 | gnorm 0.659 | clip 0 | loss_scale 1024 | train_wall 297 | gb_free 28 | wall 3779 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:46:08]    INFO >> epoch 010 | valid on 'valid' subset | loss 2.351 | nll_loss 0.33 | ppl 1.26 | bleu 78.6035 | wps 757.3 | wpb 3127.5 | bsz 62.5 | num_updates 25747 | best_bleu 78.6035 (progress_bar.py:269, print())
[2021-10-27 15:46:20]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_best.pt (epoch 10 @ 25747 updates, score 78.603498) (writing took 12.020616 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 15:47:00]    INFO >> epoch 011:    253 / 2575 loss=2.216, nll_loss=0.284, ppl=1.22, wps=1269.3, ups=3.95, wpb=321.2, bsz=4, num_updates=26000, lr=4.9e-05, gnorm=0.937, clip=0, loss_scale=1024, train_wall=62, gb_free=29.4, wall=3873 (progress_bar.py:262, log())
[2021-10-27 15:48:02]    INFO >> epoch 011:    753 / 2575 loss=2.124, nll_loss=0.201, ppl=1.15, wps=740.7, ups=8.08, wpb=91.7, bsz=4, num_updates=26500, lr=4.9e-05, gnorm=0.767, clip=0, loss_scale=1024, train_wall=60, gb_free=29.4, wall=3935 (progress_bar.py:262, log())
[2021-10-27 15:49:04]    INFO >> epoch 011:   1253 / 2575 loss=2.125, nll_loss=0.199, ppl=1.15, wps=1154.2, ups=8.07, wpb=143.1, bsz=4, num_updates=27000, lr=4.9e-05, gnorm=0.673, clip=0, loss_scale=1024, train_wall=60, gb_free=29.4, wall=3997 (progress_bar.py:262, log())
[2021-10-27 15:50:07]    INFO >> epoch 011:   1753 / 2575 loss=2.1, nll_loss=0.186, ppl=1.14, wps=1293.4, ups=7.99, wpb=162, bsz=4, num_updates=27500, lr=4.9e-05, gnorm=0.39, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=4060 (progress_bar.py:262, log())
[2021-10-27 15:51:10]    INFO >> epoch 011:   2253 / 2575 loss=2.112, nll_loss=0.194, ppl=1.14, wps=1632.3, ups=7.96, wpb=205, bsz=4, num_updates=28000, lr=4.9e-05, gnorm=0.392, clip=0, loss_scale=2048, train_wall=61, gb_free=29.3, wall=4122 (progress_bar.py:262, log())
[2021-10-27 15:51:40]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 15:51:40]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:51:52]    INFO >> epoch 011 | loss 2.151 | nll_loss 0.227 | ppl 1.17 | wps 1258.6 | ups 6.69 | wpb 188.3 | bsz 4 | num_updates 28322 | lr 4.9e-05 | gnorm 0.638 | clip 0 | loss_scale 512 | train_wall 312 | gb_free 28 | wall 4164 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:52:38]    INFO >> epoch 011 | valid on 'valid' subset | loss 2.349 | nll_loss 0.33 | ppl 1.26 | bleu 78.5137 | wps 669.2 | wpb 3127.5 | bsz 62.5 | num_updates 28322 | best_bleu 78.6035 (progress_bar.py:269, print())
[2021-10-27 15:52:45]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 11 @ 28322 updates, score 78.513693) (writing took 7.279054 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 15:53:15]    INFO >> epoch 012:    178 / 2575 loss=2.214, nll_loss=0.283, ppl=1.22, wps=1423.4, ups=3.98, wpb=357.5, bsz=4, num_updates=28500, lr=4.9e-05, gnorm=0.954, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=4248 (progress_bar.py:262, log())
[2021-10-27 15:54:17]    INFO >> epoch 012:    678 / 2575 loss=2.128, nll_loss=0.207, ppl=1.15, wps=689.8, ups=8.03, wpb=85.9, bsz=4, num_updates=29000, lr=4.9e-05, gnorm=0.861, clip=0, loss_scale=512, train_wall=60, gb_free=29.4, wall=4310 (progress_bar.py:262, log())
[2021-10-27 15:55:20]    INFO >> epoch 012:   1178 / 2575 loss=2.124, nll_loss=0.199, ppl=1.15, wps=1063.3, ups=8.01, wpb=132.8, bsz=4, num_updates=29500, lr=4.9e-05, gnorm=0.726, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=4373 (progress_bar.py:262, log())
[2021-10-27 15:56:22]    INFO >> epoch 012:   1678 / 2575 loss=2.111, nll_loss=0.193, ppl=1.14, wps=1337.5, ups=8, wpb=167.3, bsz=4, num_updates=30000, lr=4.9e-05, gnorm=0.516, clip=0, loss_scale=512, train_wall=60, gb_free=29.4, wall=4435 (progress_bar.py:262, log())
[2021-10-27 15:57:25]    INFO >> epoch 012:   2178 / 2575 loss=2.105, nll_loss=0.19, ppl=1.14, wps=1497.9, ups=8, wpb=187.2, bsz=4, num_updates=30500, lr=4.9e-05, gnorm=0.366, clip=0, loss_scale=1024, train_wall=60, gb_free=29.3, wall=4498 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:58:16]    INFO >> epoch 012 | loss 2.152 | nll_loss 0.228 | ppl 1.17 | wps 1262.3 | ups 6.7 | wpb 188.3 | bsz 4 | num_updates 30897 | lr 4.9e-05 | gnorm 0.684 | clip 0 | loss_scale 1024 | train_wall 312 | gb_free 28 | wall 4548 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 15:58:58]    INFO >> epoch 012 | valid on 'valid' subset | loss 2.352 | nll_loss 0.333 | ppl 1.26 | bleu 78.3409 | wps 742.3 | wpb 3127.5 | bsz 62.5 | num_updates 30897 | best_bleu 78.6035 (progress_bar.py:269, print())
[2021-10-27 15:59:05]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 12 @ 30897 updates, score 78.340884) (writing took 7.044201 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 15:59:27]    INFO >> epoch 013:    103 / 2575 loss=2.207, nll_loss=0.277, ppl=1.21, wps=1592.6, ups=4.11, wpb=387.5, bsz=4, num_updates=31000, lr=4.9e-05, gnorm=0.835, clip=0, loss_scale=1024, train_wall=62, gb_free=29.4, wall=4619 (progress_bar.py:262, log())
[2021-10-27 16:00:29]    INFO >> epoch 013:    603 / 2575 loss=2.126, nll_loss=0.205, ppl=1.15, wps=641.4, ups=8.07, wpb=79.5, bsz=4, num_updates=31500, lr=4.8e-05, gnorm=0.878, clip=0, loss_scale=1024, train_wall=60, gb_free=29.4, wall=4681 (progress_bar.py:262, log())
[2021-10-27 16:01:18]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 16:01:18]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-10-27 16:01:21]    INFO >> epoch 013:   1104 / 2575 loss=2.117, nll_loss=0.193, ppl=1.14, wps=1173.6, ups=9.61, wpb=122.2, bsz=4, num_updates=32000, lr=4.8e-05, gnorm=0.701, clip=0, loss_scale=512, train_wall=50, gb_free=29.4, wall=4733 (progress_bar.py:262, log())
[2021-10-27 16:02:23]    INFO >> epoch 013:   1604 / 2575 loss=2.118, nll_loss=0.197, ppl=1.15, wps=1390, ups=8.08, wpb=172, bsz=4, num_updates=32500, lr=4.8e-05, gnorm=0.581, clip=0, loss_scale=512, train_wall=60, gb_free=29.4, wall=4795 (progress_bar.py:262, log())
[2021-10-27 16:03:25]    INFO >> epoch 013:   2104 / 2575 loss=2.091, nll_loss=0.18, ppl=1.13, wps=1375.3, ups=8, wpb=171.9, bsz=4, num_updates=33000, lr=4.8e-05, gnorm=0.305, clip=0, loss_scale=512, train_wall=60, gb_free=29.4, wall=4858 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:04:25]    INFO >> epoch 013 | loss 2.151 | nll_loss 0.228 | ppl 1.17 | wps 1311.8 | ups 6.97 | wpb 188.2 | bsz 4 | num_updates 33471 | lr 4.8e-05 | gnorm 0.665 | clip 0 | loss_scale 512 | train_wall 301 | gb_free 28 | wall 4918 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:05:08]    INFO >> epoch 013 | valid on 'valid' subset | loss 2.349 | nll_loss 0.332 | ppl 1.26 | bleu 78.685 | wps 715.6 | wpb 3127.5 | bsz 62.5 | num_updates 33471 | best_bleu 78.685 (progress_bar.py:269, print())
[2021-10-27 16:05:20]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_best.pt (epoch 13 @ 33471 updates, score 78.684993) (writing took 12.013863 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 16:05:33]    INFO >> epoch 014:     29 / 2575 loss=2.204, nll_loss=0.275, ppl=1.21, wps=1628.4, ups=3.91, wpb=416.2, bsz=4, num_updates=33500, lr=4.8e-05, gnorm=0.837, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=4986 (progress_bar.py:262, log())
[2021-10-27 16:06:28]    INFO >> epoch 014:    529 / 2575 loss=2.128, nll_loss=0.21, ppl=1.16, wps=658.2, ups=9.11, wpb=72.3, bsz=4, num_updates=34000, lr=4.8e-05, gnorm=0.96, clip=0, loss_scale=1024, train_wall=53, gb_free=29.4, wall=5040 (progress_bar.py:262, log())
[2021-10-27 16:07:27]    INFO >> epoch 014:   1029 / 2575 loss=2.117, nll_loss=0.194, ppl=1.14, wps=970.9, ups=8.5, wpb=114.2, bsz=4, num_updates=34500, lr=4.8e-05, gnorm=0.701, clip=0, loss_scale=1024, train_wall=57, gb_free=29.4, wall=5099 (progress_bar.py:262, log())
[2021-10-27 16:08:29]    INFO >> epoch 014:   1529 / 2575 loss=2.118, nll_loss=0.196, ppl=1.15, wps=1350.3, ups=7.99, wpb=169, bsz=4, num_updates=35000, lr=4.8e-05, gnorm=0.596, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=5162 (progress_bar.py:262, log())
[2021-10-27 16:09:32]    INFO >> epoch 014:   2029 / 2575 loss=2.084, nll_loss=0.175, ppl=1.13, wps=1319.7, ups=7.89, wpb=167.2, bsz=4, num_updates=35500, lr=4.8e-05, gnorm=0.287, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=5225 (progress_bar.py:262, log())
[2021-10-27 16:10:35]    INFO >> epoch 014:   2529 / 2575 loss=2.176, nll_loss=0.248, ppl=1.19, wps=2883.6, ups=7.96, wpb=362.1, bsz=4, num_updates=36000, lr=4.8e-05, gnorm=0.708, clip=0, loss_scale=2048, train_wall=61, gb_free=28.3, wall=5288 (progress_bar.py:262, log())
[2021-10-27 16:10:37]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:10:42]    INFO >> epoch 014 | loss 2.147 | nll_loss 0.224 | ppl 1.17 | wps 1285.5 | ups 6.83 | wpb 188.3 | bsz 4 | num_updates 36046 | lr 4.8e-05 | gnorm 0.66 | clip 0 | loss_scale 1024 | train_wall 302 | gb_free 28 | wall 5295 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:11:26]    INFO >> epoch 014 | valid on 'valid' subset | loss 2.354 | nll_loss 0.335 | ppl 1.26 | bleu 78.49 | wps 710.8 | wpb 3127.5 | bsz 62.5 | num_updates 36046 | best_bleu 78.685 (progress_bar.py:269, print())
[2021-10-27 16:11:33]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 14 @ 36046 updates, score 78.489995) (writing took 7.181901 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 16:12:38]    INFO >> epoch 015:    454 / 2575 loss=2.203, nll_loss=0.277, ppl=1.21, wps=584.4, ups=4.07, wpb=143.4, bsz=4, num_updates=36500, lr=4.8e-05, gnorm=0.913, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=5411 (progress_bar.py:262, log())
[2021-10-27 16:13:40]    INFO >> epoch 015:    954 / 2575 loss=2.117, nll_loss=0.195, ppl=1.15, wps=861.2, ups=8.04, wpb=107.1, bsz=4, num_updates=37000, lr=4.8e-05, gnorm=0.734, clip=0, loss_scale=1024, train_wall=60, gb_free=29.4, wall=5473 (progress_bar.py:262, log())
[2021-10-27 16:14:42]    INFO >> epoch 015:   1454 / 2575 loss=2.118, nll_loss=0.196, ppl=1.15, wps=1310.3, ups=8.07, wpb=162.4, bsz=4, num_updates=37500, lr=4.8e-05, gnorm=0.667, clip=0, loss_scale=1024, train_wall=60, gb_free=29.4, wall=5535 (progress_bar.py:262, log())
[2021-10-27 16:15:40]    INFO >> epoch 015:   1954 / 2575 loss=2.088, nll_loss=0.178, ppl=1.13, wps=1439.8, ups=8.64, wpb=166.7, bsz=4, num_updates=38000, lr=4.8e-05, gnorm=0.321, clip=0, loss_scale=1024, train_wall=56, gb_free=29.4, wall=5593 (progress_bar.py:262, log())
[2021-10-27 16:16:38]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 16:16:44]    INFO >> epoch 015:   2454 / 2575 loss=2.15, nll_loss=0.226, ppl=1.17, wps=2279.4, ups=7.8, wpb=292.4, bsz=4, num_updates=38500, lr=4.8e-05, gnorm=0.596, clip=0, loss_scale=1024, train_wall=62, gb_free=29.1, wall=5657 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:17:00]    INFO >> epoch 015 | loss 2.147 | nll_loss 0.225 | ppl 1.17 | wps 1281.5 | ups 6.81 | wpb 188.3 | bsz 4 | num_updates 38621 | lr 4.8e-05 | gnorm 0.659 | clip 0 | loss_scale 1024 | train_wall 309 | gb_free 28 | wall 5673 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:17:43]    INFO >> epoch 015 | valid on 'valid' subset | loss 2.352 | nll_loss 0.338 | ppl 1.26 | bleu 78.1872 | wps 735 | wpb 3127.5 | bsz 62.5 | num_updates 38621 | best_bleu 78.685 (progress_bar.py:269, print())
[2021-10-27 16:17:51]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 15 @ 38621 updates, score 78.187153) (writing took 7.136061 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 16:18:45]    INFO >> epoch 016:    379 / 2575 loss=2.223, nll_loss=0.295, ppl=1.23, wps=946.3, ups=4.15, wpb=227.9, bsz=4, num_updates=39000, lr=4.8e-05, gnorm=0.949, clip=0, loss_scale=1024, train_wall=60, gb_free=29.4, wall=5777 (progress_bar.py:262, log())
[2021-10-27 16:19:47]    INFO >> epoch 016:    879 / 2575 loss=2.118, nll_loss=0.197, ppl=1.15, wps=809.9, ups=8, wpb=101.3, bsz=4, num_updates=39500, lr=4.8e-05, gnorm=0.747, clip=0, loss_scale=1024, train_wall=60, gb_free=29.4, wall=5840 (progress_bar.py:262, log())
[2021-10-27 16:20:50]    INFO >> epoch 016:   1379 / 2575 loss=2.125, nll_loss=0.201, ppl=1.15, wps=1296.6, ups=7.97, wpb=162.7, bsz=4, num_updates=40000, lr=4.8e-05, gnorm=0.718, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=5903 (progress_bar.py:262, log())
[2021-10-27 16:21:53]    INFO >> epoch 016:   1879 / 2575 loss=2.088, nll_loss=0.178, ppl=1.13, wps=1280, ups=7.91, wpb=161.9, bsz=4, num_updates=40500, lr=4.8e-05, gnorm=0.326, clip=0, loss_scale=2048, train_wall=61, gb_free=29.4, wall=5966 (progress_bar.py:262, log())
[2021-10-27 16:22:56]    INFO >> epoch 016:   2379 / 2575 loss=2.13, nll_loss=0.211, ppl=1.16, wps=1922.1, ups=7.94, wpb=242, bsz=4, num_updates=41000, lr=4.8e-05, gnorm=0.511, clip=0, loss_scale=2048, train_wall=61, gb_free=29.2, wall=6029 (progress_bar.py:262, log())
[2021-10-27 16:23:16]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 16:23:16]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:23:22]    INFO >> epoch 016 | loss 2.149 | nll_loss 0.227 | ppl 1.17 | wps 1268.5 | ups 6.75 | wpb 188 | bsz 4 | num_updates 41195 | lr 4.8e-05 | gnorm 0.662 | clip 0 | loss_scale 1024 | train_wall 312 | gb_free 28 | wall 6055 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:24:04]    INFO >> epoch 016 | valid on 'valid' subset | loss 2.354 | nll_loss 0.34 | ppl 1.27 | bleu 77.8489 | wps 749.6 | wpb 3127.5 | bsz 62.5 | num_updates 41195 | best_bleu 78.685 (progress_bar.py:269, print())
[2021-10-27 16:24:11]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 16 @ 41195 updates, score 77.848897) (writing took 7.204700 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 16:24:58]    INFO >> epoch 017:    305 / 2575 loss=2.224, nll_loss=0.295, ppl=1.23, wps=1186, ups=4.12, wpb=288.2, bsz=4, num_updates=41500, lr=4.8e-05, gnorm=0.988, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=6150 (progress_bar.py:262, log())
[2021-10-27 16:26:01]    INFO >> epoch 017:    805 / 2575 loss=2.118, nll_loss=0.198, ppl=1.15, wps=757, ups=7.92, wpb=95.5, bsz=4, num_updates=42000, lr=4.8e-05, gnorm=0.813, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=6213 (progress_bar.py:262, log())
[2021-10-27 16:27:03]    INFO >> epoch 017:   1305 / 2575 loss=2.123, nll_loss=0.199, ppl=1.15, wps=1214.1, ups=7.98, wpb=152.1, bsz=4, num_updates=42500, lr=4.8e-05, gnorm=0.751, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=6276 (progress_bar.py:262, log())
[2021-10-27 16:28:06]    INFO >> epoch 017:   1805 / 2575 loss=2.092, nll_loss=0.18, ppl=1.13, wps=1292.1, ups=8.02, wpb=161.2, bsz=4, num_updates=43000, lr=4.8e-05, gnorm=0.368, clip=0, loss_scale=1024, train_wall=60, gb_free=29.4, wall=6338 (progress_bar.py:262, log())
[2021-10-27 16:29:05]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 16:29:05]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 16:29:08]    INFO >> epoch 017:   2305 / 2575 loss=2.117, nll_loss=0.201, ppl=1.15, wps=1724.8, ups=8.02, wpb=215.1, bsz=4, num_updates=43500, lr=4.8e-05, gnorm=0.47, clip=0, loss_scale=512, train_wall=60, gb_free=29.3, wall=6401 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:29:43]    INFO >> epoch 017 | loss 2.154 | nll_loss 0.232 | ppl 1.17 | wps 1272.1 | ups 6.76 | wpb 188.3 | bsz 4 | num_updates 43770 | lr 4.8e-05 | gnorm 0.696 | clip 0 | loss_scale 512 | train_wall 313 | gb_free 28 | wall 6436 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:30:25]    INFO >> epoch 017 | valid on 'valid' subset | loss 2.358 | nll_loss 0.339 | ppl 1.26 | bleu 78.3854 | wps 746 | wpb 3127.5 | bsz 62.5 | num_updates 43770 | best_bleu 78.685 (progress_bar.py:269, print())
[2021-10-27 16:30:32]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 17 @ 43770 updates, score 78.385399) (writing took 7.217385 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 16:31:04]    INFO >> epoch 018:    230 / 2575 loss=2.233, nll_loss=0.304, ppl=1.23, wps=1447.3, ups=4.32, wpb=335, bsz=4, num_updates=44000, lr=4.8e-05, gnorm=1.161, clip=0, loss_scale=512, train_wall=56, gb_free=29.4, wall=6517 (progress_bar.py:262, log())
[2021-10-27 16:32:06]    INFO >> epoch 018:    730 / 2575 loss=2.125, nll_loss=0.205, ppl=1.15, wps=722, ups=8.05, wpb=89.6, bsz=4, num_updates=44500, lr=4.8e-05, gnorm=0.898, clip=0, loss_scale=512, train_wall=60, gb_free=29.4, wall=6579 (progress_bar.py:262, log())
[2021-10-27 16:33:08]    INFO >> epoch 018:   1230 / 2575 loss=2.124, nll_loss=0.201, ppl=1.15, wps=1122.2, ups=8.04, wpb=139.5, bsz=4, num_updates=45000, lr=4.8e-05, gnorm=0.854, clip=0, loss_scale=512, train_wall=60, gb_free=29.4, wall=6641 (progress_bar.py:262, log())
[2021-10-27 16:34:11]    INFO >> epoch 018:   1730 / 2575 loss=2.103, nll_loss=0.189, ppl=1.14, wps=1310.1, ups=7.98, wpb=164.2, bsz=4, num_updates=45500, lr=4.8e-05, gnorm=0.467, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=6703 (progress_bar.py:262, log())
[2021-10-27 16:35:13]    INFO >> epoch 018:   2230 / 2575 loss=2.109, nll_loss=0.195, ppl=1.15, wps=1566.8, ups=7.99, wpb=196, bsz=4, num_updates=46000, lr=4.8e-05, gnorm=0.429, clip=0, loss_scale=1024, train_wall=61, gb_free=29.3, wall=6766 (progress_bar.py:262, log())
[2021-10-27 16:35:52]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 16:35:52]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:35:58]    INFO >> epoch 018 | loss 2.155 | nll_loss 0.234 | ppl 1.18 | wps 1291.7 | ups 6.87 | wpb 188 | bsz 4 | num_updates 46344 | lr 4.8e-05 | gnorm 0.763 | clip 0 | loss_scale 512 | train_wall 306 | gb_free 28 | wall 6810 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:36:40]    INFO >> epoch 018 | valid on 'valid' subset | loss 2.358 | nll_loss 0.343 | ppl 1.27 | bleu 78.5607 | wps 734.6 | wpb 3127.5 | bsz 62.5 | num_updates 46344 | best_bleu 78.685 (progress_bar.py:269, print())
[2021-10-27 16:36:47]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 18 @ 46344 updates, score 78.560737) (writing took 7.339796 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 16:37:15]    INFO >> epoch 019:    156 / 2575 loss=2.222, nll_loss=0.293, ppl=1.23, wps=1507.5, ups=4.09, wpb=368.5, bsz=4, num_updates=46500, lr=4.8e-05, gnorm=1.042, clip=0, loss_scale=512, train_wall=62, gb_free=29.4, wall=6888 (progress_bar.py:262, log())
[2021-10-27 16:38:18]    INFO >> epoch 019:    656 / 2575 loss=2.127, nll_loss=0.209, ppl=1.16, wps=672.3, ups=8.02, wpb=83.8, bsz=4, num_updates=47000, lr=4.8e-05, gnorm=0.982, clip=0, loss_scale=512, train_wall=60, gb_free=29.4, wall=6951 (progress_bar.py:262, log())
[2021-10-27 16:39:20]    INFO >> epoch 019:   1156 / 2575 loss=2.122, nll_loss=0.2, ppl=1.15, wps=1050.6, ups=8.08, wpb=130.1, bsz=4, num_updates=47500, lr=4.8e-05, gnorm=0.864, clip=0.2, loss_scale=512, train_wall=60, gb_free=29.4, wall=7012 (progress_bar.py:262, log())
[2021-10-27 16:40:22]    INFO >> epoch 019:   1656 / 2575 loss=2.111, nll_loss=0.195, ppl=1.14, wps=1351.1, ups=8.01, wpb=168.8, bsz=4, num_updates=48000, lr=4.8e-05, gnorm=0.595, clip=0, loss_scale=512, train_wall=60, gb_free=29.4, wall=7075 (progress_bar.py:262, log())
[2021-10-27 16:41:25]    INFO >> epoch 019:   2156 / 2575 loss=2.1, nll_loss=0.189, ppl=1.14, wps=1436.2, ups=7.93, wpb=181.2, bsz=4, num_updates=48500, lr=4.8e-05, gnorm=0.378, clip=0, loss_scale=1024, train_wall=61, gb_free=29.3, wall=7138 (progress_bar.py:262, log())
[2021-10-27 16:41:37]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 16:42:06]    INFO >> AMP: overflow detected, setting scale to to 256.0 (amp_optimizer.py:66, clip_grad_norm())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:42:19]    INFO >> epoch 019 | loss 2.157 | nll_loss 0.236 | ppl 1.18 | wps 1270.4 | ups 6.75 | wpb 188.3 | bsz 4 | num_updates 48919 | lr 4.8e-05 | gnorm 0.775 | clip 0 | loss_scale 256 | train_wall 312 | gb_free 28 | wall 7192 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:43:04]    INFO >> epoch 019 | valid on 'valid' subset | loss 2.358 | nll_loss 0.347 | ppl 1.27 | bleu 78.1184 | wps 689.4 | wpb 3127.5 | bsz 62.5 | num_updates 48919 | best_bleu 78.685 (progress_bar.py:269, print())
[2021-10-27 16:43:12]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 19 @ 48919 updates, score 78.118418) (writing took 7.455994 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 16:43:30]    INFO >> epoch 020:     81 / 2575 loss=2.221, nll_loss=0.294, ppl=1.23, wps=1591.1, ups=4, wpb=397.4, bsz=4, num_updates=49000, lr=4.8e-05, gnorm=1.021, clip=0, loss_scale=256, train_wall=62, gb_free=29.4, wall=7263 (progress_bar.py:262, log())
[2021-10-27 16:44:32]    INFO >> epoch 020:    581 / 2575 loss=2.136, nll_loss=0.218, ppl=1.16, wps=619.9, ups=8.03, wpb=77.2, bsz=4, num_updates=49500, lr=4.8e-05, gnorm=1.088, clip=0, loss_scale=256, train_wall=60, gb_free=29.4, wall=7325 (progress_bar.py:262, log())
[2021-10-27 16:45:34]    INFO >> epoch 020:   1081 / 2575 loss=2.129, nll_loss=0.207, ppl=1.15, wps=963.1, ups=8.05, wpb=119.7, bsz=4, num_updates=50000, lr=4.8e-05, gnorm=0.957, clip=0, loss_scale=256, train_wall=60, gb_free=29.4, wall=7387 (progress_bar.py:262, log())
[2021-10-27 16:46:37]    INFO >> epoch 020:   1581 / 2575 loss=2.128, nll_loss=0.208, ppl=1.15, wps=1386, ups=8, wpb=173.3, bsz=4, num_updates=50500, lr=4.8e-05, gnorm=0.782, clip=0, loss_scale=256, train_wall=60, gb_free=29.4, wall=7450 (progress_bar.py:262, log())
[2021-10-27 16:47:41]    INFO >> epoch 020:   2081 / 2575 loss=2.098, nll_loss=0.19, ppl=1.14, wps=1330.9, ups=7.82, wpb=170.2, bsz=4, num_updates=51000, lr=4.8e-05, gnorm=0.395, clip=0, loss_scale=512, train_wall=62, gb_free=29.4, wall=7514 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:48:44]    INFO >> epoch 020 | loss 2.159 | nll_loss 0.239 | ppl 1.18 | wps 1259.3 | ups 6.69 | wpb 188.3 | bsz 4 | num_updates 51494 | lr 4.7e-05 | gnorm 0.839 | clip 0 | loss_scale 512 | train_wall 313 | gb_free 28 | wall 7577 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:49:26]    INFO >> epoch 020 | valid on 'valid' subset | loss 2.366 | nll_loss 0.348 | ppl 1.27 | bleu 77.7653 | wps 753.1 | wpb 3127.5 | bsz 62.5 | num_updates 51494 | best_bleu 78.685 (progress_bar.py:269, print())
[2021-10-27 16:49:34]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 20 @ 51494 updates, score 77.76531) (writing took 7.445847 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 16:49:43]    INFO >> epoch 021:      6 / 2575 loss=2.21, nll_loss=0.285, ppl=1.22, wps=1734.6, ups=4.11, wpb=422.1, bsz=4, num_updates=51500, lr=4.7e-05, gnorm=0.943, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=7635 (progress_bar.py:262, log())
[2021-10-27 16:50:45]    INFO >> epoch 021:    506 / 2575 loss=2.129, nll_loss=0.213, ppl=1.16, wps=560.7, ups=7.99, wpb=70.2, bsz=4, num_updates=52000, lr=4.7e-05, gnorm=1.096, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=7698 (progress_bar.py:262, log())
[2021-10-27 16:51:48]    INFO >> epoch 021:   1006 / 2575 loss=2.118, nll_loss=0.199, ppl=1.15, wps=887.6, ups=7.95, wpb=111.6, bsz=4, num_updates=52500, lr=4.7e-05, gnorm=0.817, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=7761 (progress_bar.py:262, log())
[2021-10-27 16:52:51]    INFO >> epoch 021:   1506 / 2575 loss=2.119, nll_loss=0.2, ppl=1.15, wps=1323.3, ups=7.96, wpb=166.3, bsz=4, num_updates=53000, lr=4.7e-05, gnorm=0.74, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=7824 (progress_bar.py:262, log())
[2021-10-27 16:53:54]    INFO >> epoch 021:   2006 / 2575 loss=2.089, nll_loss=0.18, ppl=1.13, wps=1340.3, ups=7.97, wpb=168.2, bsz=4, num_updates=53500, lr=4.7e-05, gnorm=0.343, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=7886 (progress_bar.py:262, log())
[2021-10-27 16:54:44]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 16:54:44]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-10-27 16:54:56]    INFO >> epoch 021:   2507 / 2575 loss=2.174, nll_loss=0.251, ppl=1.19, wps=2694.6, ups=7.99, wpb=337.2, bsz=4, num_updates=54000, lr=4.7e-05, gnorm=0.778, clip=0, loss_scale=512, train_wall=61, gb_free=28.7, wall=7949 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:55:06]    INFO >> epoch 021 | loss 2.152 | nll_loss 0.232 | ppl 1.17 | wps 1268.4 | ups 6.74 | wpb 188.1 | bsz 4 | num_updates 54068 | lr 4.7e-05 | gnorm 0.769 | clip 0 | loss_scale 512 | train_wall 313 | gb_free 28 | wall 7959 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 16:55:49]    INFO >> epoch 021 | valid on 'valid' subset | loss 2.361 | nll_loss 0.346 | ppl 1.27 | bleu 77.8739 | wps 724.2 | wpb 3127.5 | bsz 62.5 | num_updates 54068 | best_bleu 78.685 (progress_bar.py:269, print())
[2021-10-27 16:55:56]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 21 @ 54068 updates, score 77.873904) (writing took 7.082568 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 16:56:58]    INFO >> epoch 022:    432 / 2575 loss=2.223, nll_loss=0.299, ppl=1.23, wps=705.3, ups=4.11, wpb=171.7, bsz=4, num_updates=54500, lr=4.7e-05, gnorm=1.061, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=8071 (progress_bar.py:262, log())
[2021-10-27 16:58:01]    INFO >> epoch 022:    932 / 2575 loss=2.116, nll_loss=0.197, ppl=1.15, wps=834.3, ups=7.93, wpb=105.2, bsz=4, num_updates=55000, lr=4.7e-05, gnorm=0.811, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=8134 (progress_bar.py:262, log())
[2021-10-27 16:59:04]    INFO >> epoch 022:   1432 / 2575 loss=2.119, nll_loss=0.199, ppl=1.15, wps=1296.8, ups=7.96, wpb=162.8, bsz=4, num_updates=55500, lr=4.7e-05, gnorm=0.761, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=8197 (progress_bar.py:262, log())
[2021-10-27 17:00:06]    INFO >> epoch 022:   1932 / 2575 loss=2.089, nll_loss=0.181, ppl=1.13, wps=1327.4, ups=8.03, wpb=165.3, bsz=4, num_updates=56000, lr=4.7e-05, gnorm=0.386, clip=0, loss_scale=1024, train_wall=60, gb_free=29.4, wall=8259 (progress_bar.py:262, log())
[2021-10-27 17:01:09]    INFO >> epoch 022:   2432 / 2575 loss=2.152, nll_loss=0.233, ppl=1.18, wps=2204.8, ups=7.99, wpb=276, bsz=4, num_updates=56500, lr=4.7e-05, gnorm=0.716, clip=0, loss_scale=1024, train_wall=61, gb_free=29.1, wall=8321 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 17:01:28]    INFO >> epoch 022 | loss 2.151 | nll_loss 0.232 | ppl 1.17 | wps 1269.9 | ups 6.75 | wpb 188.3 | bsz 4 | num_updates 56643 | lr 4.7e-05 | gnorm 0.757 | clip 0 | loss_scale 1024 | train_wall 312 | gb_free 28 | wall 8340 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 17:02:10]    INFO >> epoch 022 | valid on 'valid' subset | loss 2.363 | nll_loss 0.354 | ppl 1.28 | bleu 77.6116 | wps 743.5 | wpb 3127.5 | bsz 62.5 | num_updates 56643 | best_bleu 78.685 (progress_bar.py:269, print())
[2021-10-27 17:02:18]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 22 @ 56643 updates, score 77.61164) (writing took 7.609446 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 17:03:10]    INFO >> epoch 023:    357 / 2575 loss=2.227, nll_loss=0.302, ppl=1.23, wps=1015.6, ups=4.1, wpb=247.4, bsz=4, num_updates=57000, lr=4.7e-05, gnorm=0.983, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=8443 (progress_bar.py:262, log())
[2021-10-27 17:04:09]    INFO >> epoch 023:    857 / 2575 loss=2.114, nll_loss=0.196, ppl=1.15, wps=848.8, ups=8.5, wpb=99.9, bsz=4, num_updates=57500, lr=4.7e-05, gnorm=0.815, clip=0, loss_scale=1024, train_wall=57, gb_free=29.4, wall=8502 (progress_bar.py:262, log())
[2021-10-27 17:05:08]    INFO >> epoch 023:   1357 / 2575 loss=2.12, nll_loss=0.198, ppl=1.15, wps=1376.8, ups=8.53, wpb=161.5, bsz=4, num_updates=58000, lr=4.7e-05, gnorm=0.776, clip=0, loss_scale=2048, train_wall=57, gb_free=29.4, wall=8561 (progress_bar.py:262, log())
[2021-10-27 17:06:10]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 17:06:11]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 17:06:11]    INFO >> AMP: overflow detected, setting scale to to 256.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-27 17:06:11]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-10-27 17:06:11]    INFO >> epoch 023:   1858 / 2575 loss=2.087, nll_loss=0.178, ppl=1.13, wps=1279.4, ups=7.94, wpb=161.2, bsz=4, num_updates=58500, lr=4.7e-05, gnorm=0.366, clip=0, loss_scale=256, train_wall=61, gb_free=29.4, wall=8624 (progress_bar.py:262, log())
[2021-10-27 17:07:13]    INFO >> epoch 023:   2358 / 2575 loss=2.135, nll_loss=0.22, ppl=1.16, wps=1848.6, ups=8, wpb=231, bsz=4, num_updates=59000, lr=4.7e-05, gnorm=0.635, clip=0, loss_scale=256, train_wall=60, gb_free=29.3, wall=8686 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 17:07:42]    INFO >> epoch 023 | loss 2.155 | nll_loss 0.236 | ppl 1.18 | wps 1294 | ups 6.88 | wpb 188.2 | bsz 4 | num_updates 59217 | lr 4.7e-05 | gnorm 0.739 | clip 0 | loss_scale 256 | train_wall 305 | gb_free 28 | wall 8715 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-27 17:08:28]    INFO >> epoch 023 | valid on 'valid' subset | loss 2.371 | nll_loss 0.356 | ppl 1.28 | bleu 77.8449 | wps 659.1 | wpb 3127.5 | bsz 62.5 | num_updates 59217 | best_bleu 78.685 (progress_bar.py:269, print())
[2021-10-27 17:08:35]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/csharp-java/checkpoints/checkpoint_last.pt (epoch 23 @ 59217 updates, score 77.844944) (writing took 7.087107 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-27 17:08:35]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:179, should_stop_early())
[2021-10-27 17:08:35]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:277, single_main())
[2021-10-27 17:08:35]    INFO >> done training in 8767.9 seconds (train.py:288, single_main())
