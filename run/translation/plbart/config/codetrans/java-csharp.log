nohup: ignoring input
Using backend: pytorch
[2021-11-02 17:30:50]    INFO >> Load arguments in /home/wanyao/yang/naturalcc-dev/run/translation/plbart/config/codetrans/java-csharp.yml (train.py:314, cli_main())
[2021-11-02 17:30:50]    INFO >> {'criterion': 'label_smoothed_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 500, 'log_format': 'simple', 'tensorboard_logdir': '', 'memory_efficient_fp16': 0, 'fp16_no_flatten_grads': 1, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'empty_cache_freq': 0, 'task': 'plbart_translation', 'seed': 1234, 'cpu': 0, 'fp16': 0, 'fp16_opt_level': '01', 'bf16': 0, 'memory_efficient_bf16': 0, 'server_ip': '', 'server_port': '', 'amp': 1, 'amp_batch_retries': 2, 'amp_init_scale': '2 ** 7', 'amp_scale_window': None}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 4, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 64, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'append_lang_id': 1}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'pipeline_model_parallel': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1, 'block_momentum': 0.875, 'block_lr': 1, 'use_nbm': 0, 'average_sync': 0}, 'task': {'data': '/home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap', 'source_lang': 'java', 'target_lang': 'csharp', 'load_alignments': 0, 'left_pad_source': 0, 'left_pad_target': 0, 'max_source_positions': 320, 'max_target_positions': 256, 'upsample_primary': 1, 'truncate_source': 1, 'truncate_target': 1, 'append_eos_to_target': 1, 'eval_bleu': 1, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': True, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_args': None, 'eval_bleu_print_samples': 0, 'eval_with_sacrebleu': 1}, 'model': {'arch': 'fairseq_transformer', 'offset_positions_by_padding': 1, 'pooler_dropout': 0.1, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.1, 'encoder_positional_embeddings': 0, 'encoder_learned_pos': 1, 'encoder_max_relative_len': 0, 'encoder_embed_path': 0, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_layers': 6, 'encoder_attention_heads': 12, 'encoder_normalize_before': 0, 'decoder_embed_path': '', 'decoder_positional_embeddings': 0, 'decoder_learned_pos': 1, 'decoder_max_relative_len': 0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_attention_heads': 12, 'decoder_normalize_before': 0, 'no_decoder_final_norm': 0, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.1, 'adaptive_softmax_factor': 0.0, 'share_decoder_input_output_embed': 1, 'decoder_out_embed_bias': 1, 'share_all_embeddings': 1, 'adaptive_input': 0, 'adaptive_input_factor': 0.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': 0, 'tie_adaptive_proj': 0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 1, 'no_scale_embedding': 0, 'no_token_positional_embeddings': 0, 'encoder_dropout_in': 0.1, 'encoder_dropout_out': 0.1, 'decoder_dropout_in': 0.1, 'decoder_dropout_out': 0.1, 'max_source_positions': 1024, 'max_target_positions': 1024, 'multihead_attention_version': 'ncc', 'encoder_position_encoding_version': 'ncc_learned', 'decoder_position_encoding_version': 'ncc_learned'}, 'optimization': {'max_epoch': 100, 'max_update': 0, 'clip_norm': 25.0, 'update_freq': [1], 'lrs': [5e-05], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1500, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'sentence_avg': 0, 'label_smoothing': 0.1, 'adam': {'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': 0}}, 'checkpoint': {'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': 1, 'patience': 10, 'save_dir': '/home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints', 'should_continue': 0, 'model_name_or_path': None, 'cache_dir': None, 'logging_steps': 500, 'save_steps': 2000, 'save_total_limit': 2, 'overwrite_output_dir': 0, 'overwrite_cache': 0, 'init_checkpoint': '/home/wanyao/yang/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt'}, 'eval': {'path': '/home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_best.pt', 'remove_bpe': 'sentencepiece', 'quiet': 1, 'results_path': None, 'model_overrides': '{}', 'topk': 5, 'max_sentences': 2, 'beam': 5, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 500, 'min_len': 1, 'match_source_len': 0, 'no_early_stop': 1, 'unnormalized': 0, 'no_beamable_mm': 0, 'lenpen': 1, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': 0, 'score_reference': 0, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': 0, 'sampling_topk': -1, 'sampling_topp': -1, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 0, 'print_step': 0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': 0, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': 0, 'retain_iter_history': 0, 'decoding_format': None, 'nltk_bleu': 1, 'rouge': 1}} (train.py:316, cli_main())
[2021-11-02 17:30:50]    INFO >> single GPU training... (train.py:345, cli_main())
[2021-11-02 17:30:50]    INFO >> [java] dictionary: 50006 types (plbart_translation.py:135, setup_task())
[2021-11-02 17:30:50]    INFO >> [csharp] dictionary: 50006 types (plbart_translation.py:136, setup_task())
[2021-11-02 17:30:51]    INFO >> truncate java/valid.code_tokens to 320 (plbart_translation.py:72, load_langpair_dataset())
[2021-11-02 17:30:51]    INFO >> truncate csharp/valid.code_tokens to 256 (plbart_translation.py:88, load_langpair_dataset())
[2021-11-02 17:30:52]    INFO >> truncate java/test.code_tokens to 320 (plbart_translation.py:72, load_langpair_dataset())
[2021-11-02 17:30:52]    INFO >> truncate csharp/test.code_tokens to 256 (plbart_translation.py:88, load_langpair_dataset())
[2021-11-02 17:31:04]    INFO >> Restore parameters from /home/wanyao/yang/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt (train.py:229, single_main())
[2021-11-02 17:31:04]    INFO >> FairseqTransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(50006, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(50006, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50006, bias=False)
  )
) (train.py:232, single_main())
[2021-11-02 17:31:04]    INFO >> model fairseq_transformer, criterion LabelSmoothedCrossEntropyCriterion (train.py:233, single_main())
[2021-11-02 17:31:04]    INFO >> num. model params: 139221504 (num. trained: 139221504) (train.py:236, single_main())
[2021-11-02 17:31:08]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:542, pretty_print_cuda_env_list())
[2021-11-02 17:31:08]    INFO >> rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                     (utils.py:548, pretty_print_cuda_env_list())
[2021-11-02 17:31:08]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:550, pretty_print_cuda_env_list())
[2021-11-02 17:31:08]    INFO >> training on 1 GPUs (train.py:241, single_main())
[2021-11-02 17:31:08]    INFO >> max tokens per GPU = None and max sentences per GPU = 4 (train.py:244, single_main())
[2021-11-02 17:31:08]    INFO >> no existing checkpoint found /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())
[2021-11-02 17:31:08]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())
[2021-11-02 17:31:08]    INFO >> truncate java/train.code_tokens to 320 (plbart_translation.py:72, load_langpair_dataset())
[2021-11-02 17:31:08]    INFO >> truncate csharp/train.code_tokens to 256 (plbart_translation.py:88, load_langpair_dataset())
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:57: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
[2021-11-02 17:31:16]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 17:31:17]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 17:31:17]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 17:31:17]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-02 17:31:17]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 17:31:17]    INFO >> AMP: overflow detected, setting scale to to 4.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 17:31:17]    INFO >> AMP: overflow detected, setting scale to to 2.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 17:31:17]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-02 17:32:14]    INFO >> epoch 001:    502 / 2575 loss=6.392, nll_loss=4.39, ppl=20.97, wps=1825.8, ups=8.79, wpb=207.4, bsz=4, num_updates=500, lr=1.7e-05, gnorm=134.911, clip=38.2, loss_scale=2, train_wall=56, gb_free=29.4, wall=66 (progress_bar.py:262, log())
[2021-11-02 17:33:14]    INFO >> epoch 001:   1002 / 2575 loss=2.938, nll_loss=0.905, ppl=1.87, wps=1870.6, ups=8.25, wpb=226.7, bsz=4, num_updates=1000, lr=3.3e-05, gnorm=6.035, clip=0.4, loss_scale=2, train_wall=59, gb_free=29.4, wall=126 (progress_bar.py:262, log())
[2021-11-02 17:34:12]    INFO >> epoch 001:   1502 / 2575 loss=2.69, nll_loss=0.693, ppl=1.62, wps=2023.7, ups=8.7, wpb=232.5, bsz=4, num_updates=1500, lr=5e-05, gnorm=4.173, clip=0.6, loss_scale=2, train_wall=56, gb_free=29.4, wall=184 (progress_bar.py:262, log())
[2021-11-02 17:35:10]    INFO >> epoch 001:   2002 / 2575 loss=2.634, nll_loss=0.67, ppl=1.59, wps=1981.5, ups=8.65, wpb=229, bsz=4, num_updates=2000, lr=5e-05, gnorm=3.669, clip=0, loss_scale=2, train_wall=57, gb_free=29.4, wall=241 (progress_bar.py:262, log())
[2021-11-02 17:36:07]    INFO >> epoch 001:   2502 / 2575 loss=2.564, nll_loss=0.611, ppl=1.53, wps=2045.3, ups=8.71, wpb=234.8, bsz=4, num_updates=2500, lr=5e-05, gnorm=3.141, clip=0, loss_scale=4, train_wall=56, gb_free=29.4, wall=299 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 17:36:30]    INFO >> epoch 001 | loss 3.359 | nll_loss 1.371 | ppl 2.59 | wps 1855 | ups 8.23 | wpb 225.4 | bsz 4 | num_updates 2573 | lr 5e-05 | gnorm 29.627 | clip 7.6 | loss_scale 4 | train_wall 292 | gb_free 29.3 | wall 322 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 17:37:25]    INFO >> epoch 001 | valid on 'valid' subset | loss 2.511 | nll_loss 0.387 | ppl 1.31 | bleu 73.2478 | wps 599.1 | wpb 3764.1 | bsz 62.5 | num_updates 2573 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 17:38:49]    INFO >> epoch 001 | valid on 'test' subset | loss 2.498 | nll_loss 0.367 | ppl 1.29 | bleu 74.4247 | wps 721.4 | wpb 3487.6 | bsz 62.5 | num_updates 2573 (progress_bar.py:269, print())
[2021-11-02 17:38:55]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_best.pt (epoch 1 @ 2573 updates, score 73.247827) (writing took 5.779324 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 17:39:49]    INFO >> epoch 002:    427 / 2575 loss=2.487, nll_loss=0.532, ppl=1.45, wps=522, ups=2.26, wpb=231.3, bsz=4, num_updates=3000, lr=5e-05, gnorm=2.988, clip=0, loss_scale=4, train_wall=52, gb_free=29.4, wall=520 (progress_bar.py:262, log())
[2021-11-02 17:40:50]    INFO >> epoch 002:    927 / 2575 loss=2.442, nll_loss=0.498, ppl=1.41, wps=1903.7, ups=8.17, wpb=233.1, bsz=4, num_updates=3500, lr=5e-05, gnorm=2.746, clip=0, loss_scale=4, train_wall=60, gb_free=29.4, wall=582 (progress_bar.py:262, log())
[2021-11-02 17:41:43]    INFO >> epoch 002:   1427 / 2575 loss=2.442, nll_loss=0.503, ppl=1.42, wps=2162.3, ups=9.47, wpb=228.3, bsz=4, num_updates=4000, lr=5e-05, gnorm=2.961, clip=0.2, loss_scale=4, train_wall=52, gb_free=29.4, wall=634 (progress_bar.py:262, log())
[2021-11-02 17:42:43]    INFO >> epoch 002:   1927 / 2575 loss=2.387, nll_loss=0.452, ppl=1.37, wps=1769.5, ups=8.33, wpb=212.4, bsz=4, num_updates=4500, lr=5e-05, gnorm=2.613, clip=0, loss_scale=8, train_wall=59, gb_free=29.4, wall=694 (progress_bar.py:262, log())
[2021-11-02 17:43:37]    INFO >> epoch 002:   2427 / 2575 loss=2.44, nll_loss=0.506, ppl=1.42, wps=2074.1, ups=9.21, wpb=225.2, bsz=4, num_updates=5000, lr=5e-05, gnorm=2.563, clip=0, loss_scale=8, train_wall=53, gb_free=29.4, wall=749 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 17:44:10]    INFO >> epoch 002 | loss 2.438 | nll_loss 0.498 | ppl 1.41 | wps 1260.6 | ups 5.59 | wpb 225.5 | bsz 4 | num_updates 5148 | lr 5e-05 | gnorm 2.739 | clip 0 | loss_scale 8 | train_wall 286 | gb_free 29.4 | wall 782 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 17:45:07]    INFO >> epoch 002 | valid on 'valid' subset | loss 2.396 | nll_loss 0.33 | ppl 1.26 | bleu 76.1822 | wps 594.8 | wpb 3764.1 | bsz 62.5 | num_updates 5148 | best_bleu 76.1822 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 17:46:34]    INFO >> epoch 002 | valid on 'test' subset | loss 2.395 | nll_loss 0.326 | ppl 1.25 | bleu 77.3067 | wps 696.6 | wpb 3487.6 | bsz 62.5 | num_updates 5148 | best_bleu 77.3067 (progress_bar.py:269, print())
[2021-11-02 17:46:55]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_best.pt (epoch 2 @ 5148 updates, score 76.18225) (writing took 21.111588 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 17:47:37]    INFO >> epoch 003:    352 / 2575 loss=2.366, nll_loss=0.431, ppl=1.35, wps=444.7, ups=2.08, wpb=213.7, bsz=4, num_updates=5500, lr=5e-05, gnorm=2.323, clip=0, loss_scale=8, train_wall=52, gb_free=28.1, wall=989 (progress_bar.py:262, log())
[2021-11-02 17:48:38]    INFO >> epoch 003:    852 / 2575 loss=2.358, nll_loss=0.422, ppl=1.34, wps=1831.8, ups=8.16, wpb=224.4, bsz=4, num_updates=6000, lr=5e-05, gnorm=2.203, clip=0, loss_scale=8, train_wall=60, gb_free=29.4, wall=1050 (progress_bar.py:262, log())
[2021-11-02 17:49:35]    INFO >> epoch 003:   1352 / 2575 loss=2.319, nll_loss=0.386, ppl=1.31, wps=2023.2, ups=8.8, wpb=229.9, bsz=4, num_updates=6500, lr=5e-05, gnorm=2.05, clip=0, loss_scale=16, train_wall=56, gb_free=29.4, wall=1107 (progress_bar.py:262, log())
[2021-11-02 17:50:37]    INFO >> epoch 003:   1852 / 2575 loss=2.334, nll_loss=0.403, ppl=1.32, wps=1819.8, ups=8.1, wpb=224.7, bsz=4, num_updates=7000, lr=5e-05, gnorm=2.073, clip=0, loss_scale=16, train_wall=61, gb_free=29.4, wall=1169 (progress_bar.py:262, log())
[2021-11-02 17:51:38]    INFO >> epoch 003:   2352 / 2575 loss=2.34, nll_loss=0.412, ppl=1.33, wps=1869.9, ups=8.17, wpb=228.8, bsz=4, num_updates=7500, lr=5e-05, gnorm=2.088, clip=0.2, loss_scale=16, train_wall=60, gb_free=29.4, wall=1230 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 17:52:20]    INFO >> epoch 003 | loss 2.338 | nll_loss 0.404 | ppl 1.32 | wps 1186 | ups 5.26 | wpb 225.5 | bsz 4 | num_updates 7723 | lr 5e-05 | gnorm 2.11 | clip 0 | loss_scale 16 | train_wall 296 | gb_free 29.3 | wall 1272 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 17:53:16]    INFO >> epoch 003 | valid on 'valid' subset | loss 2.362 | nll_loss 0.313 | ppl 1.24 | bleu 78.724 | wps 595.7 | wpb 3764.1 | bsz 62.5 | num_updates 7723 | best_bleu 78.724 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 17:54:43]    INFO >> epoch 003 | valid on 'test' subset | loss 2.36 | nll_loss 0.306 | ppl 1.24 | bleu 79.8377 | wps 693.2 | wpb 3487.6 | bsz 62.5 | num_updates 7723 | best_bleu 79.8377 (progress_bar.py:269, print())
[2021-11-02 17:55:04]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_best.pt (epoch 3 @ 7723 updates, score 78.723968) (writing took 20.784579 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 17:55:42]    INFO >> epoch 004:    277 / 2575 loss=2.294, nll_loss=0.358, ppl=1.28, wps=422.2, ups=2.05, wpb=205.8, bsz=4, num_updates=8000, lr=5e-05, gnorm=1.904, clip=0, loss_scale=16, train_wall=56, gb_free=29.4, wall=1474 (progress_bar.py:262, log())
[2021-11-02 17:56:38]    INFO >> epoch 004:    777 / 2575 loss=2.265, nll_loss=0.334, ppl=1.26, wps=2007.1, ups=8.84, wpb=227, bsz=4, num_updates=8500, lr=5e-05, gnorm=1.706, clip=0, loss_scale=32, train_wall=55, gb_free=29.4, wall=1530 (progress_bar.py:262, log())
[2021-11-02 17:57:35]    INFO >> epoch 004:   1277 / 2575 loss=2.288, nll_loss=0.357, ppl=1.28, wps=2129.8, ups=8.86, wpb=240.5, bsz=4, num_updates=9000, lr=5e-05, gnorm=1.815, clip=0, loss_scale=32, train_wall=55, gb_free=28.7, wall=1587 (progress_bar.py:262, log())
[2021-11-02 17:58:36]    INFO >> epoch 004:   1777 / 2575 loss=2.287, nll_loss=0.357, ppl=1.28, wps=1841.6, ups=8.13, wpb=226.6, bsz=4, num_updates=9500, lr=5e-05, gnorm=1.964, clip=0, loss_scale=32, train_wall=60, gb_free=29.4, wall=1648 (progress_bar.py:262, log())
[2021-11-02 17:59:37]    INFO >> epoch 004:   2277 / 2575 loss=2.253, nll_loss=0.323, ppl=1.25, wps=1806.5, ups=8.28, wpb=218.2, bsz=4, num_updates=10000, lr=5e-05, gnorm=2.042, clip=0.2, loss_scale=32, train_wall=59, gb_free=29.4, wall=1709 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:00:27]    INFO >> epoch 004 | loss 2.273 | nll_loss 0.342 | ppl 1.27 | wps 1192.6 | ups 5.29 | wpb 225.5 | bsz 4 | num_updates 10298 | lr 5e-05 | gnorm 1.868 | clip 0 | loss_scale 64 | train_wall 294 | gb_free 28.3 | wall 1759 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:01:23]    INFO >> epoch 004 | valid on 'valid' subset | loss 2.353 | nll_loss 0.296 | ppl 1.23 | bleu 81.456 | wps 593.3 | wpb 3764.1 | bsz 62.5 | num_updates 10298 | best_bleu 81.456 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:02:52]    INFO >> epoch 004 | valid on 'test' subset | loss 2.354 | nll_loss 0.295 | ppl 1.23 | bleu 80.9651 | wps 680.6 | wpb 3487.6 | bsz 62.5 | num_updates 10298 | best_bleu 80.9651 (progress_bar.py:269, print())
[2021-11-02 18:03:14]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_best.pt (epoch 4 @ 10298 updates, score 81.456026) (writing took 21.421698 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 18:03:44]    INFO >> epoch 005:    202 / 2575 loss=2.264, nll_loss=0.333, ppl=1.26, wps=482.3, ups=2.02, wpb=238.4, bsz=4, num_updates=10500, lr=5e-05, gnorm=1.653, clip=0, loss_scale=64, train_wall=57, gb_free=29.4, wall=1956 (progress_bar.py:262, log())
[2021-11-02 18:04:44]    INFO >> epoch 005:    702 / 2575 loss=2.212, nll_loss=0.28, ppl=1.21, wps=1776.4, ups=8.29, wpb=214.2, bsz=4, num_updates=11000, lr=5e-05, gnorm=1.464, clip=0, loss_scale=64, train_wall=59, gb_free=29.4, wall=2016 (progress_bar.py:262, log())
[2021-11-02 18:05:44]    INFO >> epoch 005:   1202 / 2575 loss=2.23, nll_loss=0.297, ppl=1.23, wps=1901.2, ups=8.41, wpb=226, bsz=4, num_updates=11500, lr=4.9e-05, gnorm=1.628, clip=0.2, loss_scale=64, train_wall=58, gb_free=29.4, wall=2076 (progress_bar.py:262, log())
[2021-11-02 18:06:31]    INFO >> epoch 005:   1702 / 2575 loss=2.227, nll_loss=0.298, ppl=1.23, wps=2424.7, ups=10.5, wpb=230.9, bsz=4, num_updates=12000, lr=4.9e-05, gnorm=1.539, clip=0, loss_scale=64, train_wall=46, gb_free=28.8, wall=2123 (progress_bar.py:262, log())
[2021-11-02 18:07:32]    INFO >> epoch 005:   2202 / 2575 loss=2.228, nll_loss=0.299, ppl=1.23, wps=1852.2, ups=8.23, wpb=225.2, bsz=4, num_updates=12500, lr=4.9e-05, gnorm=1.405, clip=0, loss_scale=128, train_wall=60, gb_free=29.4, wall=2184 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:08:32]    INFO >> epoch 005 | loss 2.226 | nll_loss 0.296 | ppl 1.23 | wps 1197.7 | ups 5.31 | wpb 225.5 | bsz 4 | num_updates 12873 | lr 4.9e-05 | gnorm 1.479 | clip 0 | loss_scale 128 | train_wall 289 | gb_free 29 | wall 2244 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:09:29]    INFO >> epoch 005 | valid on 'valid' subset | loss 2.331 | nll_loss 0.276 | ppl 1.21 | bleu 80.698 | wps 599.1 | wpb 3764.1 | bsz 62.5 | num_updates 12873 | best_bleu 81.456 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:10:52]    INFO >> epoch 005 | valid on 'test' subset | loss 2.332 | nll_loss 0.277 | ppl 1.21 | bleu 82.0247 | wps 731.4 | wpb 3487.6 | bsz 62.5 | num_updates 12873 | best_bleu 82.0247 (progress_bar.py:269, print())
[2021-11-02 18:11:24]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 5 @ 12873 updates, score 80.698008) (writing took 32.263513 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 18:11:48]    INFO >> epoch 006:    127 / 2575 loss=2.225, nll_loss=0.298, ppl=1.23, wps=445.4, ups=1.96, wpb=227.6, bsz=4, num_updates=13000, lr=4.9e-05, gnorm=1.349, clip=0, loss_scale=128, train_wall=59, gb_free=29.4, wall=2439 (progress_bar.py:262, log())
[2021-11-02 18:12:46]    INFO >> epoch 006:    627 / 2575 loss=2.194, nll_loss=0.264, ppl=1.2, wps=1876, ups=8.55, wpb=219.3, bsz=4, num_updates=13500, lr=4.9e-05, gnorm=1.217, clip=0, loss_scale=128, train_wall=57, gb_free=29.4, wall=2498 (progress_bar.py:262, log())
[2021-11-02 18:13:45]    INFO >> epoch 006:   1127 / 2575 loss=2.186, nll_loss=0.258, ppl=1.2, wps=1944.5, ups=8.44, wpb=230.5, bsz=4, num_updates=14000, lr=4.9e-05, gnorm=1.184, clip=0.2, loss_scale=128, train_wall=58, gb_free=29.4, wall=2557 (progress_bar.py:262, log())
[2021-11-02 18:14:46]    INFO >> epoch 006:   1627 / 2575 loss=2.192, nll_loss=0.266, ppl=1.2, wps=1851.4, ups=8.19, wpb=226, bsz=4, num_updates=14500, lr=4.9e-05, gnorm=1.238, clip=0, loss_scale=256, train_wall=60, gb_free=29.4, wall=2618 (progress_bar.py:262, log())
[2021-11-02 18:15:48]    INFO >> epoch 006:   2127 / 2575 loss=2.178, nll_loss=0.25, ppl=1.19, wps=1845.5, ups=8.15, wpb=226.6, bsz=4, num_updates=15000, lr=4.9e-05, gnorm=1.132, clip=0, loss_scale=256, train_wall=60, gb_free=29.4, wall=2680 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:16:58]    INFO >> epoch 006 | loss 2.187 | nll_loss 0.258 | ppl 1.2 | wps 1148.4 | ups 5.09 | wpb 225.5 | bsz 4 | num_updates 15448 | lr 4.9e-05 | gnorm 1.196 | clip 0 | loss_scale 256 | train_wall 304 | gb_free 29.4 | wall 2749 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:17:55]    INFO >> epoch 006 | valid on 'valid' subset | loss 2.313 | nll_loss 0.275 | ppl 1.21 | bleu 82.4156 | wps 598.5 | wpb 3764.1 | bsz 62.5 | num_updates 15448 | best_bleu 82.4156 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:19:18]    INFO >> epoch 006 | valid on 'test' subset | loss 2.315 | nll_loss 0.278 | ppl 1.21 | bleu 82.6636 | wps 732.7 | wpb 3487.6 | bsz 62.5 | num_updates 15448 | best_bleu 82.6636 (progress_bar.py:269, print())
[2021-11-02 18:19:38]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_best.pt (epoch 6 @ 15448 updates, score 82.415616) (writing took 20.029698 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 18:19:52]    INFO >> epoch 007:     52 / 2575 loss=2.174, nll_loss=0.248, ppl=1.19, wps=466, ups=2.05, wpb=227.6, bsz=4, num_updates=15500, lr=4.9e-05, gnorm=1.141, clip=0, loss_scale=256, train_wall=60, gb_free=29.4, wall=2924 (progress_bar.py:262, log())
[2021-11-02 18:20:50]    INFO >> epoch 007:    552 / 2575 loss=2.163, nll_loss=0.235, ppl=1.18, wps=1971.9, ups=8.63, wpb=228.5, bsz=4, num_updates=16000, lr=4.9e-05, gnorm=0.976, clip=0, loss_scale=256, train_wall=57, gb_free=29.4, wall=2982 (progress_bar.py:262, log())
[2021-11-02 18:21:51]    INFO >> epoch 007:   1052 / 2575 loss=2.152, nll_loss=0.226, ppl=1.17, wps=1738.6, ups=8.23, wpb=211.2, bsz=4, num_updates=16500, lr=4.9e-05, gnorm=0.979, clip=0, loss_scale=512, train_wall=60, gb_free=29.4, wall=3042 (progress_bar.py:262, log())
[2021-11-02 18:22:50]    INFO >> epoch 007:   1552 / 2575 loss=2.164, nll_loss=0.238, ppl=1.18, wps=1943.7, ups=8.39, wpb=231.6, bsz=4, num_updates=17000, lr=4.9e-05, gnorm=0.854, clip=0, loss_scale=512, train_wall=58, gb_free=29.4, wall=3102 (progress_bar.py:262, log())
[2021-11-02 18:23:47]    INFO >> epoch 007:   2052 / 2575 loss=2.157, nll_loss=0.232, ppl=1.17, wps=2007.7, ups=8.73, wpb=230, bsz=4, num_updates=17500, lr=4.9e-05, gnorm=0.872, clip=0, loss_scale=512, train_wall=56, gb_free=29.4, wall=3159 (progress_bar.py:262, log())
[2021-11-02 18:24:45]    INFO >> epoch 007:   2552 / 2575 loss=2.161, nll_loss=0.237, ppl=1.18, wps=1962.2, ups=8.7, wpb=225.6, bsz=4, num_updates=18000, lr=4.9e-05, gnorm=0.831, clip=0, loss_scale=512, train_wall=56, gb_free=29.4, wall=3217 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:25:03]    INFO >> epoch 007 | loss 2.159 | nll_loss 0.233 | ppl 1.18 | wps 1197 | ups 5.31 | wpb 225.5 | bsz 4 | num_updates 18023 | lr 4.9e-05 | gnorm 0.901 | clip 0 | loss_scale 1024 | train_wall 296 | gb_free 29.4 | wall 3235 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:25:58]    INFO >> epoch 007 | valid on 'valid' subset | loss 2.309 | nll_loss 0.273 | ppl 1.21 | bleu 81.9948 | wps 603.2 | wpb 3764.1 | bsz 62.5 | num_updates 18023 | best_bleu 82.4156 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:27:24]    INFO >> epoch 007 | valid on 'test' subset | loss 2.312 | nll_loss 0.275 | ppl 1.21 | bleu 82.6212 | wps 711.3 | wpb 3487.6 | bsz 62.5 | num_updates 18023 | best_bleu 82.6212 (progress_bar.py:269, print())
[2021-11-02 18:27:36]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 7 @ 18023 updates, score 81.994808) (writing took 11.989792 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 18:28:41]    INFO >> epoch 008:    477 / 2575 loss=2.144, nll_loss=0.217, ppl=1.16, wps=490.6, ups=2.12, wpb=231.5, bsz=4, num_updates=18500, lr=4.9e-05, gnorm=0.744, clip=0, loss_scale=1024, train_wall=59, gb_free=29.4, wall=3453 (progress_bar.py:262, log())
[2021-11-02 18:29:41]    INFO >> epoch 008:    977 / 2575 loss=2.136, nll_loss=0.212, ppl=1.16, wps=1806.9, ups=8.37, wpb=215.8, bsz=4, num_updates=19000, lr=4.9e-05, gnorm=0.724, clip=0, loss_scale=1024, train_wall=59, gb_free=29.4, wall=3512 (progress_bar.py:262, log())
[2021-11-02 18:30:44]    INFO >> epoch 008:   1477 / 2575 loss=2.144, nll_loss=0.22, ppl=1.16, wps=1684.7, ups=7.85, wpb=214.7, bsz=4, num_updates=19500, lr=4.9e-05, gnorm=0.752, clip=0, loss_scale=1024, train_wall=62, gb_free=29.4, wall=3576 (progress_bar.py:262, log())
[2021-11-02 18:31:47]    INFO >> epoch 008:   1977 / 2575 loss=2.152, nll_loss=0.228, ppl=1.17, wps=1833.4, ups=7.93, wpb=231.1, bsz=4, num_updates=20000, lr=4.9e-05, gnorm=0.835, clip=0, loss_scale=1024, train_wall=62, gb_free=29.4, wall=3639 (progress_bar.py:262, log())
[2021-11-02 18:31:58]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 18:32:41]    INFO >> epoch 008:   2477 / 2575 loss=2.142, nll_loss=0.22, ppl=1.16, wps=2138.1, ups=9.37, wpb=228.1, bsz=4, num_updates=20500, lr=4.9e-05, gnorm=0.762, clip=0, loss_scale=1024, train_wall=52, gb_free=29.4, wall=3693 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:33:05]    INFO >> epoch 008 | loss 2.144 | nll_loss 0.22 | ppl 1.16 | wps 1205.4 | ups 5.34 | wpb 225.5 | bsz 4 | num_updates 20598 | lr 4.9e-05 | gnorm 0.76 | clip 0 | loss_scale 1024 | train_wall 300 | gb_free 29.4 | wall 3716 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:34:01]    INFO >> epoch 008 | valid on 'valid' subset | loss 2.312 | nll_loss 0.272 | ppl 1.21 | bleu 82.3585 | wps 598.4 | wpb 3764.1 | bsz 62.5 | num_updates 20598 | best_bleu 82.4156 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:35:27]    INFO >> epoch 008 | valid on 'test' subset | loss 2.314 | nll_loss 0.274 | ppl 1.21 | bleu 83.039 | wps 711.1 | wpb 3487.6 | bsz 62.5 | num_updates 20598 | best_bleu 83.039 (progress_bar.py:269, print())
[2021-11-02 18:35:39]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 8 @ 20598 updates, score 82.358484) (writing took 12.550164 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 18:36:29]    INFO >> epoch 009:    402 / 2575 loss=2.141, nll_loss=0.216, ppl=1.16, wps=511.7, ups=2.19, wpb=234, bsz=4, num_updates=21000, lr=4.9e-05, gnorm=0.677, clip=0, loss_scale=1024, train_wall=49, gb_free=29.4, wall=3921 (progress_bar.py:262, log())
[2021-11-02 18:36:43]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 18:37:17]    INFO >> epoch 009:    902 / 2575 loss=2.137, nll_loss=0.213, ppl=1.16, wps=2355.2, ups=10.37, wpb=227, bsz=4, num_updates=21500, lr=4.9e-05, gnorm=0.767, clip=0, loss_scale=512, train_wall=47, gb_free=29.4, wall=3969 (progress_bar.py:262, log())
[2021-11-02 18:38:17]    INFO >> epoch 009:   1402 / 2575 loss=2.137, nll_loss=0.214, ppl=1.16, wps=1778.2, ups=8.4, wpb=211.7, bsz=4, num_updates=22000, lr=4.9e-05, gnorm=0.736, clip=0, loss_scale=512, train_wall=58, gb_free=29.4, wall=4029 (progress_bar.py:262, log())
[2021-11-02 18:39:17]    INFO >> epoch 009:   1902 / 2575 loss=2.141, nll_loss=0.218, ppl=1.16, wps=1931.7, ups=8.39, wpb=230.2, bsz=4, num_updates=22500, lr=4.9e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=58, gb_free=29.4, wall=4088 (progress_bar.py:262, log())
[2021-11-02 18:40:19]    INFO >> epoch 009:   2402 / 2575 loss=2.16, nll_loss=0.235, ppl=1.18, wps=1875.2, ups=8.03, wpb=233.6, bsz=4, num_updates=23000, lr=4.9e-05, gnorm=0.783, clip=0, loss_scale=512, train_wall=61, gb_free=28.3, wall=4151 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:40:55]    INFO >> epoch 009 | loss 2.143 | nll_loss 0.219 | ppl 1.16 | wps 1234.8 | ups 5.48 | wpb 225.5 | bsz 4 | num_updates 23173 | lr 4.9e-05 | gnorm 0.744 | clip 0 | loss_scale 1024 | train_wall 286 | gb_free 29.4 | wall 4187 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:41:52]    INFO >> epoch 009 | valid on 'valid' subset | loss 2.308 | nll_loss 0.274 | ppl 1.21 | bleu 82.5436 | wps 595.2 | wpb 3764.1 | bsz 62.5 | num_updates 23173 | best_bleu 82.5436 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:43:16]    INFO >> epoch 009 | valid on 'test' subset | loss 2.312 | nll_loss 0.278 | ppl 1.21 | bleu 83.2384 | wps 722.3 | wpb 3487.6 | bsz 62.5 | num_updates 23173 | best_bleu 83.2384 (progress_bar.py:269, print())
[2021-11-02 18:43:36]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_best.pt (epoch 9 @ 23173 updates, score 82.543637) (writing took 20.312433 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 18:44:02]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 18:44:02]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-02 18:44:17]    INFO >> epoch 010:    328 / 2575 loss=2.14, nll_loss=0.218, ppl=1.16, wps=463.1, ups=2.1, wpb=220.3, bsz=4, num_updates=23500, lr=4.9e-05, gnorm=0.731, clip=0, loss_scale=512, train_wall=51, gb_free=29.4, wall=4389 (progress_bar.py:262, log())
[2021-11-02 18:45:15]    INFO >> epoch 010:    828 / 2575 loss=2.135, nll_loss=0.212, ppl=1.16, wps=1919.7, ups=8.52, wpb=225.2, bsz=4, num_updates=24000, lr=4.9e-05, gnorm=0.717, clip=0, loss_scale=512, train_wall=57, gb_free=29.4, wall=4447 (progress_bar.py:262, log())
[2021-11-02 18:46:15]    INFO >> epoch 010:   1328 / 2575 loss=2.143, nll_loss=0.22, ppl=1.16, wps=1905.4, ups=8.37, wpb=227.6, bsz=4, num_updates=24500, lr=4.9e-05, gnorm=0.781, clip=0, loss_scale=512, train_wall=59, gb_free=29.4, wall=4507 (progress_bar.py:262, log())
[2021-11-02 18:47:16]    INFO >> epoch 010:   1828 / 2575 loss=2.139, nll_loss=0.216, ppl=1.16, wps=1808.6, ups=8.17, wpb=221.4, bsz=4, num_updates=25000, lr=4.9e-05, gnorm=0.786, clip=0, loss_scale=512, train_wall=60, gb_free=29.4, wall=4568 (progress_bar.py:262, log())
[2021-11-02 18:48:03]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 18:48:03]    INFO >> AMP: overflow detected, setting scale to to 256.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 18:48:16]    INFO >> epoch 010:   2328 / 2575 loss=2.134, nll_loss=0.212, ppl=1.16, wps=1926, ups=8.39, wpb=229.7, bsz=4, num_updates=25500, lr=4.9e-05, gnorm=0.751, clip=0, loss_scale=256, train_wall=58, gb_free=29.4, wall=4628 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:49:02]    INFO >> epoch 010 | loss 2.14 | nll_loss 0.217 | ppl 1.16 | wps 1191.3 | ups 5.28 | wpb 225.4 | bsz 4 | num_updates 25747 | lr 4.9e-05 | gnorm 0.773 | clip 0 | loss_scale 256 | train_wall 295 | gb_free 29 | wall 4674 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:49:59]    INFO >> epoch 010 | valid on 'valid' subset | loss 2.313 | nll_loss 0.277 | ppl 1.21 | bleu 82.3373 | wps 592.1 | wpb 3764.1 | bsz 62.5 | num_updates 25747 | best_bleu 82.5436 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:51:25]    INFO >> epoch 010 | valid on 'test' subset | loss 2.315 | nll_loss 0.28 | ppl 1.21 | bleu 82.7086 | wps 707.9 | wpb 3487.6 | bsz 62.5 | num_updates 25747 | best_bleu 82.7086 (progress_bar.py:269, print())
[2021-11-02 18:51:36]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 10 @ 25747 updates, score 82.337333) (writing took 11.422671 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 18:52:14]    INFO >> epoch 011:    253 / 2575 loss=2.149, nll_loss=0.225, ppl=1.17, wps=480.5, ups=2.1, wpb=228.6, bsz=4, num_updates=26000, lr=4.9e-05, gnorm=0.871, clip=0, loss_scale=256, train_wall=59, gb_free=29.4, wall=4866 (progress_bar.py:262, log())
[2021-11-02 18:53:13]    INFO >> epoch 011:    753 / 2575 loss=2.14, nll_loss=0.216, ppl=1.16, wps=1906.1, ups=8.4, wpb=227, bsz=4, num_updates=26500, lr=4.9e-05, gnorm=0.801, clip=0, loss_scale=256, train_wall=58, gb_free=28.8, wall=4925 (progress_bar.py:262, log())
[2021-11-02 18:54:11]    INFO >> epoch 011:   1253 / 2575 loss=2.142, nll_loss=0.22, ppl=1.16, wps=1947.8, ups=8.61, wpb=226.3, bsz=4, num_updates=27000, lr=4.9e-05, gnorm=0.807, clip=0, loss_scale=256, train_wall=57, gb_free=29.4, wall=4983 (progress_bar.py:262, log())
[2021-11-02 18:55:11]    INFO >> epoch 011:   1753 / 2575 loss=2.139, nll_loss=0.216, ppl=1.16, wps=1846, ups=8.41, wpb=219.5, bsz=4, num_updates=27500, lr=4.9e-05, gnorm=0.976, clip=0.2, loss_scale=512, train_wall=58, gb_free=29.4, wall=5043 (progress_bar.py:262, log())
[2021-11-02 18:56:10]    INFO >> epoch 011:   2253 / 2575 loss=2.144, nll_loss=0.222, ppl=1.17, wps=1918, ups=8.38, wpb=228.9, bsz=4, num_updates=28000, lr=4.9e-05, gnorm=0.767, clip=0, loss_scale=512, train_wall=59, gb_free=29.4, wall=5102 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:57:03]    INFO >> epoch 011 | loss 2.14 | nll_loss 0.218 | ppl 1.16 | wps 1206.3 | ups 5.35 | wpb 225.5 | bsz 4 | num_updates 28322 | lr 4.9e-05 | gnorm 0.822 | clip 0 | loss_scale 512 | train_wall 298 | gb_free 29.4 | wall 5155 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:58:01]    INFO >> epoch 011 | valid on 'valid' subset | loss 2.304 | nll_loss 0.283 | ppl 1.22 | bleu 82.993 | wps 592.9 | wpb 3764.1 | bsz 62.5 | num_updates 28322 | best_bleu 82.993 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 18:59:25]    INFO >> epoch 011 | valid on 'test' subset | loss 2.307 | nll_loss 0.285 | ppl 1.22 | bleu 83.2405 | wps 722.2 | wpb 3487.6 | bsz 62.5 | num_updates 28322 | best_bleu 83.2405 (progress_bar.py:269, print())
[2021-11-02 18:59:47]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_best.pt (epoch 11 @ 28322 updates, score 82.993038) (writing took 21.579103 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 19:00:15]    INFO >> epoch 012:    178 / 2575 loss=2.129, nll_loss=0.207, ppl=1.15, wps=443.2, ups=2.04, wpb=217, bsz=4, num_updates=28500, lr=4.9e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=57, gb_free=29.2, wall=5347 (progress_bar.py:262, log())
[2021-11-02 19:01:11]    INFO >> epoch 012:    678 / 2575 loss=2.124, nll_loss=0.204, ppl=1.15, wps=1925.4, ups=8.95, wpb=215.2, bsz=4, num_updates=29000, lr=4.9e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=55, gb_free=29.4, wall=5403 (progress_bar.py:262, log())
[2021-11-02 19:02:04]    INFO >> epoch 012:   1178 / 2575 loss=2.139, nll_loss=0.217, ppl=1.16, wps=2203.8, ups=9.5, wpb=231.9, bsz=4, num_updates=29500, lr=4.9e-05, gnorm=0.765, clip=0, loss_scale=1024, train_wall=51, gb_free=29.4, wall=5456 (progress_bar.py:262, log())
[2021-11-02 19:03:02]    INFO >> epoch 012:   1678 / 2575 loss=2.134, nll_loss=0.212, ppl=1.16, wps=1958.1, ups=8.58, wpb=228.2, bsz=4, num_updates=30000, lr=4.9e-05, gnorm=0.68, clip=0, loss_scale=1024, train_wall=57, gb_free=29.4, wall=5514 (progress_bar.py:262, log())
[2021-11-02 19:03:56]    INFO >> epoch 012:   2178 / 2575 loss=2.132, nll_loss=0.21, ppl=1.16, wps=2149, ups=9.25, wpb=232.3, bsz=4, num_updates=30500, lr=4.9e-05, gnorm=0.695, clip=0, loss_scale=1024, train_wall=53, gb_free=29.4, wall=5568 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:04:57]    INFO >> epoch 012 | loss 2.131 | nll_loss 0.21 | ppl 1.16 | wps 1225.4 | ups 5.43 | wpb 225.5 | bsz 4 | num_updates 30897 | lr 4.9e-05 | gnorm 0.697 | clip 0 | loss_scale 1024 | train_wall 282 | gb_free 29.4 | wall 5629 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:05:54]    INFO >> epoch 012 | valid on 'valid' subset | loss 2.306 | nll_loss 0.281 | ppl 1.22 | bleu 82.8752 | wps 596.9 | wpb 3764.1 | bsz 62.5 | num_updates 30897 | best_bleu 82.993 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:07:17]    INFO >> epoch 012 | valid on 'test' subset | loss 2.309 | nll_loss 0.285 | ppl 1.22 | bleu 83.1502 | wps 730.6 | wpb 3487.6 | bsz 62.5 | num_updates 30897 | best_bleu 83.1502 (progress_bar.py:269, print())
[2021-11-02 19:07:41]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 12 @ 30897 updates, score 82.875208) (writing took 24.661711 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 19:08:02]    INFO >> epoch 013:    103 / 2575 loss=2.13, nll_loss=0.209, ppl=1.16, wps=451.7, ups=2.03, wpb=222.6, bsz=4, num_updates=31000, lr=4.9e-05, gnorm=0.699, clip=0, loss_scale=1024, train_wall=57, gb_free=29.4, wall=5814 (progress_bar.py:262, log())
[2021-11-02 19:08:59]    INFO >> epoch 013:    603 / 2575 loss=2.121, nll_loss=0.201, ppl=1.15, wps=1981.5, ups=8.91, wpb=222.4, bsz=4, num_updates=31500, lr=4.8e-05, gnorm=0.634, clip=0, loss_scale=2048, train_wall=55, gb_free=29.3, wall=5870 (progress_bar.py:262, log())
[2021-11-02 19:10:01]    INFO >> epoch 013:   1103 / 2575 loss=2.121, nll_loss=0.201, ppl=1.15, wps=1748.5, ups=8.03, wpb=217.6, bsz=4, num_updates=32000, lr=4.8e-05, gnorm=0.585, clip=0, loss_scale=2048, train_wall=61, gb_free=29.4, wall=5933 (progress_bar.py:262, log())
[2021-11-02 19:10:51]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 19:10:51]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-02 19:11:03]    INFO >> epoch 013:   1604 / 2575 loss=2.113, nll_loss=0.195, ppl=1.14, wps=1709.7, ups=8.03, wpb=212.9, bsz=4, num_updates=32500, lr=4.8e-05, gnorm=0.605, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=5995 (progress_bar.py:262, log())
[2021-11-02 19:11:55]    INFO >> epoch 013:   2104 / 2575 loss=2.133, nll_loss=0.212, ppl=1.16, wps=2240.6, ups=9.54, wpb=234.8, bsz=4, num_updates=33000, lr=4.8e-05, gnorm=0.651, clip=0, loss_scale=1024, train_wall=51, gb_free=29.4, wall=6047 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:12:52]    INFO >> epoch 013 | loss 2.127 | nll_loss 0.206 | ppl 1.15 | wps 1220.2 | ups 5.42 | wpb 225.3 | bsz 4 | num_updates 33471 | lr 4.8e-05 | gnorm 0.629 | clip 0 | loss_scale 1024 | train_wall 281 | gb_free 29.4 | wall 6104 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:13:48]    INFO >> epoch 013 | valid on 'valid' subset | loss 2.311 | nll_loss 0.283 | ppl 1.22 | bleu 82.7515 | wps 601.9 | wpb 3764.1 | bsz 62.5 | num_updates 33471 | best_bleu 82.993 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:15:12]    INFO >> epoch 013 | valid on 'test' subset | loss 2.314 | nll_loss 0.286 | ppl 1.22 | bleu 83.0257 | wps 733.1 | wpb 3487.6 | bsz 62.5 | num_updates 33471 | best_bleu 83.0257 (progress_bar.py:269, print())
[2021-11-02 19:15:23]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 13 @ 33471 updates, score 82.751507) (writing took 11.621495 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 19:15:35]    INFO >> epoch 014:     29 / 2575 loss=2.144, nll_loss=0.222, ppl=1.17, wps=550.2, ups=2.28, wpb=241.7, bsz=4, num_updates=33500, lr=4.8e-05, gnorm=0.662, clip=0, loss_scale=1024, train_wall=44, gb_free=29.4, wall=6267 (progress_bar.py:262, log())
[2021-11-02 19:16:33]    INFO >> epoch 014:    529 / 2575 loss=2.131, nll_loss=0.209, ppl=1.16, wps=1996.2, ups=8.66, wpb=230.4, bsz=4, num_updates=34000, lr=4.8e-05, gnorm=0.617, clip=0, loss_scale=1024, train_wall=57, gb_free=29.4, wall=6325 (progress_bar.py:262, log())
[2021-11-02 19:17:31]    INFO >> epoch 014:   1029 / 2575 loss=2.129, nll_loss=0.208, ppl=1.15, wps=1900, ups=8.65, wpb=219.7, bsz=4, num_updates=34500, lr=4.8e-05, gnorm=0.682, clip=0, loss_scale=2048, train_wall=57, gb_free=29.4, wall=6383 (progress_bar.py:262, log())
[2021-11-02 19:18:29]    INFO >> epoch 014:   1529 / 2575 loss=2.127, nll_loss=0.207, ppl=1.15, wps=1963.6, ups=8.64, wpb=227.4, bsz=4, num_updates=35000, lr=4.8e-05, gnorm=0.622, clip=0, loss_scale=2048, train_wall=57, gb_free=29.4, wall=6440 (progress_bar.py:262, log())
[2021-11-02 19:19:03]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 19:19:31]    INFO >> epoch 014:   2029 / 2575 loss=2.128, nll_loss=0.208, ppl=1.16, wps=1781.6, ups=8.02, wpb=222.2, bsz=4, num_updates=35500, lr=4.8e-05, gnorm=0.629, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=6503 (progress_bar.py:262, log())
[2021-11-02 19:20:34]    INFO >> epoch 014:   2529 / 2575 loss=2.129, nll_loss=0.208, ppl=1.16, wps=1824.9, ups=7.97, wpb=229, bsz=4, num_updates=36000, lr=4.8e-05, gnorm=0.663, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=6566 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:20:54]    INFO >> epoch 014 | loss 2.128 | nll_loss 0.207 | ppl 1.15 | wps 1205.3 | ups 5.34 | wpb 225.5 | bsz 4 | num_updates 36046 | lr 4.8e-05 | gnorm 0.64 | clip 0 | loss_scale 1024 | train_wall 301 | gb_free 29.4 | wall 6586 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:21:51]    INFO >> epoch 014 | valid on 'valid' subset | loss 2.316 | nll_loss 0.284 | ppl 1.22 | bleu 81.9308 | wps 589.5 | wpb 3764.1 | bsz 62.5 | num_updates 36046 | best_bleu 82.993 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:23:16]    INFO >> epoch 014 | valid on 'test' subset | loss 2.321 | nll_loss 0.288 | ppl 1.22 | bleu 83.0032 | wps 718 | wpb 3487.6 | bsz 62.5 | num_updates 36046 | best_bleu 83.0032 (progress_bar.py:269, print())
[2021-11-02 19:23:28]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 14 @ 36046 updates, score 81.930795) (writing took 11.767129 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 19:24:26]    INFO >> epoch 015:    454 / 2575 loss=2.124, nll_loss=0.205, ppl=1.15, wps=480.7, ups=2.15, wpb=223.9, bsz=4, num_updates=36500, lr=4.8e-05, gnorm=0.619, clip=0, loss_scale=1024, train_wall=55, gb_free=29.4, wall=6798 (progress_bar.py:262, log())
[2021-11-02 19:25:26]    INFO >> epoch 015:    954 / 2575 loss=2.124, nll_loss=0.203, ppl=1.15, wps=1838.4, ups=8.36, wpb=219.9, bsz=4, num_updates=37000, lr=4.8e-05, gnorm=0.608, clip=0, loss_scale=1024, train_wall=59, gb_free=29.4, wall=6858 (progress_bar.py:262, log())
[2021-11-02 19:26:27]    INFO >> epoch 015:   1454 / 2575 loss=2.123, nll_loss=0.203, ppl=1.15, wps=1836.9, ups=8.26, wpb=222.3, bsz=4, num_updates=37500, lr=4.8e-05, gnorm=0.623, clip=0, loss_scale=2048, train_wall=59, gb_free=29.4, wall=6919 (progress_bar.py:262, log())
[2021-11-02 19:26:38]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 19:27:25]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 19:27:25]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-02 19:27:30]    INFO >> epoch 015:   1955 / 2575 loss=2.138, nll_loss=0.218, ppl=1.16, wps=1855.9, ups=7.96, wpb=233.2, bsz=4, num_updates=38000, lr=4.8e-05, gnorm=0.657, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=6982 (progress_bar.py:262, log())
[2021-11-02 19:28:19]    INFO >> epoch 015:   2455 / 2575 loss=2.138, nll_loss=0.217, ppl=1.16, wps=2333.2, ups=10.1, wpb=231, bsz=4, num_updates=38500, lr=4.8e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=48, gb_free=29.4, wall=7031 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:28:49]    INFO >> epoch 015 | loss 2.131 | nll_loss 0.21 | ppl 1.16 | wps 1222.2 | ups 5.42 | wpb 225.5 | bsz 4 | num_updates 38620 | lr 4.8e-05 | gnorm 0.653 | clip 0 | loss_scale 512 | train_wall 292 | gb_free 29.4 | wall 7061 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:29:47]    INFO >> epoch 015 | valid on 'valid' subset | loss 2.307 | nll_loss 0.289 | ppl 1.22 | bleu 82.0596 | wps 593.8 | wpb 3764.1 | bsz 62.5 | num_updates 38620 | best_bleu 82.993 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:31:11]    INFO >> epoch 015 | valid on 'test' subset | loss 2.311 | nll_loss 0.293 | ppl 1.23 | bleu 82.8732 | wps 727.3 | wpb 3487.6 | bsz 62.5 | num_updates 38620 | best_bleu 82.993 (progress_bar.py:269, print())
[2021-11-02 19:31:23]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 15 @ 38620 updates, score 82.059594) (writing took 11.653655 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 19:32:16]    INFO >> epoch 016:    380 / 2575 loss=2.13, nll_loss=0.209, ppl=1.16, wps=462.9, ups=2.11, wpb=219.1, bsz=4, num_updates=39000, lr=4.8e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=59, gb_free=29.4, wall=7268 (progress_bar.py:262, log())
[2021-11-02 19:33:19]    INFO >> epoch 016:    880 / 2575 loss=2.136, nll_loss=0.216, ppl=1.16, wps=1874.9, ups=7.97, wpb=235.1, bsz=4, num_updates=39500, lr=4.8e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=61, gb_free=29.4, wall=7330 (progress_bar.py:262, log())
[2021-11-02 19:34:21]    INFO >> epoch 016:   1380 / 2575 loss=2.127, nll_loss=0.208, ppl=1.15, wps=1764.6, ups=8.04, wpb=219.5, bsz=4, num_updates=40000, lr=4.8e-05, gnorm=0.736, clip=0, loss_scale=1024, train_wall=61, gb_free=28.9, wall=7393 (progress_bar.py:262, log())
[2021-11-02 19:35:23]    INFO >> epoch 016:   1880 / 2575 loss=2.134, nll_loss=0.214, ppl=1.16, wps=1849.4, ups=8.02, wpb=230.6, bsz=4, num_updates=40500, lr=4.8e-05, gnorm=0.702, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=7455 (progress_bar.py:262, log())
[2021-11-02 19:36:30]    INFO >> epoch 016:   2380 / 2575 loss=2.131, nll_loss=0.212, ppl=1.16, wps=1640.9, ups=7.53, wpb=218, bsz=4, num_updates=41000, lr=4.8e-05, gnorm=0.722, clip=0, loss_scale=1024, train_wall=65, gb_free=29.4, wall=7521 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:37:11]    INFO >> epoch 016 | loss 2.133 | nll_loss 0.213 | ppl 1.16 | wps 1157.6 | ups 5.13 | wpb 225.5 | bsz 4 | num_updates 41195 | lr 4.8e-05 | gnorm 0.705 | clip 0 | loss_scale 1024 | train_wall 318 | gb_free 29.4 | wall 7563 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:38:08]    INFO >> epoch 016 | valid on 'valid' subset | loss 2.316 | nll_loss 0.283 | ppl 1.22 | bleu 82.5994 | wps 596.1 | wpb 3764.1 | bsz 62.5 | num_updates 41195 | best_bleu 82.993 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:39:33]    INFO >> epoch 016 | valid on 'test' subset | loss 2.322 | nll_loss 0.29 | ppl 1.22 | bleu 82.433 | wps 726.4 | wpb 3487.6 | bsz 62.5 | num_updates 41195 | best_bleu 82.993 (progress_bar.py:269, print())
[2021-11-02 19:39:46]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 16 @ 41195 updates, score 82.59942) (writing took 12.796487 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 19:40:35]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 19:40:38]    INFO >> epoch 017:    305 / 2575 loss=2.138, nll_loss=0.217, ppl=1.16, wps=472.6, ups=2.02, wpb=233.9, bsz=4, num_updates=41500, lr=4.8e-05, gnorm=0.67, clip=0, loss_scale=512, train_wall=68, gb_free=29.4, wall=7769 (progress_bar.py:262, log())
[2021-11-02 19:41:46]    INFO >> epoch 017:    805 / 2575 loss=2.126, nll_loss=0.206, ppl=1.15, wps=1600.6, ups=7.36, wpb=217.5, bsz=4, num_updates=42000, lr=4.8e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=66, gb_free=29.4, wall=7837 (progress_bar.py:262, log())
[2021-11-02 19:42:56]    INFO >> epoch 017:   1305 / 2575 loss=2.131, nll_loss=0.211, ppl=1.16, wps=1612.2, ups=7.17, wpb=224.8, bsz=4, num_updates=42500, lr=4.8e-05, gnorm=0.734, clip=0, loss_scale=512, train_wall=68, gb_free=29.4, wall=7907 (progress_bar.py:262, log())
[2021-11-02 19:44:04]    INFO >> epoch 017:   1805 / 2575 loss=2.136, nll_loss=0.217, ppl=1.16, wps=1697.3, ups=7.29, wpb=232.9, bsz=4, num_updates=43000, lr=4.8e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=67, gb_free=29.4, wall=7975 (progress_bar.py:262, log())
[2021-11-02 19:45:12]    INFO >> epoch 017:   2305 / 2575 loss=2.136, nll_loss=0.216, ppl=1.16, wps=1688.8, ups=7.45, wpb=226.8, bsz=4, num_updates=43500, lr=4.8e-05, gnorm=0.804, clip=0, loss_scale=1024, train_wall=66, gb_free=29.4, wall=8042 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:46:04]    INFO >> epoch 017 | loss 2.133 | nll_loss 0.213 | ppl 1.16 | wps 1092.6 | ups 4.84 | wpb 225.5 | bsz 4 | num_updates 43770 | lr 4.8e-05 | gnorm 0.722 | clip 0 | loss_scale 1024 | train_wall 346 | gb_free 29.4 | wall 8094 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:47:04]    INFO >> epoch 017 | valid on 'valid' subset | loss 2.314 | nll_loss 0.278 | ppl 1.21 | bleu 83.0313 | wps 548.9 | wpb 3764.1 | bsz 62.5 | num_updates 43770 | best_bleu 83.0313 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:48:33]    INFO >> epoch 017 | valid on 'test' subset | loss 2.32 | nll_loss 0.284 | ppl 1.22 | bleu 82.7735 | wps 679.6 | wpb 3487.6 | bsz 62.5 | num_updates 43770 | best_bleu 82.993 (progress_bar.py:269, print())
[2021-11-02 19:48:54]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_best.pt (epoch 17 @ 43770 updates, score 83.031317) (writing took 20.843776 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 19:49:32]    INFO >> epoch 018:    230 / 2575 loss=2.133, nll_loss=0.214, ppl=1.16, wps=424.6, ups=1.92, wpb=221.1, bsz=4, num_updates=44000, lr=4.8e-05, gnorm=0.722, clip=0, loss_scale=1024, train_wall=67, gb_free=29.4, wall=8303 (progress_bar.py:262, log())
[2021-11-02 19:50:38]    INFO >> epoch 018:    730 / 2575 loss=2.123, nll_loss=0.205, ppl=1.15, wps=1602.3, ups=7.55, wpb=212.1, bsz=4, num_updates=44500, lr=4.8e-05, gnorm=0.688, clip=0, loss_scale=1024, train_wall=65, gb_free=29.4, wall=8369 (progress_bar.py:262, log())
[2021-11-02 19:51:46]    INFO >> epoch 018:   1230 / 2575 loss=2.135, nll_loss=0.215, ppl=1.16, wps=1671.2, ups=7.35, wpb=227.3, bsz=4, num_updates=45000, lr=4.8e-05, gnorm=0.703, clip=0, loss_scale=1024, train_wall=66, gb_free=29.4, wall=8437 (progress_bar.py:262, log())
[2021-11-02 19:52:53]    INFO >> epoch 018:   1730 / 2575 loss=2.132, nll_loss=0.213, ppl=1.16, wps=1718, ups=7.43, wpb=231.3, bsz=4, num_updates=45500, lr=4.8e-05, gnorm=0.66, clip=0, loss_scale=2048, train_wall=66, gb_free=29.4, wall=8504 (progress_bar.py:262, log())
[2021-11-02 19:54:03]    INFO >> epoch 018:   2230 / 2575 loss=2.13, nll_loss=0.211, ppl=1.16, wps=1667.3, ups=7.17, wpb=232.4, bsz=4, num_updates=46000, lr=4.8e-05, gnorm=0.649, clip=0, loss_scale=2048, train_wall=68, gb_free=29.4, wall=8574 (progress_bar.py:262, log())
[2021-11-02 19:54:06]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:55:05]    INFO >> epoch 018 | loss 2.13 | nll_loss 0.211 | ppl 1.16 | wps 1071.9 | ups 4.75 | wpb 225.5 | bsz 4 | num_updates 46345 | lr 4.8e-05 | gnorm 0.684 | clip 0 | loss_scale 1024 | train_wall 342 | gb_free 29.4 | wall 8636 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:56:05]    INFO >> epoch 018 | valid on 'valid' subset | loss 2.312 | nll_loss 0.289 | ppl 1.22 | bleu 82.4499 | wps 561.1 | wpb 3764.1 | bsz 62.5 | num_updates 46345 | best_bleu 83.0313 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 19:57:36]    INFO >> epoch 018 | valid on 'test' subset | loss 2.318 | nll_loss 0.296 | ppl 1.23 | bleu 82.7577 | wps 666.9 | wpb 3487.6 | bsz 62.5 | num_updates 46345 | best_bleu 83.0313 (progress_bar.py:269, print())
[2021-11-02 19:57:48]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 18 @ 46345 updates, score 82.44988) (writing took 11.779071 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 19:58:15]    INFO >> epoch 019:    155 / 2575 loss=2.138, nll_loss=0.219, ppl=1.16, wps=458.7, ups=1.99, wpb=230.6, bsz=4, num_updates=46500, lr=4.8e-05, gnorm=0.711, clip=0, loss_scale=1024, train_wall=65, gb_free=29.4, wall=8825 (progress_bar.py:262, log())
[2021-11-02 19:59:22]    INFO >> epoch 019:    655 / 2575 loss=2.134, nll_loss=0.214, ppl=1.16, wps=1715.3, ups=7.48, wpb=229.4, bsz=4, num_updates=47000, lr=4.8e-05, gnorm=0.671, clip=0, loss_scale=1024, train_wall=65, gb_free=29.4, wall=8892 (progress_bar.py:262, log())
[2021-11-02 20:00:09]    INFO >> epoch 019:   1155 / 2575 loss=2.124, nll_loss=0.206, ppl=1.15, wps=2288, ups=10.51, wpb=217.7, bsz=4, num_updates=47500, lr=4.8e-05, gnorm=0.648, clip=0, loss_scale=1024, train_wall=46, gb_free=29.4, wall=8940 (progress_bar.py:262, log())
[2021-11-02 20:01:04]    INFO >> epoch 019:   1655 / 2575 loss=2.123, nll_loss=0.205, ppl=1.15, wps=1959.5, ups=9.08, wpb=215.8, bsz=4, num_updates=48000, lr=4.8e-05, gnorm=0.655, clip=0, loss_scale=1024, train_wall=54, gb_free=29.4, wall=8995 (progress_bar.py:262, log())
[2021-11-02 20:01:23]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 20:01:23]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-02 20:02:02]    INFO >> epoch 019:   2156 / 2575 loss=2.139, nll_loss=0.22, ppl=1.16, wps=2015, ups=8.62, wpb=233.7, bsz=4, num_updates=48500, lr=4.8e-05, gnorm=0.682, clip=0, loss_scale=1024, train_wall=57, gb_free=29.4, wall=9053 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:03:07]    INFO >> epoch 019 | loss 2.132 | nll_loss 0.213 | ppl 1.16 | wps 1204.3 | ups 5.34 | wpb 225.4 | bsz 4 | num_updates 48919 | lr 4.8e-05 | gnorm 0.67 | clip 0 | loss_scale 1024 | train_wall 290 | gb_free 29.3 | wall 9118 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:04:09]    INFO >> epoch 019 | valid on 'valid' subset | loss 2.317 | nll_loss 0.284 | ppl 1.22 | bleu 82.0026 | wps 555 | wpb 3764.1 | bsz 62.5 | num_updates 48919 | best_bleu 83.0313 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:05:37]    INFO >> epoch 019 | valid on 'test' subset | loss 2.321 | nll_loss 0.289 | ppl 1.22 | bleu 82.8621 | wps 686.5 | wpb 3487.6 | bsz 62.5 | num_updates 48919 | best_bleu 83.0313 (progress_bar.py:269, print())
[2021-11-02 20:05:48]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 19 @ 48919 updates, score 82.002612) (writing took 10.896282 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 20:06:04]    INFO >> epoch 020:     81 / 2575 loss=2.13, nll_loss=0.211, ppl=1.16, wps=449.9, ups=2.07, wpb=217.6, bsz=4, num_updates=49000, lr=4.8e-05, gnorm=0.685, clip=0, loss_scale=1024, train_wall=58, gb_free=29.4, wall=9294 (progress_bar.py:262, log())
[2021-11-02 20:07:02]    INFO >> epoch 020:    581 / 2575 loss=2.127, nll_loss=0.21, ppl=1.16, wps=1855.7, ups=8.67, wpb=214, bsz=4, num_updates=49500, lr=4.8e-05, gnorm=0.651, clip=0, loss_scale=1024, train_wall=57, gb_free=29.4, wall=9352 (progress_bar.py:262, log())
[2021-11-02 20:08:00]    INFO >> epoch 020:   1081 / 2575 loss=2.137, nll_loss=0.218, ppl=1.16, wps=2011, ups=8.64, wpb=232.9, bsz=4, num_updates=50000, lr=4.8e-05, gnorm=0.676, clip=0, loss_scale=1024, train_wall=57, gb_free=28.7, wall=9410 (progress_bar.py:262, log())
[2021-11-02 20:08:46]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 20:08:58]    INFO >> epoch 020:   1581 / 2575 loss=2.125, nll_loss=0.207, ppl=1.15, wps=1975, ups=8.67, wpb=227.7, bsz=4, num_updates=50500, lr=4.8e-05, gnorm=0.697, clip=0, loss_scale=1024, train_wall=56, gb_free=29.4, wall=9468 (progress_bar.py:262, log())
[2021-11-02 20:09:55]    INFO >> epoch 020:   2081 / 2575 loss=2.148, nll_loss=0.229, ppl=1.17, wps=2025.6, ups=8.71, wpb=232.6, bsz=4, num_updates=51000, lr=4.8e-05, gnorm=0.761, clip=0, loss_scale=1024, train_wall=56, gb_free=29.4, wall=9525 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:11:07]    INFO >> epoch 020 | loss 2.134 | nll_loss 0.216 | ppl 1.16 | wps 1212.1 | ups 5.37 | wpb 225.5 | bsz 4 | num_updates 51494 | lr 4.7e-05 | gnorm 0.71 | clip 0 | loss_scale 1024 | train_wall 291 | gb_free 29.4 | wall 9597 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:12:03]    INFO >> epoch 020 | valid on 'valid' subset | loss 2.319 | nll_loss 0.288 | ppl 1.22 | bleu 81.9327 | wps 596.6 | wpb 3764.1 | bsz 62.5 | num_updates 51494 | best_bleu 83.0313 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:13:25]    INFO >> epoch 020 | valid on 'test' subset | loss 2.326 | nll_loss 0.297 | ppl 1.23 | bleu 82.754 | wps 733.5 | wpb 3487.6 | bsz 62.5 | num_updates 51494 | best_bleu 83.0313 (progress_bar.py:269, print())
[2021-11-02 20:13:36]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 20 @ 51494 updates, score 81.932749) (writing took 11.140250 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 20:13:45]    INFO >> epoch 021:      6 / 2575 loss=2.135, nll_loss=0.216, ppl=1.16, wps=489, ups=2.18, wpb=224.1, bsz=4, num_updates=51500, lr=4.7e-05, gnorm=0.773, clip=0, loss_scale=1024, train_wall=56, gb_free=29.4, wall=9754 (progress_bar.py:262, log())
[2021-11-02 20:14:32]    INFO >> epoch 021:    506 / 2575 loss=2.138, nll_loss=0.218, ppl=1.16, wps=2451.4, ups=10.54, wpb=232.6, bsz=4, num_updates=52000, lr=4.7e-05, gnorm=0.695, clip=0, loss_scale=1024, train_wall=46, gb_free=29.4, wall=9802 (progress_bar.py:262, log())
[2021-11-02 20:15:22]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 20:15:31]    INFO >> epoch 021:   1006 / 2575 loss=2.134, nll_loss=0.215, ppl=1.16, wps=1837.8, ups=8.44, wpb=217.8, bsz=4, num_updates=52500, lr=4.7e-05, gnorm=0.713, clip=0, loss_scale=1024, train_wall=58, gb_free=29.4, wall=9861 (progress_bar.py:262, log())
[2021-11-02 20:16:17]    INFO >> epoch 021:   1506 / 2575 loss=2.143, nll_loss=0.225, ppl=1.17, wps=2477.5, ups=10.86, wpb=228.2, bsz=4, num_updates=53000, lr=4.7e-05, gnorm=0.716, clip=0, loss_scale=1024, train_wall=45, gb_free=28.2, wall=9907 (progress_bar.py:262, log())
[2021-11-02 20:17:20]    INFO >> epoch 021:   2006 / 2575 loss=2.137, nll_loss=0.218, ppl=1.16, wps=1829.3, ups=7.97, wpb=229.6, bsz=4, num_updates=53500, lr=4.7e-05, gnorm=0.777, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=9970 (progress_bar.py:262, log())
[2021-11-02 20:18:19]    INFO >> epoch 021:   2506 / 2575 loss=2.127, nll_loss=0.21, ppl=1.16, wps=1858.7, ups=8.42, wpb=220.7, bsz=4, num_updates=54000, lr=4.7e-05, gnorm=0.707, clip=0, loss_scale=1024, train_wall=58, gb_free=29.4, wall=10029 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:18:41]    INFO >> epoch 021 | loss 2.136 | nll_loss 0.217 | ppl 1.16 | wps 1279.7 | ups 5.67 | wpb 225.5 | bsz 4 | num_updates 54069 | lr 4.7e-05 | gnorm 0.719 | clip 0 | loss_scale 1024 | train_wall 276 | gb_free 29.4 | wall 10051 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:19:37]    INFO >> epoch 021 | valid on 'valid' subset | loss 2.313 | nll_loss 0.293 | ppl 1.22 | bleu 81.2231 | wps 601.9 | wpb 3764.1 | bsz 62.5 | num_updates 54069 | best_bleu 83.0313 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:20:59]    INFO >> epoch 021 | valid on 'test' subset | loss 2.322 | nll_loss 0.302 | ppl 1.23 | bleu 82.55 | wps 731.4 | wpb 3487.6 | bsz 62.5 | num_updates 54069 | best_bleu 83.0313 (progress_bar.py:269, print())
[2021-11-02 20:21:11]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 21 @ 54069 updates, score 81.223136) (writing took 11.332854 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 20:22:04]    INFO >> epoch 022:    431 / 2575 loss=2.13, nll_loss=0.212, ppl=1.16, wps=496.8, ups=2.23, wpb=223.1, bsz=4, num_updates=54500, lr=4.7e-05, gnorm=0.697, clip=0, loss_scale=2048, train_wall=51, gb_free=29.4, wall=10254 (progress_bar.py:262, log())
[2021-11-02 20:23:05]    INFO >> epoch 022:    931 / 2575 loss=2.138, nll_loss=0.22, ppl=1.16, wps=1902.4, ups=8.2, wpb=232, bsz=4, num_updates=55000, lr=4.7e-05, gnorm=0.657, clip=0, loss_scale=2048, train_wall=60, gb_free=29.4, wall=10315 (progress_bar.py:262, log())
[2021-11-02 20:23:40]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 20:23:40]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-02 20:24:07]    INFO >> epoch 022:   1432 / 2575 loss=2.133, nll_loss=0.214, ppl=1.16, wps=1829.7, ups=8.04, wpb=227.7, bsz=4, num_updates=55500, lr=4.7e-05, gnorm=0.712, clip=0, loss_scale=1024, train_wall=61, gb_free=29.4, wall=10377 (progress_bar.py:262, log())
[2021-11-02 20:25:03]    INFO >> epoch 022:   1932 / 2575 loss=2.149, nll_loss=0.232, ppl=1.17, wps=1975.4, ups=8.96, wpb=220.6, bsz=4, num_updates=56000, lr=4.7e-05, gnorm=0.76, clip=0, loss_scale=1024, train_wall=55, gb_free=29, wall=10433 (progress_bar.py:262, log())
[2021-11-02 20:26:02]    INFO >> epoch 022:   2432 / 2575 loss=2.145, nll_loss=0.228, ppl=1.17, wps=1942.8, ups=8.41, wpb=230.9, bsz=4, num_updates=56500, lr=4.7e-05, gnorm=0.746, clip=0, loss_scale=1024, train_wall=58, gb_free=29.4, wall=10492 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:26:35]    INFO >> epoch 022 | loss 2.138 | nll_loss 0.22 | ppl 1.16 | wps 1222.4 | ups 5.43 | wpb 225.3 | bsz 4 | num_updates 56643 | lr 4.7e-05 | gnorm 0.724 | clip 0 | loss_scale 1024 | train_wall 295 | gb_free 29.4 | wall 10525 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:27:30]    INFO >> epoch 022 | valid on 'valid' subset | loss 2.323 | nll_loss 0.293 | ppl 1.22 | bleu 81.6262 | wps 603.7 | wpb 3764.1 | bsz 62.5 | num_updates 56643 | best_bleu 83.0313 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:28:55]    INFO >> epoch 022 | valid on 'test' subset | loss 2.329 | nll_loss 0.3 | ppl 1.23 | bleu 82.5194 | wps 709.6 | wpb 3487.6 | bsz 62.5 | num_updates 56643 | best_bleu 83.0313 (progress_bar.py:269, print())
[2021-11-02 20:29:06]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 22 @ 56643 updates, score 81.626176) (writing took 11.116791 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 20:29:54]    INFO >> epoch 023:    357 / 2575 loss=2.126, nll_loss=0.209, ppl=1.16, wps=446.1, ups=2.16, wpb=206.8, bsz=4, num_updates=57000, lr=4.7e-05, gnorm=0.779, clip=0, loss_scale=1024, train_wall=58, gb_free=29.4, wall=10724 (progress_bar.py:262, log())
[2021-11-02 20:30:40]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 20:30:52]    INFO >> epoch 023:    857 / 2575 loss=2.139, nll_loss=0.221, ppl=1.17, wps=1937.4, ups=8.65, wpb=224.1, bsz=4, num_updates=57500, lr=4.7e-05, gnorm=0.758, clip=0, loss_scale=1024, train_wall=57, gb_free=29.4, wall=10782 (progress_bar.py:262, log())
[2021-11-02 20:31:51]    INFO >> epoch 023:   1357 / 2575 loss=2.146, nll_loss=0.229, ppl=1.17, wps=1967.6, ups=8.47, wpb=232.2, bsz=4, num_updates=58000, lr=4.7e-05, gnorm=0.779, clip=0, loss_scale=1024, train_wall=58, gb_free=29.4, wall=10841 (progress_bar.py:262, log())
[2021-11-02 20:32:53]    INFO >> epoch 023:   1857 / 2575 loss=2.142, nll_loss=0.224, ppl=1.17, wps=1836, ups=8.13, wpb=225.9, bsz=4, num_updates=58500, lr=4.7e-05, gnorm=0.768, clip=0, loss_scale=1024, train_wall=60, gb_free=29.4, wall=10902 (progress_bar.py:262, log())
[2021-11-02 20:33:45]    INFO >> epoch 023:   2357 / 2575 loss=2.153, nll_loss=0.235, ppl=1.18, wps=2215.4, ups=9.63, wpb=230, bsz=4, num_updates=59000, lr=4.7e-05, gnorm=0.833, clip=0, loss_scale=1024, train_wall=51, gb_free=29.4, wall=10954 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:34:20]    INFO >> epoch 023 | loss 2.143 | nll_loss 0.225 | ppl 1.17 | wps 1252 | ups 5.55 | wpb 225.5 | bsz 4 | num_updates 59218 | lr 4.7e-05 | gnorm 0.777 | clip 0 | loss_scale 1024 | train_wall 285 | gb_free 28.9 | wall 10989 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:35:16]    INFO >> epoch 023 | valid on 'valid' subset | loss 2.324 | nll_loss 0.297 | ppl 1.23 | bleu 81.507 | wps 599.2 | wpb 3764.1 | bsz 62.5 | num_updates 59218 | best_bleu 83.0313 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:36:39]    INFO >> epoch 023 | valid on 'test' subset | loss 2.332 | nll_loss 0.307 | ppl 1.24 | bleu 82.5319 | wps 734.8 | wpb 3487.6 | bsz 62.5 | num_updates 59218 | best_bleu 83.0313 (progress_bar.py:269, print())
[2021-11-02 20:36:50]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 23 @ 59218 updates, score 81.507012) (writing took 11.025476 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 20:37:15]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 20:37:27]    INFO >> epoch 024:    282 / 2575 loss=2.145, nll_loss=0.228, ppl=1.17, wps=494.7, ups=2.25, wpb=219.5, bsz=4, num_updates=59500, lr=4.7e-05, gnorm=0.785, clip=0, loss_scale=1024, train_wall=47, gb_free=29.4, wall=11176 (progress_bar.py:262, log())
[2021-11-02 20:38:24]    INFO >> epoch 024:    782 / 2575 loss=2.136, nll_loss=0.217, ppl=1.16, wps=1925.4, ups=8.8, wpb=218.8, bsz=4, num_updates=60000, lr=4.7e-05, gnorm=0.775, clip=0, loss_scale=1024, train_wall=56, gb_free=28.4, wall=11233 (progress_bar.py:262, log())
[2021-11-02 20:39:22]    INFO >> epoch 024:   1282 / 2575 loss=2.144, nll_loss=0.228, ppl=1.17, wps=2031.4, ups=8.65, wpb=234.8, bsz=4, num_updates=60500, lr=4.7e-05, gnorm=0.766, clip=0, loss_scale=1024, train_wall=57, gb_free=29.3, wall=11291 (progress_bar.py:262, log())
[2021-11-02 20:40:19]    INFO >> epoch 024:   1782 / 2575 loss=2.155, nll_loss=0.238, ppl=1.18, wps=1971.1, ups=8.73, wpb=225.9, bsz=4, num_updates=61000, lr=4.7e-05, gnorm=0.868, clip=0, loss_scale=1024, train_wall=56, gb_free=29.4, wall=11348 (progress_bar.py:262, log())
[2021-11-02 20:41:18]    INFO >> epoch 024:   2282 / 2575 loss=2.152, nll_loss=0.234, ppl=1.18, wps=2005.3, ups=8.58, wpb=233.8, bsz=4, num_updates=61500, lr=4.7e-05, gnorm=0.839, clip=0, loss_scale=2048, train_wall=57, gb_free=29.4, wall=11406 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:42:08]    INFO >> epoch 024 | loss 2.145 | nll_loss 0.228 | ppl 1.17 | wps 1242.4 | ups 5.51 | wpb 225.5 | bsz 4 | num_updates 61793 | lr 4.7e-05 | gnorm 0.805 | clip 0 | loss_scale 2048 | train_wall 288 | gb_free 29.4 | wall 11456 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:43:07]    INFO >> epoch 024 | valid on 'valid' subset | loss 2.324 | nll_loss 0.302 | ppl 1.23 | bleu 81.8519 | wps 550.7 | wpb 3764.1 | bsz 62.5 | num_updates 61793 | best_bleu 83.0313 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:44:30]    INFO >> epoch 024 | valid on 'test' subset | loss 2.33 | nll_loss 0.31 | ppl 1.24 | bleu 82.9117 | wps 738.4 | wpb 3487.6 | bsz 62.5 | num_updates 61793 | best_bleu 83.0313 (progress_bar.py:269, print())
[2021-11-02 20:44:42]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 24 @ 61793 updates, score 81.851926) (writing took 11.281529 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 20:45:11]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 20:45:11]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-02 20:45:15]    INFO >> epoch 025:    208 / 2575 loss=2.136, nll_loss=0.22, ppl=1.16, wps=465.6, ups=2.11, wpb=220.7, bsz=4, num_updates=62000, lr=4.7e-05, gnorm=0.745, clip=0, loss_scale=1024, train_wall=59, gb_free=29.4, wall=11643 (progress_bar.py:262, log())
[2021-11-02 20:45:29]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 20:46:15]    INFO >> epoch 025:    708 / 2575 loss=2.15, nll_loss=0.232, ppl=1.17, wps=1946.1, ups=8.35, wpb=233.1, bsz=4, num_updates=62500, lr=4.7e-05, gnorm=0.82, clip=0, loss_scale=512, train_wall=59, gb_free=29.4, wall=11703 (progress_bar.py:262, log())
[2021-11-02 20:47:09]    INFO >> epoch 025:   1208 / 2575 loss=2.159, nll_loss=0.242, ppl=1.18, wps=2126.4, ups=9.27, wpb=229.4, bsz=4, num_updates=63000, lr=4.7e-05, gnorm=0.943, clip=0, loss_scale=512, train_wall=53, gb_free=29.4, wall=11757 (progress_bar.py:262, log())
[2021-11-02 20:47:58]    INFO >> epoch 025:   1708 / 2575 loss=2.153, nll_loss=0.236, ppl=1.18, wps=2209.3, ups=10.27, wpb=215.1, bsz=4, num_updates=63500, lr=4.7e-05, gnorm=0.991, clip=0, loss_scale=512, train_wall=48, gb_free=29.4, wall=11806 (progress_bar.py:262, log())
[2021-11-02 20:48:55]    INFO >> epoch 025:   2208 / 2575 loss=2.155, nll_loss=0.239, ppl=1.18, wps=1988.9, ups=8.72, wpb=228, bsz=4, num_updates=64000, lr=4.7e-05, gnorm=0.971, clip=0, loss_scale=512, train_wall=56, gb_free=29.4, wall=11863 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:49:53]    INFO >> epoch 025 | loss 2.153 | nll_loss 0.237 | ppl 1.18 | wps 1250 | ups 5.54 | wpb 225.5 | bsz 4 | num_updates 64367 | lr 4.7e-05 | gnorm 0.921 | clip 0 | loss_scale 1024 | train_wall 282 | gb_free 29.3 | wall 11921 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:50:47]    INFO >> epoch 025 | valid on 'valid' subset | loss 2.327 | nll_loss 0.296 | ppl 1.23 | bleu 81.4715 | wps 606.2 | wpb 3764.1 | bsz 62.5 | num_updates 64367 | best_bleu 83.0313 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:52:07]    INFO >> epoch 025 | valid on 'test' subset | loss 2.334 | nll_loss 0.307 | ppl 1.24 | bleu 82.5604 | wps 750.8 | wpb 3487.6 | bsz 62.5 | num_updates 64367 | best_bleu 83.0313 (progress_bar.py:269, print())
[2021-11-02 20:52:19]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 25 @ 64367 updates, score 81.471473) (writing took 11.355339 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 20:52:44]    INFO >> epoch 026:    133 / 2575 loss=2.149, nll_loss=0.232, ppl=1.17, wps=487.1, ups=2.19, wpb=222.8, bsz=4, num_updates=64500, lr=4.7e-05, gnorm=0.911, clip=0, loss_scale=1024, train_wall=58, gb_free=29.4, wall=12092 (progress_bar.py:262, log())
[2021-11-02 20:53:33]    INFO >> epoch 026:    633 / 2575 loss=2.134, nll_loss=0.217, ppl=1.16, wps=2239.7, ups=10.09, wpb=222, bsz=4, num_updates=65000, lr=4.7e-05, gnorm=0.791, clip=0, loss_scale=1024, train_wall=48, gb_free=29.4, wall=12141 (progress_bar.py:262, log())
[2021-11-02 20:54:32]    INFO >> epoch 026:   1133 / 2575 loss=2.139, nll_loss=0.223, ppl=1.17, wps=1811.8, ups=8.46, wpb=214.2, bsz=4, num_updates=65500, lr=4.7e-05, gnorm=0.856, clip=0, loss_scale=1024, train_wall=58, gb_free=29.4, wall=12200 (progress_bar.py:262, log())
[2021-11-02 20:55:37]    INFO >> epoch 026:   1633 / 2575 loss=2.149, nll_loss=0.233, ppl=1.18, wps=1751.2, ups=7.73, wpb=226.4, bsz=4, num_updates=66000, lr=4.7e-05, gnorm=0.842, clip=0, loss_scale=1024, train_wall=63, gb_free=29.4, wall=12265 (progress_bar.py:262, log())
[2021-11-02 20:55:57]    INFO >> AMP: overflow detected, setting scale to to 1024.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 20:56:10]    INFO >> AMP: overflow detected, setting scale to to 512.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-02 20:56:10]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-02 20:56:33]    INFO >> epoch 026:   2134 / 2575 loss=2.168, nll_loss=0.251, ppl=1.19, wps=2176.1, ups=8.94, wpb=243.3, bsz=4, num_updates=66500, lr=4.7e-05, gnorm=0.873, clip=0, loss_scale=512, train_wall=55, gb_free=29.4, wall=12321 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:57:37]    INFO >> epoch 026 | loss 2.148 | nll_loss 0.232 | ppl 1.17 | wps 1247.1 | ups 5.54 | wpb 225.2 | bsz 4 | num_updates 66941 | lr 4.7e-05 | gnorm 0.849 | clip 0 | loss_scale 512 | train_wall 289 | gb_free 29.4 | wall 12386 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:58:32]    INFO >> epoch 026 | valid on 'valid' subset | loss 2.332 | nll_loss 0.316 | ppl 1.24 | bleu 81.1528 | wps 607.2 | wpb 3764.1 | bsz 62.5 | num_updates 66941 | best_bleu 83.0313 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 20:59:54]    INFO >> epoch 026 | valid on 'test' subset | loss 2.334 | nll_loss 0.32 | ppl 1.25 | bleu 82.4402 | wps 746 | wpb 3487.6 | bsz 62.5 | num_updates 66941 | best_bleu 83.0313 (progress_bar.py:269, print())
[2021-11-02 21:00:05]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 26 @ 66941 updates, score 81.15277) (writing took 10.659772 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 21:00:19]    INFO >> epoch 027:     59 / 2575 loss=2.16, nll_loss=0.244, ppl=1.18, wps=497.3, ups=2.21, wpb=225, bsz=4, num_updates=67000, lr=4.7e-05, gnorm=0.953, clip=0, loss_scale=512, train_wall=55, gb_free=29.4, wall=12547 (progress_bar.py:262, log())
[2021-11-02 21:01:17]    INFO >> epoch 027:    559 / 2575 loss=2.157, nll_loss=0.241, ppl=1.18, wps=1994.2, ups=8.67, wpb=230.1, bsz=4, num_updates=67500, lr=4.7e-05, gnorm=0.878, clip=0, loss_scale=512, train_wall=57, gb_free=29.4, wall=12605 (progress_bar.py:262, log())
[2021-11-02 21:02:15]    INFO >> epoch 027:   1059 / 2575 loss=2.148, nll_loss=0.233, ppl=1.18, wps=1920.5, ups=8.65, wpb=222, bsz=4, num_updates=68000, lr=4.7e-05, gnorm=0.877, clip=0, loss_scale=512, train_wall=57, gb_free=29.4, wall=12663 (progress_bar.py:262, log())
[2021-11-02 21:03:12]    INFO >> epoch 027:   1559 / 2575 loss=2.155, nll_loss=0.239, ppl=1.18, wps=1976.3, ups=8.75, wpb=226, bsz=4, num_updates=68500, lr=4.7e-05, gnorm=0.933, clip=0, loss_scale=1024, train_wall=56, gb_free=29.4, wall=12720 (progress_bar.py:262, log())
[2021-11-02 21:04:04]    INFO >> epoch 027:   2059 / 2575 loss=2.146, nll_loss=0.23, ppl=1.17, wps=2139.6, ups=9.61, wpb=222.7, bsz=4, num_updates=69000, lr=4.7e-05, gnorm=0.89, clip=0, loss_scale=1024, train_wall=51, gb_free=29.3, wall=12772 (progress_bar.py:262, log())
[2021-11-02 21:05:05]    INFO >> epoch 027:   2559 / 2575 loss=2.148, nll_loss=0.233, ppl=1.18, wps=1802.6, ups=8.14, wpb=221.3, bsz=4, num_updates=69500, lr=4.7e-05, gnorm=0.901, clip=0, loss_scale=1024, train_wall=60, gb_free=29.4, wall=12833 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 21:05:22]    INFO >> epoch 027 | loss 2.152 | nll_loss 0.236 | ppl 1.18 | wps 1249.8 | ups 5.54 | wpb 225.5 | bsz 4 | num_updates 69516 | lr 4.7e-05 | gnorm 0.9 | clip 0 | loss_scale 1024 | train_wall 289 | gb_free 29.4 | wall 12850 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 21:06:17]    INFO >> epoch 027 | valid on 'valid' subset | loss 2.326 | nll_loss 0.302 | ppl 1.23 | bleu 81.8386 | wps 603.1 | wpb 3764.1 | bsz 62.5 | num_updates 69516 | best_bleu 83.0313 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-02 21:07:39]    INFO >> epoch 027 | valid on 'test' subset | loss 2.332 | nll_loss 0.309 | ppl 1.24 | bleu 82.4772 | wps 747.4 | wpb 3487.6 | bsz 62.5 | num_updates 69516 | best_bleu 83.0313 (progress_bar.py:269, print())
[2021-11-02 21:07:50]    INFO >> saved checkpoint /home/wanyao/yang/ncc_data/codexglue/code_to_code/translation/vanilla/data-mmap/plbart/java-csharp/checkpoints/checkpoint_last.pt (epoch 27 @ 69516 updates, score 81.838627) (writing took 11.278142 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-11-02 21:07:50]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:181, should_stop_early())
[2021-11-02 21:07:50]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:282, single_main())
[2021-11-02 21:07:50]    INFO >> done training in 12997.9 seconds (train.py:293, single_main())
