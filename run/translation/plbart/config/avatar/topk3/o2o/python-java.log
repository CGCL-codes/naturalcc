nohup: ignoring input
Using backend: pytorch
[2021-10-22 10:51:49]    INFO >> Load arguments in /home/wanyao/yang/naturalcc-dev/run/translation/plbart/config/avatar/topk3/o2o/python-java.yml (train.py:309, cli_main())
[2021-10-22 10:51:49]    INFO >> {'criterion': 'label_smoothed_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 500, 'log_format': 'simple', 'tensorboard_logdir': '', 'memory_efficient_fp16': 0, 'fp16_no_flatten_grads': 1, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'empty_cache_freq': 0, 'task': 'bart_finetune', 'seed': 1234, 'cpu': 0, 'fp16': 0, 'fp16_opt_level': '01', 'bf16': 0, 'memory_efficient_bf16': 0, 'server_ip': '', 'server_port': '', 'amp': 1, 'amp_batch_retries': 2, 'amp_init_scale': '2 ** 7', 'amp_scale_window': None}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 4, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'pipeline_model_parallel': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1, 'block_momentum': 0.875, 'block_lr': 1, 'use_nbm': 0, 'average_sync': 0}, 'task': {'data': '/mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap', 'source_lang': 'python', 'target_lang': 'java', 'load_alignments': 0, 'left_pad_source': 0, 'left_pad_target': 0, 'max_source_positions': 511, 'max_target_positions': 511, 'upsample_primary': 1, 'truncate_source': 1, 'truncate_target': 1, 'append_eos_to_target': 1, 'eval_bleu': 1, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': True, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_args': None, 'eval_bleu_print_samples': 0, 'eval_with_sacrebleu': 1}, 'model': {'arch': 'fairseq_transformer', 'offset_positions_by_padding': 1, 'pooler_dropout': 0.1, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.1, 'encoder_positional_embeddings': 0, 'encoder_learned_pos': 1, 'encoder_max_relative_len': 0, 'encoder_embed_path': 0, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_layers': 6, 'encoder_attention_heads': 12, 'encoder_normalize_before': 0, 'decoder_embed_path': '', 'decoder_positional_embeddings': 0, 'decoder_learned_pos': 1, 'decoder_max_relative_len': 0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_attention_heads': 12, 'decoder_normalize_before': 0, 'no_decoder_final_norm': 0, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.1, 'adaptive_softmax_factor': 0.0, 'share_decoder_input_output_embed': 1, 'decoder_out_embed_bias': 1, 'share_all_embeddings': 1, 'adaptive_input': 0, 'adaptive_input_factor': 0.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': 0, 'tie_adaptive_proj': 0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 1, 'no_scale_embedding': 0, 'no_token_positional_embeddings': 0, 'encoder_dropout_in': 0.1, 'encoder_dropout_out': 0.1, 'decoder_dropout_in': 0.1, 'decoder_dropout_out': 0.1, 'max_source_positions': 1024, 'max_target_positions': 1024, 'multihead_attention_version': 'ncc', 'encoder_position_encoding_version': 'ncc_learned', 'decoder_position_encoding_version': 'ncc_learned'}, 'optimization': {'max_epoch': 0, 'max_update': 30000, 'clip_norm': 0.0, 'update_freq': [4], 'lrs': [5e-05], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1500, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'sentence_avg': 0, 'label_smoothing': 0.1, 'adam': {'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': 0}}, 'checkpoint': {'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': 1, 'patience': 10, 'save_dir': '/mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints', 'should_continue': 0, 'model_name_or_path': None, 'cache_dir': None, 'logging_steps': 500, 'save_steps': 2000, 'save_total_limit': 2, 'overwrite_output_dir': 0, 'overwrite_cache': 0, 'init_checkpoint': '/mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt'}, 'eval': {'path': '/mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt', 'remove_bpe': 'sentencepiece', 'quiet': 1, 'results_path': None, 'model_overrides': '{}', 'topk': 5, 'max_sentences': 4, 'beam': 5, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 500, 'min_len': 1, 'match_source_len': 0, 'no_early_stop': 1, 'unnormalized': 0, 'no_beamable_mm': 0, 'lenpen': 1, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': 0, 'score_reference': 0, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': 0, 'sampling_topk': -1, 'sampling_topp': -1, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 0, 'print_step': 0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': 0, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': 0, 'retain_iter_history': 0, 'decoding_format': None, 'nltk_bleu': 1, 'rouge': 1}} (train.py:311, cli_main())
[2021-10-22 10:51:49]    INFO >> single GPU training... (train.py:340, cli_main())
[2021-10-22 10:51:50]    INFO >> [python] dictionary: 50005 types (bart_finetune.py:128, setup_task())
[2021-10-22 10:51:50]    INFO >> [java] dictionary: 50005 types (bart_finetune.py:129, setup_task())
[2021-10-22 10:51:52]    INFO >> truncate python/valid.code_tokens to 511 (bart_finetune.py:72, load_langpair_dataset())
[2021-10-22 10:51:55]    INFO >> truncate java/valid.code_tokens to 511 (bart_finetune.py:88, load_langpair_dataset())
[2021-10-22 10:52:00]    INFO >> Restore parameters from /mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt (train.py:228, single_main())
[2021-10-22 10:52:00]    INFO >> FairseqTransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
) (train.py:231, single_main())
[2021-10-22 10:52:00]    INFO >> model fairseq_transformer, criterion LabelSmoothedCrossEntropyCriterion (train.py:232, single_main())
[2021-10-22 10:52:00]    INFO >> num. model params: 139220736 (num. trained: 139220736) (train.py:235, single_main())
[2021-10-22 10:52:06]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:542, pretty_print_cuda_env_list())
[2021-10-22 10:52:06]    INFO >> rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                     (utils.py:548, pretty_print_cuda_env_list())
[2021-10-22 10:52:06]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:550, pretty_print_cuda_env_list())
[2021-10-22 10:52:06]    INFO >> training on 1 GPUs (train.py:240, single_main())
[2021-10-22 10:52:06]    INFO >> max tokens per GPU = None and max sentences per GPU = 4 (train.py:243, single_main())
[2021-10-22 10:52:06]    INFO >> no existing checkpoint found /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())
[2021-10-22 10:52:06]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())
[2021-10-22 10:52:06]    INFO >> truncate python/train.code_tokens to 511 (bart_finetune.py:72, load_langpair_dataset())
[2021-10-22 10:52:06]    INFO >> truncate java/train.code_tokens to 511 (bart_finetune.py:88, load_langpair_dataset())
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:57: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
[2021-10-22 10:52:17]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 10:52:17]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 10:52:19]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 10:52:19]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-10-22 10:52:20]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 10:52:27]    INFO >> AMP: overflow detected, setting scale to to 4.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 10:56:48]    INFO >> epoch 001:    501 / 1326 loss=5.193, nll_loss=3.277, ppl=9.69, wps=5432.1, ups=1.86, wpb=2914, bsz=16, num_updates=500, lr=1.7e-05, gnorm=13.251, loss_scale=4, train_wall=263, gb_free=26.8, wall=281 (progress_bar.py:262, log())
[2021-10-22 11:02:01]    INFO >> epoch 001:   1001 / 1326 loss=3.125, nll_loss=1.236, ppl=2.36, wps=5547.3, ups=1.59, wpb=3479.9, bsz=16, num_updates=1000, lr=3.3e-05, gnorm=1.444, loss_scale=4, train_wall=308, gb_free=26.6, wall=595 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 11:07:40]    INFO >> epoch 001 | loss 3.714 | nll_loss 1.823 | ppl 3.54 | wps 5212.5 | ups 1.44 | wpb 3623.6 | bsz 16 | num_updates 1325 | lr 4.4e-05 | gnorm 5.834 | loss_scale 4 | train_wall 905 | gb_free 25.5 | wall 934 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 11:16:06]    INFO >> epoch 001 | valid on 'valid' subset | loss 2.672 | nll_loss 0.667 | ppl 1.59 | bleu 51.2248 | wps 369.2 | wpb 6755.2 | bsz 31.4 | num_updates 1325 (progress_bar.py:269, print())
[2021-10-22 11:16:12]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 1 @ 1325 updates, score 51.224757) (writing took 5.700302 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 11:18:01]    INFO >> epoch 002:    175 / 1326 loss=2.995, nll_loss=1.124, ppl=2.18, wps=2186.3, ups=0.52, wpb=4197, bsz=16, num_updates=1500, lr=5e-05, gnorm=1.258, loss_scale=4, train_wall=433, gb_free=27.5, wall=1555 (progress_bar.py:262, log())
[2021-10-22 11:22:49]    INFO >> epoch 002:    675 / 1326 loss=2.865, nll_loss=1.001, ppl=2, wps=5306.4, ups=1.74, wpb=3056.6, bsz=16, num_updates=2000, lr=5e-05, gnorm=1.185, loss_scale=4, train_wall=281, gb_free=27.6, wall=1843 (progress_bar.py:262, log())
[2021-10-22 11:29:16]    INFO >> epoch 002:   1175 / 1326 loss=2.795, nll_loss=0.93, ppl=1.9, wps=5015, ups=1.29, wpb=3883, bsz=16, num_updates=2500, lr=5e-05, gnorm=0.991, loss_scale=8, train_wall=381, gb_free=26, wall=2230 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 11:31:58]    INFO >> epoch 002 | loss 2.844 | nll_loss 0.979 | ppl 1.97 | wps 3294.9 | ups 0.91 | wpb 3623.1 | bsz 16 | num_updates 2651 | lr 5e-05 | gnorm 1.119 | loss_scale 8 | train_wall 921 | gb_free 25.5 | wall 2392 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 11:42:08]    INFO >> epoch 002 | valid on 'valid' subset | loss 2.586 | nll_loss 0.605 | ppl 1.52 | bleu 59.5332 | wps 301.2 | wpb 6755.2 | bsz 31.4 | num_updates 2651 | best_bleu 59.5332 (progress_bar.py:269, print())
[2021-10-22 11:42:21]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 2 @ 2651 updates, score 59.533228) (writing took 12.377943 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 11:45:35]    INFO >> epoch 003:    349 / 1326 loss=2.791, nll_loss=0.927, ppl=1.9, wps=1874.2, ups=0.51, wpb=3669.1, bsz=16, num_updates=3000, lr=5e-05, gnorm=1.043, loss_scale=8, train_wall=341, gb_free=28.9, wall=3209 (progress_bar.py:262, log())
[2021-10-22 11:50:35]    INFO >> epoch 003:    849 / 1326 loss=2.71, nll_loss=0.844, ppl=1.79, wps=5447, ups=1.66, wpb=3272.9, bsz=16, num_updates=3500, lr=5e-05, gnorm=0.973, loss_scale=8, train_wall=293, gb_free=26.7, wall=3509 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 11:57:00]    INFO >> epoch 003 | loss 2.716 | nll_loss 0.85 | ppl 1.8 | wps 3199 | ups 0.88 | wpb 3623.1 | bsz 16 | num_updates 3977 | lr 5e-05 | gnorm 0.968 | loss_scale 8 | train_wall 852 | gb_free 25.5 | wall 3894 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 12:06:47]    INFO >> epoch 003 | valid on 'valid' subset | loss 2.564 | nll_loss 0.583 | ppl 1.5 | bleu 59.2386 | wps 313.2 | wpb 6755.2 | bsz 31.4 | num_updates 3977 | best_bleu 59.5332 (progress_bar.py:269, print())
[2021-10-22 12:06:54]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 3 @ 3977 updates, score 59.238615) (writing took 7.236244 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 12:07:16]    INFO >> epoch 004:     23 / 1326 loss=2.713, nll_loss=0.847, ppl=1.8, wps=2242.9, ups=0.5, wpb=4488.3, bsz=16, num_updates=4000, lr=5e-05, gnorm=0.896, loss_scale=8, train_wall=391, gb_free=27.2, wall=4510 (progress_bar.py:262, log())
[2021-10-22 12:11:55]    INFO >> epoch 004:    523 / 1326 loss=2.63, nll_loss=0.758, ppl=1.69, wps=5248.8, ups=1.8, wpb=2923.2, bsz=16, num_updates=4500, lr=5e-05, gnorm=0.947, loss_scale=16, train_wall=271, gb_free=27.4, wall=4788 (progress_bar.py:262, log())
[2021-10-22 12:17:17]    INFO >> epoch 004:   1023 / 1326 loss=2.621, nll_loss=0.749, ppl=1.68, wps=5437.7, ups=1.55, wpb=3508.9, bsz=16, num_updates=5000, lr=5e-05, gnorm=0.866, loss_scale=16, train_wall=314, gb_free=26.3, wall=5111 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 12:21:47]    INFO >> epoch 004 | loss 2.635 | nll_loss 0.763 | ppl 1.7 | wps 3231.6 | ups 0.89 | wpb 3623.1 | bsz 16 | num_updates 5303 | lr 5e-05 | gnorm 0.894 | loss_scale 16 | train_wall 862 | gb_free 25.5 | wall 5381 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 12:31:42]    INFO >> epoch 004 | valid on 'valid' subset | loss 2.545 | nll_loss 0.575 | ppl 1.49 | bleu 60.2038 | wps 310.7 | wpb 6755.2 | bsz 31.4 | num_updates 5303 | best_bleu 60.2038 (progress_bar.py:269, print())
[2021-10-22 12:31:54]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 4 @ 5303 updates, score 60.203794) (writing took 12.881872 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 12:33:52]    INFO >> epoch 005:    197 / 1326 loss=2.635, nll_loss=0.762, ppl=1.7, wps=2082.2, ups=0.5, wpb=4143.9, bsz=16, num_updates=5500, lr=5e-05, gnorm=0.898, loss_scale=16, train_wall=369, gb_free=26.9, wall=6106 (progress_bar.py:262, log())
[2021-10-22 12:38:46]    INFO >> epoch 005:    697 / 1326 loss=2.573, nll_loss=0.698, ppl=1.62, wps=5244.5, ups=1.7, wpb=3085.5, bsz=16, num_updates=6000, lr=5e-05, gnorm=0.893, loss_scale=16, train_wall=288, gb_free=26.7, wall=6400 (progress_bar.py:262, log())
[2021-10-22 12:44:30]    INFO >> epoch 005:   1197 / 1326 loss=2.555, nll_loss=0.677, ppl=1.6, wps=5768.4, ups=1.46, wpb=3960.1, bsz=16, num_updates=6500, lr=5e-05, gnorm=0.819, loss_scale=32, train_wall=336, gb_free=25.9, wall=6743 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 12:46:49]    INFO >> epoch 005 | loss 2.579 | nll_loss 0.703 | ppl 1.63 | wps 3198.2 | ups 0.88 | wpb 3623.1 | bsz 16 | num_updates 6629 | lr 5e-05 | gnorm 0.872 | loss_scale 32 | train_wall 865 | gb_free 25.5 | wall 6883 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 12:56:32]    INFO >> epoch 005 | valid on 'valid' subset | loss 2.539 | nll_loss 0.571 | ppl 1.49 | bleu 59.9948 | wps 318.9 | wpb 6755.2 | bsz 31.4 | num_updates 6629 | best_bleu 60.2038 (progress_bar.py:269, print())
[2021-10-22 12:56:39]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 5 @ 6629 updates, score 59.994794) (writing took 7.235339 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 13:00:15]    INFO >> epoch 006:    371 / 1326 loss=2.577, nll_loss=0.7, ppl=1.62, wps=1886.7, ups=0.53, wpb=3566.6, bsz=16, num_updates=7000, lr=5e-05, gnorm=0.873, loss_scale=32, train_wall=339, gb_free=28.7, wall=7689 (progress_bar.py:262, log())
[2021-10-22 13:05:20]    INFO >> epoch 006:    871 / 1326 loss=2.521, nll_loss=0.641, ppl=1.56, wps=5417.6, ups=1.64, wpb=3307, bsz=16, num_updates=7500, lr=5e-05, gnorm=0.843, loss_scale=32, train_wall=298, gb_free=27.6, wall=7994 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 13:11:48]    INFO >> epoch 006 | loss 2.534 | nll_loss 0.654 | ppl 1.57 | wps 3204.8 | ups 0.88 | wpb 3623.1 | bsz 16 | num_updates 7955 | lr 5e-05 | gnorm 0.845 | loss_scale 32 | train_wall 882 | gb_free 25.5 | wall 8382 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 13:25:37]    INFO >> epoch 006 | valid on 'valid' subset | loss 2.538 | nll_loss 0.576 | ppl 1.49 | bleu 58.4965 | wps 258.5 | wpb 6755.2 | bsz 31.4 | num_updates 7955 | best_bleu 60.2038 (progress_bar.py:269, print())
[2021-10-22 13:25:44]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 6 @ 7955 updates, score 58.496483) (writing took 7.509380 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 13:26:19]    INFO >> epoch 007:     45 / 1326 loss=2.547, nll_loss=0.667, ppl=1.59, wps=1770.9, ups=0.4, wpb=4459.9, bsz=16, num_updates=8000, lr=5e-05, gnorm=0.821, loss_scale=32, train_wall=408, gb_free=27.2, wall=9253 (progress_bar.py:262, log())
[2021-10-22 13:31:07]    INFO >> epoch 007:    545 / 1326 loss=2.477, nll_loss=0.593, ppl=1.51, wps=5111.8, ups=1.74, wpb=2942.3, bsz=16, num_updates=8500, lr=5e-05, gnorm=0.843, loss_scale=64, train_wall=281, gb_free=26.7, wall=9541 (progress_bar.py:262, log())
[2021-10-22 13:36:24]    INFO >> epoch 007:   1045 / 1326 loss=2.479, nll_loss=0.594, ppl=1.51, wps=5581.2, ups=1.58, wpb=3539.9, bsz=16, num_updates=9000, lr=5e-05, gnorm=0.795, loss_scale=64, train_wall=310, gb_free=27.2, wall=9858 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 13:40:50]    INFO >> epoch 007 | loss 2.493 | nll_loss 0.608 | ppl 1.52 | wps 2758.2 | ups 0.76 | wpb 3623.1 | bsz 16 | num_updates 9281 | lr 5e-05 | gnorm 0.816 | loss_scale 64 | train_wall 879 | gb_free 25.5 | wall 10124 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 13:50:19]    INFO >> epoch 007 | valid on 'valid' subset | loss 2.535 | nll_loss 0.584 | ppl 1.5 | bleu 58.9717 | wps 329 | wpb 6755.2 | bsz 31.4 | num_updates 9281 | best_bleu 60.2038 (progress_bar.py:269, print())
[2021-10-22 13:50:26]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 7 @ 9281 updates, score 58.971685) (writing took 7.084781 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 13:52:37]    INFO >> epoch 008:    219 / 1326 loss=2.502, nll_loss=0.617, ppl=1.53, wps=2099.1, ups=0.51, wpb=4085.1, bsz=16, num_updates=9500, lr=5e-05, gnorm=0.824, loss_scale=64, train_wall=381, gb_free=28.7, wall=10831 (progress_bar.py:262, log())
[2021-10-22 13:57:34]    INFO >> epoch 008:    719 / 1326 loss=2.453, nll_loss=0.566, ppl=1.48, wps=5253.7, ups=1.69, wpb=3112.7, bsz=16, num_updates=10000, lr=5e-05, gnorm=0.824, loss_scale=64, train_wall=289, gb_free=28.5, wall=11127 (progress_bar.py:262, log())
[2021-10-22 14:03:21]    INFO >> epoch 008:   1219 / 1326 loss=2.451, nll_loss=0.563, ppl=1.48, wps=5810.1, ups=1.44, wpb=4036.4, bsz=16, num_updates=10500, lr=5e-05, gnorm=0.758, loss_scale=128, train_wall=341, gb_free=26.2, wall=11475 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 14:05:16]    INFO >> epoch 008 | loss 2.464 | nll_loss 0.577 | ppl 1.49 | wps 3277.6 | ups 0.9 | wpb 3623.1 | bsz 16 | num_updates 10607 | lr 5e-05 | gnorm 0.805 | loss_scale 128 | train_wall 863 | gb_free 25.5 | wall 11589 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 14:13:05]    INFO >> epoch 008 | valid on 'valid' subset | loss 2.543 | nll_loss 0.579 | ppl 1.49 | bleu 57.8835 | wps 401.6 | wpb 6755.2 | bsz 31.4 | num_updates 10607 | best_bleu 60.2038 (progress_bar.py:269, print())
[2021-10-22 14:13:12]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 8 @ 10607 updates, score 57.883471) (writing took 7.378526 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 14:16:52]    INFO >> epoch 009:    393 / 1326 loss=2.465, nll_loss=0.577, ppl=1.49, wps=2141.6, ups=0.62, wpb=3473, bsz=16, num_updates=11000, lr=5e-05, gnorm=0.814, loss_scale=128, train_wall=319, gb_free=28.3, wall=12285 (progress_bar.py:262, log())
[2021-10-22 14:21:47]    INFO >> epoch 009:    893 / 1326 loss=2.426, nll_loss=0.535, ppl=1.45, wps=5648.6, ups=1.69, wpb=3336.9, bsz=16, num_updates=11500, lr=4.9e-05, gnorm=0.774, loss_scale=128, train_wall=288, gb_free=28.4, wall=12581 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 14:27:48]    INFO >> epoch 009 | loss 2.439 | nll_loss 0.55 | ppl 1.46 | wps 3551.7 | ups 0.98 | wpb 3623.1 | bsz 16 | num_updates 11933 | lr 4.9e-05 | gnorm 0.784 | loss_scale 128 | train_wall 850 | gb_free 25.5 | wall 12942 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 14:36:28]    INFO >> epoch 009 | valid on 'valid' subset | loss 2.542 | nll_loss 0.586 | ppl 1.5 | bleu 59.2233 | wps 363 | wpb 6755.2 | bsz 31.4 | num_updates 11933 | best_bleu 60.2038 (progress_bar.py:269, print())
[2021-10-22 14:36:35]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 9 @ 11933 updates, score 59.223319) (writing took 6.989130 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 14:37:21]    INFO >> epoch 010:     67 / 1326 loss=2.459, nll_loss=0.569, ppl=1.48, wps=2363.1, ups=0.54, wpb=4413.3, bsz=16, num_updates=12000, lr=4.9e-05, gnorm=0.78, loss_scale=128, train_wall=391, gb_free=28.5, wall=13515 (progress_bar.py:262, log())
[2021-10-22 14:37:27]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 14:37:27]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-10-22 14:42:01]    INFO >> epoch 010:    568 / 1326 loss=2.405, nll_loss=0.513, ppl=1.43, wps=5268.7, ups=1.78, wpb=2954.8, bsz=16, num_updates=12500, lr=4.9e-05, gnorm=0.844, loss_scale=64, train_wall=274, gb_free=28.9, wall=13795 (progress_bar.py:262, log())
[2021-10-22 14:47:29]    INFO >> epoch 010:   1068 / 1326 loss=2.417, nll_loss=0.526, ppl=1.44, wps=5490.2, ups=1.53, wpb=3600.1, bsz=16, num_updates=13000, lr=4.9e-05, gnorm=0.778, loss_scale=64, train_wall=321, gb_free=26.8, wall=14123 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 14:51:38]    INFO >> epoch 010 | loss 2.429 | nll_loss 0.538 | ppl 1.45 | wps 3357.6 | ups 0.93 | wpb 3621.6 | bsz 16 | num_updates 13258 | lr 4.9e-05 | gnorm 0.811 | loss_scale 64 | train_wall 876 | gb_free 25.5 | wall 14371 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 15:00:44]    INFO >> epoch 010 | valid on 'valid' subset | loss 2.544 | nll_loss 0.59 | ppl 1.51 | bleu 58.8797 | wps 339.4 | wpb 6755.2 | bsz 31.4 | num_updates 13258 | best_bleu 60.2038 (progress_bar.py:269, print())
[2021-10-22 15:00:51]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 10 @ 13258 updates, score 58.879656) (writing took 6.842869 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 15:03:17]    INFO >> epoch 011:    242 / 1326 loss=2.444, nll_loss=0.553, ppl=1.47, wps=2119.3, ups=0.53, wpb=4018.6, bsz=16, num_updates=13500, lr=4.9e-05, gnorm=0.826, loss_scale=64, train_wall=379, gb_free=28.5, wall=15071 (progress_bar.py:262, log())
[2021-10-22 15:08:29]    INFO >> epoch 011:    742 / 1326 loss=2.391, nll_loss=0.498, ppl=1.41, wps=5049.2, ups=1.61, wpb=3142.3, bsz=16, num_updates=14000, lr=4.9e-05, gnorm=0.821, loss_scale=64, train_wall=305, gb_free=26.4, wall=15382 (progress_bar.py:262, log())
[2021-10-22 15:15:03]    INFO >> epoch 011:   1242 / 1326 loss=2.402, nll_loss=0.509, ppl=1.42, wps=5247, ups=1.27, wpb=4134.6, bsz=16, num_updates=14500, lr=4.9e-05, gnorm=0.758, loss_scale=128, train_wall=387, gb_free=25.4, wall=15776 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 15:16:41]    INFO >> epoch 011 | loss 2.407 | nll_loss 0.514 | ppl 1.43 | wps 3195.9 | ups 0.88 | wpb 3623.1 | bsz 16 | num_updates 14584 | lr 4.9e-05 | gnorm 0.803 | loss_scale 128 | train_wall 924 | gb_free 25.5 | wall 15875 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 15:26:31]    INFO >> epoch 011 | valid on 'valid' subset | loss 2.547 | nll_loss 0.596 | ppl 1.51 | bleu 58.7685 | wps 319.2 | wpb 6755.2 | bsz 31.4 | num_updates 14584 | best_bleu 60.2038 (progress_bar.py:269, print())
[2021-10-22 15:26:38]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 11 @ 14584 updates, score 58.768465) (writing took 7.220355 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 15:30:51]    INFO >> epoch 012:    416 / 1326 loss=2.402, nll_loss=0.508, ppl=1.42, wps=1765.6, ups=0.53, wpb=3350.4, bsz=16, num_updates=15000, lr=4.9e-05, gnorm=0.814, loss_scale=128, train_wall=336, gb_free=28.4, wall=16725 (progress_bar.py:262, log())
[2021-10-22 15:36:17]    INFO >> epoch 012:    916 / 1326 loss=2.372, nll_loss=0.476, ppl=1.39, wps=5177.5, ups=1.54, wpb=3372.3, bsz=16, num_updates=15500, lr=4.9e-05, gnorm=0.77, loss_scale=128, train_wall=319, gb_free=26.5, wall=17051 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 15:42:38]    INFO >> epoch 012 | loss 2.386 | nll_loss 0.491 | ppl 1.41 | wps 3084.9 | ups 0.85 | wpb 3623.1 | bsz 16 | num_updates 15910 | lr 4.9e-05 | gnorm 0.779 | loss_scale 128 | train_wall 934 | gb_free 25.5 | wall 17432 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 15:52:28]    INFO >> epoch 012 | valid on 'valid' subset | loss 2.553 | nll_loss 0.603 | ppl 1.52 | bleu 58.7088 | wps 317.8 | wpb 6755.2 | bsz 31.4 | num_updates 15910 | best_bleu 60.2038 (progress_bar.py:269, print())
[2021-10-22 15:52:35]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 12 @ 15910 updates, score 58.708761) (writing took 7.327732 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 15:53:38]    INFO >> epoch 013:     90 / 1326 loss=2.407, nll_loss=0.513, ppl=1.43, wps=2099.9, ups=0.48, wpb=4371.3, bsz=16, num_updates=16000, lr=4.9e-05, gnorm=0.77, loss_scale=128, train_wall=428, gb_free=27.9, wall=18092 (progress_bar.py:262, log())
[2021-10-22 15:58:38]    INFO >> epoch 013:    590 / 1326 loss=2.344, nll_loss=0.446, ppl=1.36, wps=4945.5, ups=1.67, wpb=2968.6, bsz=16, num_updates=16500, lr=4.9e-05, gnorm=0.765, loss_scale=256, train_wall=294, gb_free=28.5, wall=18392 (progress_bar.py:262, log())
[2021-10-22 16:00:35]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 16:04:38]    INFO >> epoch 013:   1090 / 1326 loss=2.358, nll_loss=0.461, ppl=1.38, wps=5069.9, ups=1.39, wpb=3654.1, bsz=16, num_updates=17000, lr=4.9e-05, gnorm=0.754, loss_scale=128, train_wall=351, gb_free=27.4, wall=18752 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 16:08:40]    INFO >> epoch 013 | loss 2.369 | nll_loss 0.472 | ppl 1.39 | wps 3076.3 | ups 0.85 | wpb 3623.1 | bsz 16 | num_updates 17236 | lr 4.9e-05 | gnorm 0.766 | loss_scale 128 | train_wall 935 | gb_free 25.5 | wall 18994 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 16:14:43]    INFO >> epoch 013 | valid on 'valid' subset | loss 2.559 | nll_loss 0.608 | ppl 1.52 | bleu 59.2893 | wps 527.2 | wpb 6755.2 | bsz 31.4 | num_updates 17236 | best_bleu 60.2038 (progress_bar.py:269, print())
[2021-10-22 16:14:50]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 13 @ 17236 updates, score 59.289312) (writing took 7.190082 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 16:17:00]    INFO >> epoch 014:    264 / 1326 loss=2.388, nll_loss=0.492, ppl=1.41, wps=2671.9, ups=0.67, wpb=3964.7, bsz=16, num_updates=17500, lr=4.9e-05, gnorm=0.787, loss_scale=128, train_wall=356, gb_free=27, wall=19494 (progress_bar.py:262, log())
[2021-10-22 16:20:50]    INFO >> epoch 014:    764 / 1326 loss=2.346, nll_loss=0.448, ppl=1.36, wps=6880.4, ups=2.18, wpb=3159.9, bsz=16, num_updates=18000, lr=4.9e-05, gnorm=0.786, loss_scale=128, train_wall=223, gb_free=28.5, wall=19724 (progress_bar.py:262, log())
[2021-10-22 16:25:25]    INFO >> epoch 014:   1264 / 1326 loss=2.361, nll_loss=0.464, ppl=1.38, wps=7707, ups=1.82, wpb=4233.3, bsz=16, num_updates=18500, lr=4.9e-05, gnorm=0.751, loss_scale=128, train_wall=268, gb_free=25.3, wall=19998 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 16:26:13]    INFO >> epoch 014 | loss 2.361 | nll_loss 0.464 | ppl 1.38 | wps 4561.5 | ups 1.26 | wpb 3623.1 | bsz 16 | num_updates 18562 | lr 4.9e-05 | gnorm 0.779 | loss_scale 128 | train_wall 656 | gb_free 25.5 | wall 20047 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 16:32:21]    INFO >> epoch 014 | valid on 'valid' subset | loss 2.562 | nll_loss 0.613 | ppl 1.53 | bleu 59.0689 | wps 518.3 | wpb 6755.2 | bsz 31.4 | num_updates 18562 | best_bleu 60.2038 (progress_bar.py:269, print())
[2021-10-22 16:32:28]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 14 @ 18562 updates, score 59.068873) (writing took 7.291131 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 16:32:28]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:179, should_stop_early())
[2021-10-22 16:32:28]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:279, single_main())
[2021-10-22 16:32:28]    INFO >> done training in 20421.6 seconds (train.py:290, single_main())
