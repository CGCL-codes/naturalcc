nohup: ignoring input
Using backend: pytorch
[2021-10-22 11:12:59]    INFO >> Load arguments in /home/wanyao/yang/naturalcc-dev/run/translation/plbart/config/avatar/topk3/o2o/java-python.yml (train.py:309, cli_main())
[2021-10-22 11:12:59]    INFO >> {'criterion': 'label_smoothed_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 500, 'log_format': 'simple', 'tensorboard_logdir': '', 'memory_efficient_fp16': 0, 'fp16_no_flatten_grads': 1, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'empty_cache_freq': 0, 'task': 'bart_finetune', 'seed': 1234, 'cpu': 0, 'fp16': 0, 'fp16_opt_level': '01', 'bf16': 0, 'memory_efficient_bf16': 0, 'server_ip': '', 'server_port': '', 'amp': 1, 'amp_batch_retries': 2, 'amp_init_scale': '2 ** 7', 'amp_scale_window': None}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 4, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'pipeline_model_parallel': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1, 'block_momentum': 0.875, 'block_lr': 1, 'use_nbm': 0, 'average_sync': 0}, 'task': {'data': '/mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap', 'source_lang': 'java', 'target_lang': 'python', 'load_alignments': 0, 'left_pad_source': 0, 'left_pad_target': 0, 'max_source_positions': 511, 'max_target_positions': 511, 'upsample_primary': 1, 'truncate_source': 1, 'truncate_target': 1, 'append_eos_to_target': 1, 'eval_bleu': 1, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': True, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_args': None, 'eval_bleu_print_samples': 0, 'eval_with_sacrebleu': 1}, 'model': {'arch': 'fairseq_transformer', 'offset_positions_by_padding': 1, 'pooler_dropout': 0.1, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.1, 'encoder_positional_embeddings': 0, 'encoder_learned_pos': 1, 'encoder_max_relative_len': 0, 'encoder_embed_path': 0, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_layers': 6, 'encoder_attention_heads': 12, 'encoder_normalize_before': 0, 'decoder_embed_path': '', 'decoder_positional_embeddings': 0, 'decoder_learned_pos': 1, 'decoder_max_relative_len': 0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_attention_heads': 12, 'decoder_normalize_before': 0, 'no_decoder_final_norm': 0, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.1, 'adaptive_softmax_factor': 0.0, 'share_decoder_input_output_embed': 1, 'decoder_out_embed_bias': 1, 'share_all_embeddings': 1, 'adaptive_input': 0, 'adaptive_input_factor': 0.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': 0, 'tie_adaptive_proj': 0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 1, 'no_scale_embedding': 0, 'no_token_positional_embeddings': 0, 'encoder_dropout_in': 0.1, 'encoder_dropout_out': 0.1, 'decoder_dropout_in': 0.1, 'decoder_dropout_out': 0.1, 'max_source_positions': 1024, 'max_target_positions': 1024, 'multihead_attention_version': 'ncc', 'encoder_position_encoding_version': 'ncc_learned', 'decoder_position_encoding_version': 'ncc_learned'}, 'optimization': {'max_epoch': 0, 'max_update': 30000, 'clip_norm': 0.0, 'update_freq': [4], 'lrs': [5e-05], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1500, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'sentence_avg': 0, 'label_smoothing': 0.1, 'adam': {'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': 0}}, 'checkpoint': {'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': 1, 'patience': 10, 'save_dir': '/mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints', 'should_continue': 0, 'model_name_or_path': None, 'cache_dir': None, 'logging_steps': 500, 'save_steps': 2000, 'save_total_limit': 2, 'overwrite_output_dir': 0, 'overwrite_cache': 0, 'init_checkpoint': '/mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt'}, 'eval': {'path': '/mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt', 'remove_bpe': 'sentencepiece', 'quiet': 1, 'results_path': None, 'model_overrides': '{}', 'topk': 5, 'max_sentences': 4, 'beam': 5, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 500, 'min_len': 1, 'match_source_len': 0, 'no_early_stop': 1, 'unnormalized': 0, 'no_beamable_mm': 0, 'lenpen': 1, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': 0, 'score_reference': 0, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': 0, 'sampling_topk': -1, 'sampling_topp': -1, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 0, 'print_step': 0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': 0, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': 0, 'retain_iter_history': 0, 'decoding_format': None, 'nltk_bleu': 1, 'rouge': 1}} (train.py:311, cli_main())
[2021-10-22 11:12:59]    INFO >> single GPU training... (train.py:340, cli_main())
[2021-10-22 11:12:59]    INFO >> [java] dictionary: 50005 types (bart_finetune.py:128, setup_task())
[2021-10-22 11:12:59]    INFO >> [python] dictionary: 50005 types (bart_finetune.py:129, setup_task())
[2021-10-22 11:13:01]    INFO >> truncate java/valid.code_tokens to 511 (bart_finetune.py:72, load_langpair_dataset())
[2021-10-22 11:13:04]    INFO >> truncate python/valid.code_tokens to 511 (bart_finetune.py:88, load_langpair_dataset())
[2021-10-22 11:13:10]    INFO >> Restore parameters from /mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt (train.py:228, single_main())
[2021-10-22 11:13:10]    INFO >> FairseqTransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
) (train.py:231, single_main())
[2021-10-22 11:13:10]    INFO >> model fairseq_transformer, criterion LabelSmoothedCrossEntropyCriterion (train.py:232, single_main())
[2021-10-22 11:13:10]    INFO >> num. model params: 139220736 (num. trained: 139220736) (train.py:235, single_main())
[2021-10-22 11:13:16]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:542, pretty_print_cuda_env_list())
[2021-10-22 11:13:16]    INFO >> rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                     (utils.py:548, pretty_print_cuda_env_list())
[2021-10-22 11:13:16]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:550, pretty_print_cuda_env_list())
[2021-10-22 11:13:16]    INFO >> training on 1 GPUs (train.py:240, single_main())
[2021-10-22 11:13:16]    INFO >> max tokens per GPU = None and max sentences per GPU = 4 (train.py:243, single_main())
[2021-10-22 11:13:16]    INFO >> no existing checkpoint found /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())
[2021-10-22 11:13:16]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())
[2021-10-22 11:13:16]    INFO >> truncate java/train.code_tokens to 511 (bart_finetune.py:72, load_langpair_dataset())
[2021-10-22 11:13:16]    INFO >> truncate python/train.code_tokens to 511 (bart_finetune.py:88, load_langpair_dataset())
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:57: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
[2021-10-22 11:13:26]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 11:13:26]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 11:13:27]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 11:13:27]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-10-22 11:13:31]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 11:13:34]    INFO >> AMP: overflow detected, setting scale to to 4.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 11:17:28]    INFO >> epoch 001:    501 / 1326 loss=5.871, nll_loss=4.021, ppl=16.23, wps=4023.9, ups=2.07, wpb=1943.3, bsz=16, num_updates=500, lr=1.7e-05, gnorm=15.48, loss_scale=4, train_wall=234, gb_free=28.1, wall=252 (progress_bar.py:262, log())
[2021-10-22 11:23:25]    INFO >> epoch 001:   1001 / 1326 loss=3.331, nll_loss=1.472, ppl=2.77, wps=3922.4, ups=1.4, wpb=2794.7, bsz=16, num_updates=1000, lr=3.3e-05, gnorm=1.907, loss_scale=4, train_wall=350, gb_free=28.3, wall=608 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 11:28:53]    INFO >> epoch 001 | loss 4.008 | nll_loss 2.161 | ppl 4.47 | wps 3940 | ups 1.43 | wpb 2753.8 | bsz 16 | num_updates 1325 | lr 4.4e-05 | gnorm 6.946 | loss_scale 4 | train_wall 908 | gb_free 25.5 | wall 937 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 11:37:14]    INFO >> epoch 001 | valid on 'valid' subset | loss 2.703 | nll_loss 0.689 | ppl 1.61 | bleu 55.8916 | wps 335.5 | wpb 6157.8 | bsz 31.4 | num_updates 1325 (progress_bar.py:269, print())
[2021-10-22 11:37:20]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 1 @ 1325 updates, score 55.891584) (writing took 5.707034 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 11:38:52]    INFO >> epoch 002:    175 / 1326 loss=3.412, nll_loss=1.59, ppl=3.01, wps=1772.7, ups=0.54, wpb=3288.5, bsz=16, num_updates=1500, lr=5e-05, gnorm=1.786, loss_scale=4, train_wall=405, gb_free=27.2, wall=1536 (progress_bar.py:262, log())
[2021-10-22 11:42:43]    INFO >> epoch 002:    675 / 1326 loss=3.099, nll_loss=1.261, ppl=2.4, wps=4410.9, ups=2.17, wpb=2031.9, bsz=16, num_updates=2000, lr=5e-05, gnorm=1.64, loss_scale=4, train_wall=224, gb_free=27.8, wall=1766 (progress_bar.py:262, log())
[2021-10-22 11:49:12]    INFO >> epoch 002:   1175 / 1326 loss=3.043, nll_loss=1.205, ppl=2.31, wps=4209.9, ups=1.28, wpb=3279.8, bsz=16, num_updates=2500, lr=5e-05, gnorm=1.358, loss_scale=8, train_wall=383, gb_free=26, wall=2156 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 11:51:50]    INFO >> epoch 002 | loss 3.139 | nll_loss 1.307 | ppl 2.47 | wps 2650.3 | ups 0.96 | wpb 2752.7 | bsz 16 | num_updates 2651 | lr 5e-05 | gnorm 1.565 | loss_scale 8 | train_wall 844 | gb_free 25.5 | wall 2314 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 12:00:58]    INFO >> epoch 002 | valid on 'valid' subset | loss 2.616 | nll_loss 0.622 | ppl 1.54 | bleu 59.3777 | wps 309 | wpb 6157.8 | bsz 31.4 | num_updates 2651 | best_bleu 59.3777 (progress_bar.py:269, print())
[2021-10-22 12:01:10]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 2 @ 2651 updates, score 59.377724) (writing took 12.064967 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 12:03:57]    INFO >> epoch 003:    349 / 1326 loss=3.189, nll_loss=1.367, ppl=2.58, wps=1468.2, ups=0.57, wpb=2597.3, bsz=16, num_updates=3000, lr=5e-05, gnorm=1.572, loss_scale=8, train_wall=309, gb_free=27.9, wall=3040 (progress_bar.py:262, log())
[2021-10-22 12:08:21]    INFO >> epoch 003:    849 / 1326 loss=2.881, nll_loss=1.031, ppl=2.04, wps=4581.1, ups=1.89, wpb=2420.2, bsz=16, num_updates=3500, lr=5e-05, gnorm=1.347, loss_scale=8, train_wall=258, gb_free=28.3, wall=3304 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 12:15:29]    INFO >> epoch 003 | loss 2.983 | nll_loss 1.143 | ppl 2.21 | wps 2573.3 | ups 0.93 | wpb 2752.7 | bsz 16 | num_updates 3977 | lr 5e-05 | gnorm 1.388 | loss_scale 8 | train_wall 831 | gb_free 25.5 | wall 3732 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 12:24:37]    INFO >> epoch 003 | valid on 'valid' subset | loss 2.572 | nll_loss 0.601 | ppl 1.52 | bleu 59.8073 | wps 310.2 | wpb 6157.8 | bsz 31.4 | num_updates 3977 | best_bleu 59.8073 (progress_bar.py:269, print())
[2021-10-22 12:24:50]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 3 @ 3977 updates, score 59.80731) (writing took 12.182696 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 12:25:09]    INFO >> epoch 004:     23 / 1326 loss=2.964, nll_loss=1.123, ppl=2.18, wps=1805, ups=0.5, wpb=3640.6, bsz=16, num_updates=4000, lr=5e-05, gnorm=1.228, loss_scale=8, train_wall=432, gb_free=29.1, wall=4313 (progress_bar.py:262, log())
[2021-10-22 12:28:59]    INFO >> epoch 004:    523 / 1326 loss=2.989, nll_loss=1.151, ppl=2.22, wps=4224.2, ups=2.18, wpb=1938.3, bsz=16, num_updates=4500, lr=5e-05, gnorm=1.486, loss_scale=16, train_wall=223, gb_free=28.5, wall=4542 (progress_bar.py:262, log())
[2021-10-22 12:34:11]    INFO >> epoch 004:   1023 / 1326 loss=2.782, nll_loss=0.925, ppl=1.9, wps=4572.5, ups=1.6, wpb=2852.2, bsz=16, num_updates=5000, lr=5e-05, gnorm=1.215, loss_scale=16, train_wall=303, gb_free=26.6, wall=4854 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 12:39:13]    INFO >> epoch 004 | loss 2.878 | nll_loss 1.03 | ppl 2.04 | wps 2562.2 | ups 0.93 | wpb 2752.7 | bsz 16 | num_updates 5303 | lr 5e-05 | gnorm 1.312 | loss_scale 16 | train_wall 836 | gb_free 25.5 | wall 5157 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 12:49:14]    INFO >> epoch 004 | valid on 'valid' subset | loss 2.568 | nll_loss 0.591 | ppl 1.51 | bleu 59.0561 | wps 281.8 | wpb 6157.8 | bsz 31.4 | num_updates 5303 | best_bleu 59.8073 (progress_bar.py:269, print())
[2021-10-22 12:49:21]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 4 @ 5303 updates, score 59.056085) (writing took 7.437324 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 12:51:04]    INFO >> epoch 005:    197 / 1326 loss=2.948, nll_loss=1.104, ppl=2.15, wps=1585, ups=0.49, wpb=3211.9, bsz=16, num_updates=5500, lr=5e-05, gnorm=1.334, loss_scale=16, train_wall=390, gb_free=29, wall=5867 (progress_bar.py:262, log())
[2021-10-22 12:55:18]    INFO >> epoch 005:    697 / 1326 loss=2.735, nll_loss=0.873, ppl=1.83, wps=4073.8, ups=1.96, wpb=2074.4, bsz=16, num_updates=6000, lr=5e-05, gnorm=1.301, loss_scale=16, train_wall=247, gb_free=26.7, wall=6122 (progress_bar.py:262, log())
[2021-10-22 13:01:44]    INFO >> epoch 005:   1197 / 1326 loss=2.749, nll_loss=0.888, ppl=1.85, wps=4336.6, ups=1.3, wpb=3341.5, bsz=16, num_updates=6500, lr=5e-05, gnorm=1.131, loss_scale=32, train_wall=379, gb_free=25.4, wall=6507 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 13:04:06]    INFO >> epoch 005 | loss 2.802 | nll_loss 0.945 | ppl 1.93 | wps 2445.9 | ups 0.89 | wpb 2752.7 | bsz 16 | num_updates 6629 | lr 5e-05 | gnorm 1.265 | loss_scale 32 | train_wall 857 | gb_free 25.5 | wall 6649 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 13:17:50]    INFO >> epoch 005 | valid on 'valid' subset | loss 2.562 | nll_loss 0.598 | ppl 1.51 | bleu 60.8229 | wps 203 | wpb 6157.8 | bsz 31.4 | num_updates 6629 | best_bleu 60.8229 (progress_bar.py:269, print())
[2021-10-22 13:18:02]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 5 @ 6629 updates, score 60.822908) (writing took 12.419877 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 13:21:12]    INFO >> epoch 006:    371 / 1326 loss=2.886, nll_loss=1.035, ppl=2.05, wps=1071.8, ups=0.43, wpb=2504.9, bsz=16, num_updates=7000, lr=5e-05, gnorm=1.391, loss_scale=32, train_wall=306, gb_free=28.6, wall=7676 (progress_bar.py:262, log())
[2021-10-22 13:25:43]    INFO >> epoch 006:    871 / 1326 loss=2.645, nll_loss=0.773, ppl=1.71, wps=4582.4, ups=1.84, wpb=2484.8, bsz=16, num_updates=7500, lr=5e-05, gnorm=1.199, loss_scale=32, train_wall=265, gb_free=26, wall=7947 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 13:32:36]    INFO >> epoch 006 | loss 2.741 | nll_loss 0.877 | ppl 1.84 | wps 2133.8 | ups 0.78 | wpb 2752.7 | bsz 16 | num_updates 7955 | lr 5e-05 | gnorm 1.242 | loss_scale 32 | train_wall 837 | gb_free 25.5 | wall 8360 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 13:42:16]    INFO >> epoch 006 | valid on 'valid' subset | loss 2.567 | nll_loss 0.603 | ppl 1.52 | bleu 60.7031 | wps 292.6 | wpb 6157.8 | bsz 31.4 | num_updates 7955 | best_bleu 60.8229 (progress_bar.py:269, print())
[2021-10-22 13:42:23]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 6 @ 7955 updates, score 60.703146) (writing took 7.327270 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 13:42:53]    INFO >> epoch 007:     45 / 1326 loss=2.753, nll_loss=0.889, ppl=1.85, wps=1740.3, ups=0.49, wpb=3584.1, bsz=16, num_updates=8000, lr=5e-05, gnorm=1.141, loss_scale=32, train_wall=428, gb_free=29, wall=8977 (progress_bar.py:262, log())
[2021-10-22 13:46:49]    INFO >> epoch 007:    545 / 1326 loss=2.737, nll_loss=0.872, ppl=1.83, wps=4122.1, ups=2.12, wpb=1943.8, bsz=16, num_updates=8500, lr=5e-05, gnorm=1.328, loss_scale=64, train_wall=229, gb_free=28.2, wall=9212 (progress_bar.py:262, log())
[2021-10-22 13:52:16]    INFO >> epoch 007:   1045 / 1326 loss=2.605, nll_loss=0.728, ppl=1.66, wps=4462, ups=1.53, wpb=2918.8, bsz=16, num_updates=9000, lr=5e-05, gnorm=1.118, loss_scale=64, train_wall=320, gb_free=25.9, wall=9539 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 13:56:56]    INFO >> epoch 007 | loss 2.683 | nll_loss 0.812 | ppl 1.76 | wps 2500.5 | ups 0.91 | wpb 2752.7 | bsz 16 | num_updates 9281 | lr 5e-05 | gnorm 1.204 | loss_scale 64 | train_wall 846 | gb_free 25.5 | wall 9820 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 14:06:06]    INFO >> epoch 007 | valid on 'valid' subset | loss 2.562 | nll_loss 0.611 | ppl 1.53 | bleu 60.5222 | wps 309.2 | wpb 6157.8 | bsz 31.4 | num_updates 9281 | best_bleu 60.8229 (progress_bar.py:269, print())
[2021-10-22 14:06:14]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 7 @ 9281 updates, score 60.522154) (writing took 7.709841 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 14:08:04]    INFO >> epoch 008:    219 / 1326 loss=2.762, nll_loss=0.895, ppl=1.86, wps=1646.4, ups=0.53, wpb=3123.3, bsz=16, num_updates=9500, lr=5e-05, gnorm=1.273, loss_scale=64, train_wall=375, gb_free=28.8, wall=10488 (progress_bar.py:262, log())
[2021-10-22 14:12:14]    INFO >> epoch 008:    719 / 1326 loss=2.561, nll_loss=0.679, ppl=1.6, wps=4243.5, ups=2, wpb=2118.4, bsz=16, num_updates=10000, lr=5e-05, gnorm=1.214, loss_scale=64, train_wall=244, gb_free=27.7, wall=10738 (progress_bar.py:262, log())
[2021-10-22 14:18:41]    INFO >> epoch 008:   1219 / 1326 loss=2.605, nll_loss=0.727, ppl=1.65, wps=4355.3, ups=1.29, wpb=3372.6, bsz=16, num_updates=10500, lr=5e-05, gnorm=1.067, loss_scale=128, train_wall=381, gb_free=26.8, wall=11125 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 14:20:40]    INFO >> epoch 008 | loss 2.639 | nll_loss 0.763 | ppl 1.7 | wps 2563.5 | ups 0.93 | wpb 2752.7 | bsz 16 | num_updates 10607 | lr 5e-05 | gnorm 1.197 | loss_scale 128 | train_wall 841 | gb_free 25.5 | wall 11243 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 14:29:33]    INFO >> epoch 008 | valid on 'valid' subset | loss 2.57 | nll_loss 0.613 | ppl 1.53 | bleu 60.5086 | wps 317.8 | wpb 6157.8 | bsz 31.4 | num_updates 10607 | best_bleu 60.8229 (progress_bar.py:269, print())
[2021-10-22 14:29:42]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 8 @ 10607 updates, score 60.508614) (writing took 8.112315 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 14:33:04]    INFO >> epoch 009:    393 / 1326 loss=2.715, nll_loss=0.842, ppl=1.79, wps=1414.5, ups=0.58, wpb=2440.2, bsz=16, num_updates=11000, lr=5e-05, gnorm=1.303, loss_scale=128, train_wall=305, gb_free=28.8, wall=11987 (progress_bar.py:262, log())
[2021-10-22 14:37:48]    INFO >> epoch 009:    893 / 1326 loss=2.514, nll_loss=0.626, ppl=1.54, wps=4451.5, ups=1.76, wpb=2530.9, bsz=16, num_updates=11500, lr=4.9e-05, gnorm=1.125, loss_scale=128, train_wall=277, gb_free=27.3, wall=12272 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 14:44:36]    INFO >> epoch 009 | loss 2.602 | nll_loss 0.721 | ppl 1.65 | wps 2542 | ups 0.92 | wpb 2752.7 | bsz 16 | num_updates 11933 | lr 4.9e-05 | gnorm 1.17 | loss_scale 128 | train_wall 867 | gb_free 25.5 | wall 12679 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 14:53:51]    INFO >> epoch 009 | valid on 'valid' subset | loss 2.574 | nll_loss 0.623 | ppl 1.54 | bleu 60.5461 | wps 305.7 | wpb 6157.8 | bsz 31.4 | num_updates 11933 | best_bleu 60.8229 (progress_bar.py:269, print())
[2021-10-22 14:53:58]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 9 @ 11933 updates, score 60.546057) (writing took 7.003276 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 14:54:39]    INFO >> epoch 010:     67 / 1326 loss=2.632, nll_loss=0.752, ppl=1.68, wps=1756.1, ups=0.49, wpb=3552.1, bsz=16, num_updates=12000, lr=4.9e-05, gnorm=1.103, loss_scale=128, train_wall=433, gb_free=28.8, wall=13283 (progress_bar.py:262, log())
[2021-10-22 14:58:36]    INFO >> epoch 010:    567 / 1326 loss=2.58, nll_loss=0.694, ppl=1.62, wps=4104.3, ups=2.11, wpb=1944, bsz=16, num_updates=12500, lr=4.9e-05, gnorm=1.24, loss_scale=256, train_wall=230, gb_free=28.2, wall=13520 (progress_bar.py:262, log())
[2021-10-22 15:04:20]    INFO >> epoch 010:   1067 / 1326 loss=2.51, nll_loss=0.621, ppl=1.54, wps=4326.1, ups=1.46, wpb=2970.9, bsz=16, num_updates=13000, lr=4.9e-05, gnorm=1.049, loss_scale=256, train_wall=337, gb_free=26.9, wall=13863 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 15:09:00]    INFO >> epoch 010 | loss 2.569 | nll_loss 0.683 | ppl 1.61 | wps 2492.1 | ups 0.91 | wpb 2752.7 | bsz 16 | num_updates 13259 | lr 4.9e-05 | gnorm 1.138 | loss_scale 256 | train_wall 876 | gb_free 25.5 | wall 14144 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 15:18:55]    INFO >> epoch 010 | valid on 'valid' subset | loss 2.575 | nll_loss 0.63 | ppl 1.55 | bleu 60.8343 | wps 284.7 | wpb 6157.8 | bsz 31.4 | num_updates 13259 | best_bleu 60.8343 (progress_bar.py:269, print())
[2021-10-22 15:19:08]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 10 @ 13259 updates, score 60.834274) (writing took 12.564568 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 15:21:13]    INFO >> epoch 011:    241 / 1326 loss=2.647, nll_loss=0.766, ppl=1.7, wps=1504.3, ups=0.49, wpb=3048.4, bsz=16, num_updates=13500, lr=4.9e-05, gnorm=1.204, loss_scale=256, train_wall=391, gb_free=27.8, wall=14876 (progress_bar.py:262, log())
[2021-10-22 15:22:34]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-10-22 15:22:34]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-10-22 15:25:44]    INFO >> epoch 011:    742 / 1326 loss=2.467, nll_loss=0.573, ppl=1.49, wps=3990.2, ups=1.84, wpb=2166.4, bsz=16, num_updates=14000, lr=4.9e-05, gnorm=1.142, loss_scale=128, train_wall=265, gb_free=27.1, wall=15148 (progress_bar.py:262, log())
[2021-10-22 15:32:57]    INFO >> epoch 011:   1242 / 1326 loss=2.54, nll_loss=0.653, ppl=1.57, wps=3945.2, ups=1.16, wpb=3415.4, bsz=16, num_updates=14500, lr=4.9e-05, gnorm=1.056, loss_scale=128, train_wall=426, gb_free=27.4, wall=15581 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 15:34:43]    INFO >> epoch 011 | loss 2.554 | nll_loss 0.667 | ppl 1.59 | wps 2363.6 | ups 0.86 | wpb 2752.5 | bsz 16 | num_updates 14584 | lr 4.9e-05 | gnorm 1.151 | loss_scale 128 | train_wall 910 | gb_free 25.5 | wall 15687 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 15:44:49]    INFO >> epoch 011 | valid on 'valid' subset | loss 2.581 | nll_loss 0.632 | ppl 1.55 | bleu 61.19 | wps 278.9 | wpb 6157.8 | bsz 31.4 | num_updates 14584 | best_bleu 61.19 (progress_bar.py:269, print())
[2021-10-22 15:45:01]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top3/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 11 @ 14584 updates, score 61.189979) (writing took 12.213497 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-22 15:48:31]    INFO >> epoch 012:    416 / 1326 loss=2.63, nll_loss=0.747, ppl=1.68, wps=1262.5, ups=0.54, wpb=2356.8, bsz=16, num_updates=15000, lr=4.9e-05, gnorm=1.29, loss_scale=128, train_wall=300, gb_free=28.7, wall=16514 (progress_bar.py:262, log())
[2021-10-22 15:53:42]    INFO >> epoch 012:    916 / 1326 loss=2.453, nll_loss=0.558, ppl=1.47, wps=4151.3, ups=1.61, wpb=2586.3, bsz=16, num_updates=15500, lr=4.9e-05, gnorm=1.089, loss_scale=128, train_wall=305, gb_free=27.4, wall=16826 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-22 16:00:40]    INFO >> epoch 012 | loss 2.535 | nll_loss 0.645 | ppl 1.56 | wps 2344.6 | ups 0.85 | wpb 2752.7 | bsz 16 | num_updates 15910 | lr 4.9e-05 | gnorm 1.152 | loss_scale 256 | train_wall 912 | gb_free 25.5 | wall 17244 (progress_bar.py:269, print())
[2021-10-22 16:08:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.05 GiB (GPU 0; 31.75 GiB total capacity; 5.53 GiB already allocated; 882.19 MiB free; 7.96 GiB reserved in total by PyTorch) (ncc_trainers.py:760, _log_oom())
[2021-10-22 16:08:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 5         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    5658 MB |    8781 MB |  719213 GB |  719208 GB |
|       from large pool |    5654 MB |    8777 MB |  701412 GB |  701407 GB |
|       from small pool |       3 MB |      79 MB |   17800 GB |   17800 GB |
|---------------------------------------------------------------------------|
| Active memory         |    5658 MB |    8781 MB |  719213 GB |  719208 GB |
|       from large pool |    5654 MB |    8777 MB |  701412 GB |  701407 GB |
|       from small pool |       3 MB |      79 MB |   17800 GB |   17800 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    8156 MB |   14692 MB |    2365 GB |    2357 GB |
|       from large pool |    8130 MB |   14626 MB |    2328 GB |    2320 GB |
|       from small pool |      26 MB |     184 MB |      36 GB |      36 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2497 MB |    6351 MB |  728006 GB |  728004 GB |
|       from large pool |    2475 MB |    6336 MB |  708789 GB |  708787 GB |
|       from small pool |      22 MB |      44 MB |   19216 GB |   19216 GB |
|---------------------------------------------------------------------------|
| Allocations           |    1071    |    1475    |  187331 K  |  187330 K  |
|       from large pool |     405    |     475    |   80466 K  |   80465 K  |
|       from small pool |     666    |    1013    |  106865 K  |  106864 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1071    |    1475    |  187331 K  |  187330 K  |
|       from large pool |     405    |     475    |   80466 K  |   80465 K  |
|       from small pool |     666    |    1013    |  106865 K  |  106864 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      86    |     173    |   22140    |   22054    |
|       from large pool |      73    |      81    |    3207    |    3134    |
|       from small pool |      13    |      92    |   18933    |   18920    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     160    |     257    |   99892 K  |   99892 K  |
|       from large pool |     104    |     115    |   48633 K  |   48633 K  |
|       from small pool |      56    |     161    |   51259 K  |   51259 K  |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())
[2021-10-22 16:08:51] WARNING >> ran out of memory in validation step, retrying batch (ncc_trainers.py:573, valid_step())
[2021-10-22 16:08:52] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.05 GiB (GPU 0; 31.75 GiB total capacity; 8.36 GiB already allocated; 2.98 GiB free; 9.68 GiB reserved in total by PyTorch) (ncc_trainers.py:760, _log_oom())
[2021-10-22 16:08:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8565 MB |    8781 MB |  719248 GB |  719240 GB |
|       from large pool |    8562 MB |    8777 MB |  701447 GB |  701439 GB |
|       from small pool |       3 MB |      79 MB |   17800 GB |   17800 GB |
|---------------------------------------------------------------------------|
| Active memory         |    8565 MB |    8781 MB |  719248 GB |  719240 GB |
|       from large pool |    8562 MB |    8777 MB |  701447 GB |  701439 GB |
|       from small pool |       3 MB |      79 MB |   17800 GB |   17800 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9912 MB |   14692 MB |    2370 GB |    2360 GB |
|       from large pool |    9886 MB |   14626 MB |    2333 GB |    2323 GB |
|       from small pool |      26 MB |     184 MB |      36 GB |      36 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1346 MB |    6351 MB |  728023 GB |  728022 GB |
|       from large pool |    1323 MB |    6336 MB |  708806 GB |  708805 GB |
|       from small pool |      22 MB |      44 MB |   19216 GB |   19216 GB |
|---------------------------------------------------------------------------|
| Allocations           |     821    |    1475    |  187331 K  |  187331 K  |
|       from large pool |     315    |     475    |   80466 K  |   80465 K  |
|       from small pool |     506    |    1013    |  106865 K  |  106865 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     821    |    1475    |  187331 K  |  187331 K  |
|       from large pool |     315    |     475    |   80466 K  |   80465 K  |
|       from small pool |     506    |    1013    |  106865 K  |  106865 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      80    |     173    |   22146    |   22066    |
|       from large pool |      67    |      81    |    3213    |    3146    |
|       from small pool |      13    |      92    |   18933    |   18920    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     107    |     257    |   99892 K  |   99892 K  |
|       from large pool |      72    |     115    |   48633 K  |   48633 K  |
|       from small pool |      35    |     161    |   51259 K  |   51259 K  |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
Traceback (most recent call last):
  File "/home/wanyao/yang/naturalcc-dev/ncc/trainers/ncc_trainers.py", line 566, in valid_step
    sample, self.model, self.criterion
  File "/home/wanyao/yang/naturalcc-dev/ncc/tasks/disentangle/bart_finetune.py", line 216, in valid_step
    loss, sample_size, logging_output = super().valid_step(sample, model, criterion)
  File "/home/wanyao/yang/naturalcc-dev/ncc/tasks/ncc_task.py", line 348, in valid_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/home/wanyao/anaconda3/envs/py37-1.7/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wanyao/yang/naturalcc-dev/ncc/criterions/common/label_smoothed_cross_entropy.py", line 50, in forward
    loss, nll_loss = self.compute_loss(model, net_output, sample, reduce=reduce)
  File "/home/wanyao/yang/naturalcc-dev/ncc/criterions/common/label_smoothed_cross_entropy.py", line 62, in compute_loss
    lprobs = model.get_normalized_probs(net_output, log_probs=True)
  File "/home/wanyao/yang/naturalcc-dev/ncc/models/ncc_model.py", line 53, in get_normalized_probs
    return self.get_normalized_probs_scriptable(net_output, log_probs, sample)
  File "/home/wanyao/yang/naturalcc-dev/ncc/models/ncc_model.py", line 67, in get_normalized_probs_scriptable
    return self.decoder.get_normalized_probs(net_output, log_probs, sample)
  File "/home/wanyao/yang/naturalcc-dev/ncc/modules/seq2seq/ncc_decoder.py", line 72, in get_normalized_probs
    return F.log_softmax(logits, dim=-1)
  File "/home/wanyao/anaconda3/envs/py37-1.7/lib/python3.7/site-packages/torch/nn/functional.py", line 1672, in log_softmax
    ret = input.log_softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 3.05 GiB (GPU 0; 31.75 GiB total capacity; 5.53 GiB already allocated; 882.19 MiB free; 7.96 GiB reserved in total by PyTorch)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/wanyao/anaconda3/envs/py37-1.7/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wanyao/anaconda3/envs/py37-1.7/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/wanyao/yang/naturalcc-dev/run/translation/plbart/train.py", line 345, in <module>
    cli_main()
  File "/home/wanyao/yang/naturalcc-dev/run/translation/plbart/train.py", line 341, in cli_main
    single_main(args)
  File "/home/wanyao/yang/naturalcc-dev/run/translation/plbart/train.py", line 265, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "/home/wanyao/yang/naturalcc-dev/run/translation/plbart/train.py", line 128, in validate
    trainer.valid_step(sample)
  File "/home/wanyao/anaconda3/envs/py37-1.7/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/home/wanyao/yang/naturalcc-dev/ncc/trainers/ncc_trainers.py", line 580, in valid_step
    return self.valid_step(sample, raise_oom=True)
  File "/home/wanyao/anaconda3/envs/py37-1.7/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/home/wanyao/yang/naturalcc-dev/ncc/trainers/ncc_trainers.py", line 581, in valid_step
    raise e
  File "/home/wanyao/yang/naturalcc-dev/ncc/trainers/ncc_trainers.py", line 566, in valid_step
    sample, self.model, self.criterion
  File "/home/wanyao/yang/naturalcc-dev/ncc/tasks/disentangle/bart_finetune.py", line 216, in valid_step
    loss, sample_size, logging_output = super().valid_step(sample, model, criterion)
  File "/home/wanyao/yang/naturalcc-dev/ncc/tasks/ncc_task.py", line 348, in valid_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/home/wanyao/anaconda3/envs/py37-1.7/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wanyao/yang/naturalcc-dev/ncc/criterions/common/label_smoothed_cross_entropy.py", line 50, in forward
    loss, nll_loss = self.compute_loss(model, net_output, sample, reduce=reduce)
  File "/home/wanyao/yang/naturalcc-dev/ncc/criterions/common/label_smoothed_cross_entropy.py", line 62, in compute_loss
    lprobs = model.get_normalized_probs(net_output, log_probs=True)
  File "/home/wanyao/yang/naturalcc-dev/ncc/models/ncc_model.py", line 53, in get_normalized_probs
    return self.get_normalized_probs_scriptable(net_output, log_probs, sample)
  File "/home/wanyao/yang/naturalcc-dev/ncc/models/ncc_model.py", line 67, in get_normalized_probs_scriptable
    return self.decoder.get_normalized_probs(net_output, log_probs, sample)
  File "/home/wanyao/yang/naturalcc-dev/ncc/modules/seq2seq/ncc_decoder.py", line 72, in get_normalized_probs
    return F.log_softmax(logits, dim=-1)
  File "/home/wanyao/anaconda3/envs/py37-1.7/lib/python3.7/site-packages/torch/nn/functional.py", line 1672, in log_softmax
    ret = input.log_softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 3.05 GiB (GPU 0; 31.75 GiB total capacity; 8.36 GiB already allocated; 2.98 GiB free; 9.68 GiB reserved in total by PyTorch)
