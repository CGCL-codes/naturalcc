nohup: ignoring input
Using backend: pytorch
[2021-10-18 17:04:42]    INFO >> Load arguments in /home/wanyao/yang/naturalcc-dev/run/translation/plbart/config/avatar/topk5/o2o/java-python.yml (train.py:309, cli_main())
[2021-10-18 17:04:42]    INFO >> {'criterion': 'label_smoothed_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 500, 'log_format': 'simple', 'tensorboard_logdir': '', 'memory_efficient_fp16': 1, 'fp16_no_flatten_grads': 1, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'empty_cache_freq': 0, 'task': 'bart_finetune', 'seed': 1234, 'cpu': 0, 'fp16': 0, 'fp16_opt_level': '01', 'bf16': 0, 'memory_efficient_bf16': 0, 'server_ip': '', 'server_port': ''}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 4, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'pipeline_model_parallel': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1, 'block_momentum': 0.875, 'block_lr': 1, 'use_nbm': 0, 'average_sync': 0}, 'task': {'data': '/mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap', 'source_lang': 'java', 'target_lang': 'python', 'load_alignments': 0, 'left_pad_source': 0, 'left_pad_target': 0, 'max_source_positions': 511, 'max_target_positions': 511, 'upsample_primary': 1, 'truncate_source': 1, 'truncate_target': 1, 'append_eos_to_target': 1, 'eval_bleu': 1, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': True, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_args': None, 'eval_bleu_print_samples': 0, 'eval_with_sacrebleu': 1}, 'model': {'arch': 'fairseq_transformer', 'offset_positions_by_padding': 1, 'pooler_dropout': 0.1, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.1, 'encoder_positional_embeddings': 0, 'encoder_learned_pos': 1, 'encoder_max_relative_len': 0, 'encoder_embed_path': 0, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_layers': 6, 'encoder_attention_heads': 12, 'encoder_normalize_before': 0, 'decoder_embed_path': '', 'decoder_positional_embeddings': 0, 'decoder_learned_pos': 1, 'decoder_max_relative_len': 0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_attention_heads': 12, 'decoder_normalize_before': 0, 'no_decoder_final_norm': 0, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.1, 'adaptive_softmax_factor': 0.0, 'share_decoder_input_output_embed': 1, 'decoder_out_embed_bias': 1, 'share_all_embeddings': 1, 'adaptive_input': 0, 'adaptive_input_factor': 0.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': 0, 'tie_adaptive_proj': 0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 1, 'no_scale_embedding': 0, 'no_token_positional_embeddings': 0, 'encoder_dropout_in': 0.1, 'encoder_dropout_out': 0.1, 'decoder_dropout_in': 0.1, 'decoder_dropout_out': 0.1, 'max_source_positions': 1024, 'max_target_positions': 1024, 'multihead_attention_version': 'ncc', 'encoder_position_encoding_version': 'ncc_learned', 'decoder_position_encoding_version': 'ncc_learned'}, 'optimization': {'max_epoch': 0, 'max_update': 30000, 'clip_norm': 0.0, 'update_freq': [4], 'lrs': [5e-05], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1500, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'sentence_avg': 0, 'label_smoothing': 0.1, 'adam': {'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': 0}}, 'checkpoint': {'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': 1, 'patience': 10, 'save_dir': '/mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints', 'should_continue': 0, 'model_name_or_path': None, 'cache_dir': None, 'logging_steps': 500, 'save_steps': 2000, 'save_total_limit': 2, 'overwrite_output_dir': 0, 'overwrite_cache': 0, 'init_checkpoint': '/mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt'}, 'eval': {'path': '/mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt', 'remove_bpe': 'sentencepiece', 'quiet': 1, 'results_path': None, 'model_overrides': '{}', 'topk': 5, 'max_sentences': 256, 'beam': 1, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 500, 'min_len': 1, 'match_source_len': 0, 'no_early_stop': 1, 'unnormalized': 0, 'no_beamable_mm': 0, 'lenpen': 1, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': 0, 'score_reference': 0, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': 0, 'sampling_topk': -1, 'sampling_topp': -1, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 0, 'print_step': 0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': 0, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': 0, 'retain_iter_history': 0, 'decoding_format': None, 'nltk_bleu': 1, 'rouge': 1}} (train.py:311, cli_main())
[2021-10-18 17:04:42]    INFO >> single GPU training... (train.py:340, cli_main())
[2021-10-18 17:04:43]    INFO >> [java] dictionary: 50005 types (bart_finetune.py:128, setup_task())
[2021-10-18 17:04:43]    INFO >> [python] dictionary: 50005 types (bart_finetune.py:129, setup_task())
[2021-10-18 17:04:45]    INFO >> truncate java/valid.code_tokens to 511 (bart_finetune.py:72, load_langpair_dataset())
[2021-10-18 17:04:48]    INFO >> truncate python/valid.code_tokens to 511 (bart_finetune.py:88, load_langpair_dataset())
[2021-10-18 17:04:52]    INFO >> Restore parameters from /mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt (train.py:228, single_main())
[2021-10-18 17:04:53]    INFO >> FairseqTransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
) (train.py:231, single_main())
[2021-10-18 17:04:53]    INFO >> model fairseq_transformer, criterion LabelSmoothedCrossEntropyCriterion (train.py:232, single_main())
[2021-10-18 17:04:53]    INFO >> num. model params: 139220736 (num. trained: 139220736) (train.py:235, single_main())
[2021-10-18 17:04:57]    INFO >> training on 1 GPUs (train.py:240, single_main())
[2021-10-18 17:04:57]    INFO >> max tokens per GPU = None and max sentences per GPU = 4 (train.py:243, single_main())
[2021-10-18 17:04:57]    INFO >> no existing checkpoint found /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (ncc_trainers.py:270, load_checkpoint())
[2021-10-18 17:04:57]    INFO >> loading train data for epoch 1 (ncc_trainers.py:285, get_train_iterator())
[2021-10-18 17:04:57]    INFO >> truncate java/train.code_tokens to 511 (bart_finetune.py:72, load_langpair_dataset())
[2021-10-18 17:04:57]    INFO >> truncate python/train.code_tokens to 511 (bart_finetune.py:88, load_langpair_dataset())
[2021-10-18 17:04:57]    INFO >> NOTE: your device may support faster training with fp16 (ncc_trainers.py:155, _setup_optimizer())
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:57: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
[2021-10-18 17:07:55]    INFO >> epoch 001:    500 / 2527 loss=6.147, nll_loss=4.334, ppl=20.16, wps=5370.9, ups=2.95, wpb=1820.5, bsz=16, num_updates=500, lr=1.7e-05, gnorm=16.245, train_wall=164, wall=178 (progress_bar.py:262, log())
[2021-10-18 17:10:46]    INFO >> epoch 001:   1000 / 2527 loss=3.492, nll_loss=1.652, ppl=3.14, wps=5469.6, ups=2.92, wpb=1872.3, bsz=16, num_updates=1000, lr=3.3e-05, gnorm=2.337, train_wall=165, wall=349 (progress_bar.py:262, log())
[2021-10-18 17:14:09]    INFO >> epoch 001:   1500 / 2527 loss=3.253, nll_loss=1.422, ppl=2.68, wps=5939.3, ups=2.47, wpb=2406.1, bsz=16, num_updates=1500, lr=5e-05, gnorm=1.866, train_wall=197, wall=552 (progress_bar.py:262, log())
[2021-10-18 17:18:22]    INFO >> epoch 001:   2000 / 2527 loss=3.225, nll_loss=1.408, ppl=2.65, wps=6270.4, ups=1.97, wpb=3182.7, bsz=16, num_updates=2000, lr=5e-05, gnorm=1.568, train_wall=248, wall=806 (progress_bar.py:262, log())
[2021-10-18 17:23:40]    INFO >> epoch 001:   2500 / 2527 loss=3.207, nll_loss=1.395, ppl=2.63, wps=6435.6, ups=1.58, wpb=4081.5, bsz=16, num_updates=2500, lr=5e-05, gnorm=1.396, train_wall=311, wall=1123 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 17:24:00]    INFO >> epoch 001 | loss 3.651 | nll_loss 1.831 | ppl 3.56 | wps 6001.5 | ups 2.23 | wpb 2695.1 | bsz 16 | num_updates 2527 | lr 5e-05 | gnorm 4.647 | train_wall 1105 | wall 1144 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 17:25:11]    INFO >> epoch 001 | valid on 'valid' subset | loss 2.626 | nll_loss 0.643 | ppl 1.56 | bleu 47.901 | wps 2712.4 | wpb 6157.8 | bsz 31.4 | num_updates 2527 (progress_bar.py:269, print())
[2021-10-18 17:25:16]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 1 @ 2527 updates, score 47.900994) (writing took 5.743075 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 17:28:06]    INFO >> epoch 002:    473 / 2527 loss=3.387, nll_loss=1.601, ppl=3.03, wps=3750.9, ups=1.88, wpb=1996.8, bsz=16, num_updates=3000, lr=5e-05, gnorm=1.836, train_wall=175, wall=1389 (progress_bar.py:262, log())
[2021-10-18 17:31:02]    INFO >> epoch 002:    973 / 2527 loss=3.033, nll_loss=1.214, ppl=2.32, wps=5225.6, ups=2.84, wpb=1842, bsz=16, num_updates=3500, lr=5e-05, gnorm=1.534, train_wall=170, wall=1565 (progress_bar.py:262, log())
[2021-10-18 17:34:23]    INFO >> epoch 002:   1473 / 2527 loss=2.929, nll_loss=1.101, ppl=2.14, wps=5892.3, ups=2.49, wpb=2365.3, bsz=16, num_updates=4000, lr=5e-05, gnorm=1.416, train_wall=195, wall=1766 (progress_bar.py:262, log())
[2021-10-18 17:38:36]    INFO >> epoch 002:   1973 / 2527 loss=2.963, nll_loss=1.14, ppl=2.2, wps=6197.3, ups=1.97, wpb=3144.3, bsz=16, num_updates=4500, lr=5e-05, gnorm=1.281, train_wall=247, wall=2020 (progress_bar.py:262, log())
[2021-10-18 17:43:51]    INFO >> epoch 002:   2473 / 2527 loss=2.997, nll_loss=1.178, ppl=2.26, wps=6336.8, ups=1.59, wpb=3993.2, bsz=16, num_updates=5000, lr=5e-05, gnorm=1.239, train_wall=309, wall=2335 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 17:44:34]    INFO >> epoch 002 | loss 3.036 | nll_loss 1.219 | ppl 2.33 | wps 5520.3 | ups 2.05 | wpb 2695.1 | bsz 16 | num_updates 5054 | lr 5e-05 | gnorm 1.456 | train_wall 1118 | wall 2377 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 17:45:56]    INFO >> epoch 002 | valid on 'valid' subset | loss 2.574 | nll_loss 0.604 | ppl 1.52 | bleu 49.1475 | wps 2255.8 | wpb 6157.8 | bsz 31.4 | num_updates 5054 | best_bleu 49.1475 (progress_bar.py:269, print())
[2021-10-18 17:46:08]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 2 @ 5054 updates, score 49.147509) (writing took 12.197620 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 17:48:45]    INFO >> epoch 003:    446 / 2527 loss=3.157, nll_loss=1.355, ppl=2.56, wps=3720.1, ups=1.7, wpb=2187.2, bsz=16, num_updates=5500, lr=5e-05, gnorm=1.628, train_wall=186, wall=2629 (progress_bar.py:262, log())
[2021-10-18 17:51:41]    INFO >> epoch 003:    946 / 2527 loss=2.87, nll_loss=1.04, ppl=2.06, wps=5184.2, ups=2.85, wpb=1821.3, bsz=16, num_updates=6000, lr=5e-05, gnorm=1.419, train_wall=170, wall=2804 (progress_bar.py:262, log())
[2021-10-18 17:55:04]    INFO >> epoch 003:   1446 / 2527 loss=2.784, nll_loss=0.946, ppl=1.93, wps=5735.8, ups=2.47, wpb=2323, bsz=16, num_updates=6500, lr=5e-05, gnorm=1.327, train_wall=197, wall=3007 (progress_bar.py:262, log())
[2021-10-18 17:59:10]    INFO >> epoch 003:   1946 / 2527 loss=2.832, nll_loss=1.001, ppl=2, wps=6308.1, ups=2.03, wpb=3106.2, bsz=16, num_updates=7000, lr=5e-05, gnorm=1.234, train_wall=240, wall=3253 (progress_bar.py:262, log())
[2021-10-18 18:04:17]    INFO >> epoch 003:   2446 / 2527 loss=2.872, nll_loss=1.044, ppl=2.06, wps=6416.5, ups=1.63, wpb=3948.3, bsz=16, num_updates=7500, lr=5e-05, gnorm=1.211, train_wall=302, wall=3561 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 18:05:18]    INFO >> epoch 003 | loss 2.889 | nll_loss 1.063 | ppl 2.09 | wps 5476.1 | ups 2.03 | wpb 2695.1 | bsz 16 | num_updates 7581 | lr 5e-05 | gnorm 1.36 | train_wall 1111 | wall 3621 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 18:06:28]    INFO >> epoch 003 | valid on 'valid' subset | loss 2.554 | nll_loss 0.599 | ppl 1.51 | bleu 49.6963 | wps 2688.4 | wpb 6157.8 | bsz 31.4 | num_updates 7581 | best_bleu 49.6963 (progress_bar.py:269, print())
[2021-10-18 18:06:40]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 3 @ 7581 updates, score 49.696326) (writing took 12.348591 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 18:09:11]    INFO >> epoch 004:    419 / 2527 loss=3.018, nll_loss=1.205, ppl=2.31, wps=3988.2, ups=1.71, wpb=2338.8, bsz=16, num_updates=8000, lr=5e-05, gnorm=1.547, train_wall=196, wall=3854 (progress_bar.py:262, log())
[2021-10-18 18:12:06]    INFO >> epoch 004:    919 / 2527 loss=2.764, nll_loss=0.926, ppl=1.9, wps=5095.9, ups=2.85, wpb=1787.4, bsz=16, num_updates=8500, lr=5e-05, gnorm=1.395, train_wall=169, wall=4029 (progress_bar.py:262, log())
[2021-10-18 18:15:21]    INFO >> epoch 004:   1419 / 2527 loss=2.688, nll_loss=0.842, ppl=1.79, wps=5870.5, ups=2.56, wpb=2290.4, bsz=16, num_updates=9000, lr=5e-05, gnorm=1.272, train_wall=189, wall=4224 (progress_bar.py:262, log())
[2021-10-18 18:19:24]    INFO >> epoch 004:   1919 / 2527 loss=2.737, nll_loss=0.898, ppl=1.86, wps=6287.7, ups=2.06, wpb=3059.6, bsz=16, num_updates=9500, lr=5e-05, gnorm=1.22, train_wall=237, wall=4468 (progress_bar.py:262, log())
[2021-10-18 18:24:29]    INFO >> epoch 004:   2419 / 2527 loss=2.778, nll_loss=0.943, ppl=1.92, wps=6378.3, ups=1.64, wpb=3888.3, bsz=16, num_updates=10000, lr=5e-05, gnorm=1.172, train_wall=299, wall=4773 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 18:25:48]    INFO >> epoch 004 | loss 2.788 | nll_loss 0.953 | ppl 1.94 | wps 5536.4 | ups 2.05 | wpb 2695.1 | bsz 16 | num_updates 10108 | lr 5e-05 | gnorm 1.318 | train_wall 1108 | wall 4851 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 18:26:58]    INFO >> epoch 004 | valid on 'valid' subset | loss 2.555 | nll_loss 0.613 | ppl 1.53 | bleu 49.5987 | wps 2698.7 | wpb 6157.8 | bsz 31.4 | num_updates 10108 | best_bleu 49.6963 (progress_bar.py:269, print())
[2021-10-18 18:27:05]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 4 @ 10108 updates, score 49.59872) (writing took 7.301814 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 18:29:24]    INFO >> epoch 005:    392 / 2527 loss=2.911, nll_loss=1.089, ppl=2.13, wps=4256.6, ups=1.69, wpb=2511.5, bsz=16, num_updates=10500, lr=5e-05, gnorm=1.524, train_wall=203, wall=5068 (progress_bar.py:262, log())
[2021-10-18 18:32:14]    INFO >> epoch 005:    892 / 2527 loss=2.682, nll_loss=0.836, ppl=1.79, wps=5204.4, ups=2.94, wpb=1769.3, bsz=16, num_updates=11000, lr=5e-05, gnorm=1.41, train_wall=164, wall=5238 (progress_bar.py:262, log())
[2021-10-18 18:35:26]    INFO >> epoch 005:   1392 / 2527 loss=2.61, nll_loss=0.757, ppl=1.69, wps=5879.8, ups=2.6, wpb=2259, bsz=16, num_updates=11500, lr=4.9e-05, gnorm=1.281, train_wall=186, wall=5430 (progress_bar.py:262, log())
[2021-10-18 18:39:26]    INFO >> epoch 005:   1892 / 2527 loss=2.654, nll_loss=0.807, ppl=1.75, wps=6274.4, ups=2.08, wpb=3012.8, bsz=16, num_updates=12000, lr=4.9e-05, gnorm=1.218, train_wall=234, wall=5670 (progress_bar.py:262, log())
[2021-10-18 18:44:27]    INFO >> epoch 005:   2392 / 2527 loss=2.706, nll_loss=0.866, ppl=1.82, wps=6354.4, ups=1.66, wpb=3825.2, bsz=16, num_updates=12500, lr=4.9e-05, gnorm=1.202, train_wall=295, wall=5971 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 18:46:06]    INFO >> epoch 005 | loss 2.708 | nll_loss 0.866 | ppl 1.82 | wps 5592.2 | ups 2.07 | wpb 2695.1 | bsz 16 | num_updates 12635 | lr 4.9e-05 | gnorm 1.325 | train_wall 1102 | wall 6069 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 18:47:16]    INFO >> epoch 005 | valid on 'valid' subset | loss 2.57 | nll_loss 0.631 | ppl 1.55 | bleu 49.6766 | wps 2698 | wpb 6157.8 | bsz 31.4 | num_updates 12635 | best_bleu 49.6963 (progress_bar.py:269, print())
[2021-10-18 18:47:23]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 5 @ 12635 updates, score 49.676633) (writing took 7.405153 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 18:49:39]    INFO >> epoch 006:    365 / 2527 loss=2.818, nll_loss=0.987, ppl=1.98, wps=4272.5, ups=1.6, wpb=2664.9, bsz=16, num_updates=13000, lr=4.9e-05, gnorm=1.509, train_wall=220, wall=6283 (progress_bar.py:262, log())
[2021-10-18 18:52:32]    INFO >> epoch 006:    865 / 2527 loss=2.624, nll_loss=0.773, ppl=1.71, wps=5083.6, ups=2.89, wpb=1758.2, bsz=16, num_updates=13500, lr=4.9e-05, gnorm=1.406, train_wall=167, wall=6456 (progress_bar.py:262, log())
[2021-10-18 18:55:33]    INFO >> epoch 006:   1365 / 2527 loss=2.543, nll_loss=0.683, ppl=1.61, wps=6111, ups=2.76, wpb=2213.6, bsz=16, num_updates=14000, lr=4.9e-05, gnorm=1.286, train_wall=175, wall=6637 (progress_bar.py:262, log())
[2021-10-18 18:59:30]    INFO >> epoch 006:   1865 / 2527 loss=2.588, nll_loss=0.736, ppl=1.67, wps=6265.3, ups=2.11, wpb=2971.7, bsz=16, num_updates=14500, lr=4.9e-05, gnorm=1.221, train_wall=231, wall=6874 (progress_bar.py:262, log())
[2021-10-18 19:04:27]    INFO >> epoch 006:   2365 / 2527 loss=2.643, nll_loss=0.798, ppl=1.74, wps=6324.9, ups=1.69, wpb=3753.1, bsz=16, num_updates=15000, lr=4.9e-05, gnorm=1.187, train_wall=291, wall=7170 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 19:06:25]    INFO >> epoch 006 | loss 2.641 | nll_loss 0.793 | ppl 1.73 | wps 5585.8 | ups 2.07 | wpb 2695.1 | bsz 16 | num_updates 15162 | lr 4.9e-05 | gnorm 1.319 | train_wall 1104 | wall 7288 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 19:07:47]    INFO >> epoch 006 | valid on 'valid' subset | loss 2.578 | nll_loss 0.653 | ppl 1.57 | bleu 49.8882 | wps 2255.3 | wpb 6157.8 | bsz 31.4 | num_updates 15162 | best_bleu 49.8882 (progress_bar.py:269, print())
[2021-10-18 19:07:59]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 6 @ 15162 updates, score 49.888249) (writing took 12.573824 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 19:10:02]    INFO >> epoch 007:    338 / 2527 loss=2.741, nll_loss=0.904, ppl=1.87, wps=4232.8, ups=1.49, wpb=2838.2, bsz=16, num_updates=15500, lr=4.9e-05, gnorm=1.483, train_wall=227, wall=7506 (progress_bar.py:262, log())
[2021-10-18 19:12:51]    INFO >> epoch 007:    838 / 2527 loss=2.579, nll_loss=0.725, ppl=1.65, wps=5167.2, ups=2.96, wpb=1744, bsz=16, num_updates=16000, lr=4.9e-05, gnorm=1.43, train_wall=163, wall=7675 (progress_bar.py:262, log())
[2021-10-18 19:16:02]    INFO >> epoch 007:   1338 / 2527 loss=2.484, nll_loss=0.62, ppl=1.54, wps=5700.5, ups=2.62, wpb=2178.4, bsz=16, num_updates=16500, lr=4.9e-05, gnorm=1.256, train_wall=185, wall=7866 (progress_bar.py:262, log())
[2021-10-18 19:19:57]    INFO >> epoch 007:   1838 / 2527 loss=2.532, nll_loss=0.675, ppl=1.6, wps=6273.8, ups=2.13, wpb=2946.1, bsz=16, num_updates=17000, lr=4.9e-05, gnorm=1.211, train_wall=229, wall=8100 (progress_bar.py:262, log())
[2021-10-18 19:24:49]    INFO >> epoch 007:   2338 / 2527 loss=2.588, nll_loss=0.739, ppl=1.67, wps=6351.9, ups=1.71, wpb=3706.8, bsz=16, num_updates=17500, lr=4.9e-05, gnorm=1.197, train_wall=286, wall=8392 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 19:27:03]    INFO >> epoch 007 | loss 2.584 | nll_loss 0.733 | ppl 1.66 | wps 5501.4 | ups 2.04 | wpb 2695.1 | bsz 16 | num_updates 17689 | lr 4.9e-05 | gnorm 1.313 | train_wall 1105 | wall 8526 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 19:28:12]    INFO >> epoch 007 | valid on 'valid' subset | loss 2.595 | nll_loss 0.673 | ppl 1.59 | bleu 49.2965 | wps 2717.3 | wpb 6157.8 | bsz 31.4 | num_updates 17689 | best_bleu 49.8882 (progress_bar.py:269, print())
[2021-10-18 19:28:20]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 7 @ 17689 updates, score 49.296465) (writing took 7.373699 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 19:30:07]    INFO >> epoch 008:    311 / 2527 loss=2.68, nll_loss=0.839, ppl=1.79, wps=4666.8, ups=1.57, wpb=2965.6, bsz=16, num_updates=18000, lr=4.9e-05, gnorm=1.452, train_wall=227, wall=8710 (progress_bar.py:262, log())
[2021-10-18 19:32:55]    INFO >> epoch 008:    811 / 2527 loss=2.533, nll_loss=0.676, ppl=1.6, wps=5083.9, ups=2.96, wpb=1715.6, bsz=16, num_updates=18500, lr=4.9e-05, gnorm=1.429, train_wall=163, wall=8879 (progress_bar.py:262, log())
[2021-10-18 19:36:03]    INFO >> epoch 008:   1311 / 2527 loss=2.439, nll_loss=0.572, ppl=1.49, wps=5751.9, ups=2.67, wpb=2153.4, bsz=16, num_updates=19000, lr=4.9e-05, gnorm=1.252, train_wall=181, wall=9066 (progress_bar.py:262, log())
[2021-10-18 19:39:58]    INFO >> epoch 008:   1811 / 2527 loss=2.483, nll_loss=0.623, ppl=1.54, wps=6172.6, ups=2.13, wpb=2901.1, bsz=16, num_updates=19500, lr=4.9e-05, gnorm=1.205, train_wall=229, wall=9301 (progress_bar.py:262, log())
[2021-10-18 19:44:47]    INFO >> epoch 008:   2311 / 2527 loss=2.539, nll_loss=0.686, ppl=1.61, wps=6338.1, ups=1.73, wpb=3663.2, bsz=16, num_updates=20000, lr=4.9e-05, gnorm=1.166, train_wall=283, wall=9590 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 19:47:18]    INFO >> epoch 008 | loss 2.536 | nll_loss 0.681 | ppl 1.6 | wps 5606 | ups 2.08 | wpb 2695.1 | bsz 16 | num_updates 20216 | lr 4.9e-05 | gnorm 1.3 | train_wall 1100 | wall 9741 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 19:48:40]    INFO >> epoch 008 | valid on 'valid' subset | loss 2.612 | nll_loss 0.699 | ppl 1.62 | bleu 49.316 | wps 2252.1 | wpb 6157.8 | bsz 31.4 | num_updates 20216 | best_bleu 49.8882 (progress_bar.py:269, print())
[2021-10-18 19:48:47]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 8 @ 20216 updates, score 49.316024) (writing took 7.243999 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 19:50:31]    INFO >> epoch 009:    284 / 2527 loss=2.627, nll_loss=0.783, ppl=1.72, wps=4518.8, ups=1.45, wpb=3111.7, bsz=16, num_updates=20500, lr=4.9e-05, gnorm=1.429, train_wall=241, wall=9934 (progress_bar.py:262, log())
[2021-10-18 19:53:19]    INFO >> epoch 009:    784 / 2527 loss=2.49, nll_loss=0.63, ppl=1.55, wps=5004.7, ups=2.98, wpb=1681.7, bsz=16, num_updates=21000, lr=4.9e-05, gnorm=1.396, train_wall=162, wall=10102 (progress_bar.py:262, log())
[2021-10-18 19:56:21]    INFO >> epoch 009:   1284 / 2527 loss=2.403, nll_loss=0.533, ppl=1.45, wps=5817.3, ups=2.74, wpb=2124.5, bsz=16, num_updates=21500, lr=4.9e-05, gnorm=1.241, train_wall=177, wall=10285 (progress_bar.py:262, log())
[2021-10-18 20:00:16]    INFO >> epoch 009:   1784 / 2527 loss=2.445, nll_loss=0.582, ppl=1.5, wps=6133.9, ups=2.13, wpb=2873.8, bsz=16, num_updates=22000, lr=4.9e-05, gnorm=1.187, train_wall=228, wall=10519 (progress_bar.py:262, log())
[2021-10-18 20:05:02]    INFO >> epoch 009:   2284 / 2527 loss=2.496, nll_loss=0.64, ppl=1.56, wps=6348.7, ups=1.75, wpb=3633.1, bsz=16, num_updates=22500, lr=4.9e-05, gnorm=1.144, train_wall=280, wall=10805 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 20:07:50]    INFO >> epoch 009 | loss 2.494 | nll_loss 0.636 | ppl 1.55 | wps 5527.4 | ups 2.05 | wpb 2695.1 | bsz 16 | num_updates 22743 | lr 4.9e-05 | gnorm 1.275 | train_wall 1105 | wall 10973 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 20:09:12]    INFO >> epoch 009 | valid on 'valid' subset | loss 2.622 | nll_loss 0.713 | ppl 1.64 | bleu 49.2666 | wps 2253.4 | wpb 6157.8 | bsz 31.4 | num_updates 22743 | best_bleu 49.8882 (progress_bar.py:269, print())
[2021-10-18 20:09:19]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 9 @ 22743 updates, score 49.266614) (writing took 7.537427 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 20:10:59]    INFO >> epoch 010:    257 / 2527 loss=2.575, nll_loss=0.726, ppl=1.65, wps=4490.3, ups=1.4, wpb=3205.1, bsz=16, num_updates=23000, lr=4.9e-05, gnorm=1.382, train_wall=253, wall=11162 (progress_bar.py:262, log())
[2021-10-18 20:13:47]    INFO >> epoch 010:    757 / 2527 loss=2.47, nll_loss=0.609, ppl=1.53, wps=5004.3, ups=2.97, wpb=1684.8, bsz=16, num_updates=23500, lr=4.9e-05, gnorm=1.415, train_wall=163, wall=11330 (progress_bar.py:262, log())
[2021-10-18 20:16:51]    INFO >> epoch 010:   1257 / 2527 loss=2.374, nll_loss=0.502, ppl=1.42, wps=5705.7, ups=2.72, wpb=2094, bsz=16, num_updates=24000, lr=4.9e-05, gnorm=1.219, train_wall=178, wall=11514 (progress_bar.py:262, log())
[2021-10-18 20:20:43]    INFO >> epoch 010:   1757 / 2527 loss=2.406, nll_loss=0.54, ppl=1.45, wps=6063.3, ups=2.15, wpb=2814.6, bsz=16, num_updates=24500, lr=4.9e-05, gnorm=1.162, train_wall=226, wall=11746 (progress_bar.py:262, log())
[2021-10-18 20:25:18]    INFO >> epoch 010:   2257 / 2527 loss=2.461, nll_loss=0.602, ppl=1.52, wps=6562.5, ups=1.82, wpb=3608.5, bsz=16, num_updates=25000, lr=4.9e-05, gnorm=1.134, train_wall=269, wall=12021 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 20:28:23]    INFO >> epoch 010 | loss 2.459 | nll_loss 0.599 | ppl 1.51 | wps 5524.7 | ups 2.05 | wpb 2695.1 | bsz 16 | num_updates 25270 | lr 4.9e-05 | gnorm 1.262 | train_wall 1105 | wall 12206 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 20:29:45]    INFO >> epoch 010 | valid on 'valid' subset | loss 2.644 | nll_loss 0.741 | ppl 1.67 | bleu 48.7067 | wps 2245.8 | wpb 6157.8 | bsz 31.4 | num_updates 25270 | best_bleu 49.8882 (progress_bar.py:269, print())
[2021-10-18 20:29:52]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 10 @ 25270 updates, score 48.706689) (writing took 7.344517 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 20:31:18]    INFO >> epoch 011:    230 / 2527 loss=2.53, nll_loss=0.678, ppl=1.6, wps=4601.8, ups=1.39, wpb=3321.1, bsz=16, num_updates=25500, lr=4.9e-05, gnorm=1.352, train_wall=257, wall=12382 (progress_bar.py:262, log())
[2021-10-18 20:34:06]    INFO >> epoch 011:    730 / 2527 loss=2.456, nll_loss=0.595, ppl=1.51, wps=4997.8, ups=2.98, wpb=1679, bsz=16, num_updates=26000, lr=4.9e-05, gnorm=1.369, train_wall=162, wall=12550 (progress_bar.py:262, log())
[2021-10-18 20:37:05]    INFO >> epoch 011:   1230 / 2527 loss=2.346, nll_loss=0.473, ppl=1.39, wps=5796.3, ups=2.79, wpb=2076.2, bsz=16, num_updates=26500, lr=4.9e-05, gnorm=1.19, train_wall=173, wall=12729 (progress_bar.py:262, log())
[2021-10-18 20:40:56]    INFO >> epoch 011:   1730 / 2527 loss=2.378, nll_loss=0.51, ppl=1.42, wps=6021.4, ups=2.17, wpb=2770.8, bsz=16, num_updates=27000, lr=4.9e-05, gnorm=1.137, train_wall=224, wall=12959 (progress_bar.py:262, log())
[2021-10-18 20:45:35]    INFO >> epoch 011:   2230 / 2527 loss=2.426, nll_loss=0.565, ppl=1.48, wps=6373.7, ups=1.79, wpb=3566, bsz=16, num_updates=27500, lr=4.9e-05, gnorm=1.129, train_wall=274, wall=13239 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 20:48:58]    INFO >> epoch 011 | loss 2.43 | nll_loss 0.567 | ppl 1.48 | wps 5510.8 | ups 2.04 | wpb 2695.1 | bsz 16 | num_updates 27797 | lr 4.9e-05 | gnorm 1.232 | train_wall 1108 | wall 13442 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 20:50:21]    INFO >> epoch 011 | valid on 'valid' subset | loss 2.659 | nll_loss 0.768 | ppl 1.7 | bleu 49.2842 | wps 2251.8 | wpb 6157.8 | bsz 31.4 | num_updates 27797 | best_bleu 49.8882 (progress_bar.py:269, print())
[2021-10-18 20:50:28]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 11 @ 27797 updates, score 49.284231) (writing took 7.494355 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 20:51:38]    INFO >> epoch 012:    203 / 2527 loss=2.497, nll_loss=0.642, ppl=1.56, wps=4711.6, ups=1.38, wpb=3417.1, bsz=16, num_updates=28000, lr=4.9e-05, gnorm=1.298, train_wall=258, wall=13601 (progress_bar.py:262, log())
[2021-10-18 20:54:19]    INFO >> epoch 012:    703 / 2527 loss=2.443, nll_loss=0.582, ppl=1.5, wps=5247.3, ups=3.1, wpb=1691.6, bsz=16, num_updates=28500, lr=4.9e-05, gnorm=1.371, train_wall=155, wall=13762 (progress_bar.py:262, log())
[2021-10-18 20:57:18]    INFO >> epoch 012:   1203 / 2527 loss=2.326, nll_loss=0.452, ppl=1.37, wps=5730.5, ups=2.79, wpb=2050.5, bsz=16, num_updates=29000, lr=4.9e-05, gnorm=1.19, train_wall=173, wall=13941 (progress_bar.py:262, log())
[2021-10-18 21:01:02]    INFO >> epoch 012:   1703 / 2527 loss=2.353, nll_loss=0.484, ppl=1.4, wps=6094.9, ups=2.24, wpb=2725.6, bsz=16, num_updates=29500, lr=4.9e-05, gnorm=1.14, train_wall=218, wall=14165 (progress_bar.py:262, log())
[2021-10-18 21:05:37]    INFO >> epoch 012:   2203 / 2527 loss=2.399, nll_loss=0.536, ppl=1.45, wps=6400.2, ups=1.81, wpb=3529.1, bsz=16, num_updates=30000, lr=4.9e-05, gnorm=1.092, train_wall=270, wall=14441 (progress_bar.py:262, log())
[2021-10-18 21:05:37]    INFO >> epoch 012 | loss 2.396 | nll_loss 0.531 | ppl 1.45 | wps 5423.5 | ups 2.21 | wpb 2459.4 | bsz 16 | num_updates 30000 | lr 4.9e-05 | gnorm 1.239 | train_wall 875 | wall 14441 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 21:06:47]    INFO >> epoch 012 | valid on 'valid' subset | loss 2.677 | nll_loss 0.788 | ppl 1.73 | bleu 48.44 | wps 2694.9 | wpb 6157.8 | bsz 31.4 | num_updates 30000 | best_bleu 49.8882 (progress_bar.py:269, print())
[2021-10-18 21:06:55]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 12 @ 30000 updates, score 48.439989) (writing took 7.577929 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 21:06:55]    INFO >> done training in 14517.8 seconds (train.py:290, single_main())
