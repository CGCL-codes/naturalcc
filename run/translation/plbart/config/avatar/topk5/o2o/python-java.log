nohup: ignoring input
Using backend: pytorch
[2021-10-18 17:04:50]    INFO >> Load arguments in /home/wanyao/yang/naturalcc-dev/run/translation/plbart/config/avatar/topk5/o2o/python-java.yml (train.py:309, cli_main())
[2021-10-18 17:04:50]    INFO >> {'criterion': 'label_smoothed_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 500, 'log_format': 'simple', 'tensorboard_logdir': '', 'memory_efficient_fp16': 1, 'fp16_no_flatten_grads': 1, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'empty_cache_freq': 0, 'task': 'bart_finetune', 'seed': 1234, 'cpu': 0, 'fp16': 0, 'fp16_opt_level': '01', 'bf16': 0, 'memory_efficient_bf16': 0, 'server_ip': '', 'server_port': ''}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 4, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'pipeline_model_parallel': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1, 'block_momentum': 0.875, 'block_lr': 1, 'use_nbm': 0, 'average_sync': 0}, 'task': {'data': '/mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap', 'source_lang': 'python', 'target_lang': 'java', 'load_alignments': 0, 'left_pad_source': 0, 'left_pad_target': 0, 'max_source_positions': 511, 'max_target_positions': 511, 'upsample_primary': 1, 'truncate_source': 1, 'truncate_target': 1, 'append_eos_to_target': 1, 'eval_bleu': 1, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': True, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_args': None, 'eval_bleu_print_samples': 0, 'eval_with_sacrebleu': 1}, 'model': {'arch': 'fairseq_transformer', 'offset_positions_by_padding': 1, 'pooler_dropout': 0.1, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.1, 'encoder_positional_embeddings': 0, 'encoder_learned_pos': 1, 'encoder_max_relative_len': 0, 'encoder_embed_path': 0, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_layers': 6, 'encoder_attention_heads': 12, 'encoder_normalize_before': 0, 'decoder_embed_path': '', 'decoder_positional_embeddings': 0, 'decoder_learned_pos': 1, 'decoder_max_relative_len': 0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_attention_heads': 12, 'decoder_normalize_before': 0, 'no_decoder_final_norm': 0, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.1, 'adaptive_softmax_factor': 0.0, 'share_decoder_input_output_embed': 1, 'decoder_out_embed_bias': 1, 'share_all_embeddings': 1, 'adaptive_input': 0, 'adaptive_input_factor': 0.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': 0, 'tie_adaptive_proj': 0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 1, 'no_scale_embedding': 0, 'no_token_positional_embeddings': 0, 'encoder_dropout_in': 0.1, 'encoder_dropout_out': 0.1, 'decoder_dropout_in': 0.1, 'decoder_dropout_out': 0.1, 'max_source_positions': 1024, 'max_target_positions': 1024, 'multihead_attention_version': 'ncc', 'encoder_position_encoding_version': 'ncc_learned', 'decoder_position_encoding_version': 'ncc_learned'}, 'optimization': {'max_epoch': 0, 'max_update': 30000, 'clip_norm': 0.0, 'update_freq': [4], 'lrs': [5e-05], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1500, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'sentence_avg': 0, 'label_smoothing': 0.1, 'adam': {'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': 0}}, 'checkpoint': {'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': 1, 'patience': 10, 'save_dir': '/mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints', 'should_continue': 0, 'model_name_or_path': None, 'cache_dir': None, 'logging_steps': 500, 'save_steps': 2000, 'save_total_limit': 2, 'overwrite_output_dir': 0, 'overwrite_cache': 0, 'init_checkpoint': '/mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt'}, 'eval': {'path': '/mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt', 'remove_bpe': 'sentencepiece', 'quiet': 1, 'results_path': None, 'model_overrides': '{}', 'topk': 5, 'max_sentences': 256, 'beam': 1, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 500, 'min_len': 1, 'match_source_len': 0, 'no_early_stop': 1, 'unnormalized': 0, 'no_beamable_mm': 0, 'lenpen': 1, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': 0, 'score_reference': 0, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': 0, 'sampling_topk': -1, 'sampling_topp': -1, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 0, 'print_step': 0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': 0, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': 0, 'retain_iter_history': 0, 'decoding_format': None, 'nltk_bleu': 1, 'rouge': 1}} (train.py:311, cli_main())
[2021-10-18 17:04:50]    INFO >> single GPU training... (train.py:340, cli_main())
[2021-10-18 17:04:51]    INFO >> [python] dictionary: 50005 types (bart_finetune.py:128, setup_task())
[2021-10-18 17:04:51]    INFO >> [java] dictionary: 50005 types (bart_finetune.py:129, setup_task())
[2021-10-18 17:04:53]    INFO >> truncate python/valid.code_tokens to 511 (bart_finetune.py:72, load_langpair_dataset())
[2021-10-18 17:04:56]    INFO >> truncate java/valid.code_tokens to 511 (bart_finetune.py:88, load_langpair_dataset())
[2021-10-18 17:05:01]    INFO >> Restore parameters from /mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt (train.py:228, single_main())
[2021-10-18 17:05:01]    INFO >> FairseqTransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
) (train.py:231, single_main())
[2021-10-18 17:05:01]    INFO >> model fairseq_transformer, criterion LabelSmoothedCrossEntropyCriterion (train.py:232, single_main())
[2021-10-18 17:05:01]    INFO >> num. model params: 139220736 (num. trained: 139220736) (train.py:235, single_main())
[2021-10-18 17:05:05]    INFO >> training on 1 GPUs (train.py:240, single_main())
[2021-10-18 17:05:05]    INFO >> max tokens per GPU = None and max sentences per GPU = 4 (train.py:243, single_main())
[2021-10-18 17:05:05]    INFO >> no existing checkpoint found /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (ncc_trainers.py:270, load_checkpoint())
[2021-10-18 17:05:05]    INFO >> loading train data for epoch 1 (ncc_trainers.py:285, get_train_iterator())
[2021-10-18 17:05:05]    INFO >> truncate python/train.code_tokens to 511 (bart_finetune.py:72, load_langpair_dataset())
[2021-10-18 17:05:05]    INFO >> truncate java/train.code_tokens to 511 (bart_finetune.py:88, load_langpair_dataset())
[2021-10-18 17:05:06]    INFO >> NOTE: your device may support faster training with fp16 (ncc_trainers.py:155, _setup_optimizer())
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:57: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
[2021-10-18 17:08:25]    INFO >> epoch 001:    500 / 2527 loss=5.253, nll_loss=3.347, ppl=10.18, wps=7195.4, ups=2.62, wpb=2743.2, bsz=16, num_updates=500, lr=1.7e-05, gnorm=13.266, train_wall=185, wall=200 (progress_bar.py:262, log())
[2021-10-18 17:11:47]    INFO >> epoch 001:   1000 / 2527 loss=3.122, nll_loss=1.237, ppl=2.36, wps=7515.5, ups=2.47, wpb=3045.1, bsz=16, num_updates=1000, lr=3.3e-05, gnorm=1.659, train_wall=196, wall=402 (progress_bar.py:262, log())
[2021-10-18 17:15:18]    INFO >> epoch 001:   1500 / 2527 loss=2.995, nll_loss=1.132, ppl=2.19, wps=7990.9, ups=2.37, wpb=3366.6, bsz=16, num_updates=1500, lr=5e-05, gnorm=1.358, train_wall=204, wall=613 (progress_bar.py:262, log())
[2021-10-18 17:19:19]    INFO >> epoch 001:   2000 / 2527 loss=2.913, nll_loss=1.06, ppl=2.08, wps=8030.3, ups=2.08, wpb=3861, bsz=16, num_updates=2000, lr=5e-05, gnorm=1.126, train_wall=234, wall=853 (progress_bar.py:262, log())
[2021-10-18 17:24:58]    INFO >> epoch 001:   2500 / 2527 loss=2.917, nll_loss=1.072, ppl=2.1, wps=7592.5, ups=1.47, wpb=5148.5, bsz=16, num_updates=2500, lr=5e-05, gnorm=1.034, train_wall=332, wall=1192 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 17:25:22]    INFO >> epoch 001 | loss 3.311 | nll_loss 1.446 | ppl 2.72 | wps 7682.6 | ups 2.09 | wpb 3672.5 | bsz 16 | num_updates 2527 | lr 5e-05 | gnorm 3.662 | train_wall 1174 | wall 1217 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 17:26:35]    INFO >> epoch 001 | valid on 'valid' subset | loss 2.598 | nll_loss 0.628 | ppl 1.55 | bleu 51.5598 | wps 2826.1 | wpb 6755.2 | bsz 31.4 | num_updates 2527 (progress_bar.py:269, print())
[2021-10-18 17:26:41]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 1 @ 2527 updates, score 51.559765) (writing took 5.955288 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 17:29:38]    INFO >> epoch 002:    473 / 2527 loss=2.783, nll_loss=0.93, ppl=1.91, wps=5341.3, ups=1.78, wpb=2994.5, bsz=16, num_updates=3000, lr=5e-05, gnorm=1.14, train_wall=186, wall=1473 (progress_bar.py:262, log())
[2021-10-18 17:32:45]    INFO >> epoch 002:    973 / 2527 loss=2.72, nll_loss=0.869, ppl=1.83, wps=8054.1, ups=2.68, wpb=3008.7, bsz=16, num_updates=3500, lr=5e-05, gnorm=1.041, train_wall=181, wall=1659 (progress_bar.py:262, log())
[2021-10-18 17:36:09]    INFO >> epoch 002:   1473 / 2527 loss=2.717, nll_loss=0.867, ppl=1.82, wps=8245.2, ups=2.45, wpb=3361.8, bsz=16, num_updates=4000, lr=5e-05, gnorm=0.976, train_wall=198, wall=1863 (progress_bar.py:262, log())
[2021-10-18 17:40:00]    INFO >> epoch 002:   1973 / 2527 loss=2.7, nll_loss=0.849, ppl=1.8, wps=8242.4, ups=2.16, wpb=3813.4, bsz=16, num_updates=4500, lr=5e-05, gnorm=0.906, train_wall=225, wall=2095 (progress_bar.py:262, log())
[2021-10-18 17:45:31]    INFO >> epoch 002:   2473 / 2527 loss=2.733, nll_loss=0.886, ppl=1.85, wps=7647.9, ups=1.51, wpb=5065.5, bsz=16, num_updates=5000, lr=5e-05, gnorm=0.892, train_wall=325, wall=2426 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 17:46:17]    INFO >> epoch 002 | loss 2.727 | nll_loss 0.877 | ppl 1.84 | wps 7396.1 | ups 2.01 | wpb 3672.5 | bsz 16 | num_updates 5054 | lr 5e-05 | gnorm 0.991 | train_wall 1136 | wall 2472 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 17:47:29]    INFO >> epoch 002 | valid on 'valid' subset | loss 2.554 | nll_loss 0.589 | ppl 1.5 | bleu 51.7552 | wps 2858 | wpb 6755.2 | bsz 31.4 | num_updates 5054 | best_bleu 51.7552 (progress_bar.py:269, print())
[2021-10-18 17:47:41]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 2 @ 5054 updates, score 51.755158) (writing took 12.319902 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 17:50:35]    INFO >> epoch 003:    446 / 2527 loss=2.655, nll_loss=0.8, ppl=1.74, wps=5203.8, ups=1.65, wpb=3160.5, bsz=16, num_updates=5500, lr=5e-05, gnorm=1.009, train_wall=205, wall=2729 (progress_bar.py:262, log())
[2021-10-18 17:53:45]    INFO >> epoch 003:    946 / 2527 loss=2.594, nll_loss=0.739, ppl=1.67, wps=7871.3, ups=2.63, wpb=2989.7, bsz=16, num_updates=6000, lr=5e-05, gnorm=0.929, train_wall=184, wall=2919 (progress_bar.py:262, log())
[2021-10-18 17:57:09]    INFO >> epoch 003:   1446 / 2527 loss=2.603, nll_loss=0.748, ppl=1.68, wps=8156.9, ups=2.44, wpb=3341.5, bsz=16, num_updates=6500, lr=5e-05, gnorm=0.897, train_wall=199, wall=3124 (progress_bar.py:262, log())
[2021-10-18 18:01:02]    INFO >> epoch 003:   1946 / 2527 loss=2.594, nll_loss=0.738, ppl=1.67, wps=8096, ups=2.15, wpb=3769.7, bsz=16, num_updates=7000, lr=5e-05, gnorm=0.863, train_wall=227, wall=3357 (progress_bar.py:262, log())
[2021-10-18 18:06:30]    INFO >> epoch 003:   2446 / 2527 loss=2.633, nll_loss=0.78, ppl=1.72, wps=7732.3, ups=1.53, wpb=5067.9, bsz=16, num_updates=7500, lr=5e-05, gnorm=0.847, train_wall=322, wall=3685 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 18:07:35]    INFO >> epoch 003 | loss 2.614 | nll_loss 0.759 | ppl 1.69 | wps 7260 | ups 1.98 | wpb 3672.5 | bsz 16 | num_updates 7581 | lr 5e-05 | gnorm 0.909 | train_wall 1154 | wall 3750 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 18:08:49]    INFO >> epoch 003 | valid on 'valid' subset | loss 2.536 | nll_loss 0.59 | ppl 1.51 | bleu 52.0215 | wps 2786.5 | wpb 6755.2 | bsz 31.4 | num_updates 7581 | best_bleu 52.0215 (progress_bar.py:269, print())
[2021-10-18 18:09:02]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 3 @ 7581 updates, score 52.021526) (writing took 13.005557 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 18:11:47]    INFO >> epoch 004:    419 / 2527 loss=2.575, nll_loss=0.717, ppl=1.64, wps=5102.8, ups=1.58, wpb=3238.6, bsz=16, num_updates=8000, lr=5e-05, gnorm=0.949, train_wall=216, wall=4002 (progress_bar.py:262, log())
[2021-10-18 18:14:59]    INFO >> epoch 004:    919 / 2527 loss=2.51, nll_loss=0.648, ppl=1.57, wps=7726.6, ups=2.61, wpb=2962.8, bsz=16, num_updates=8500, lr=5e-05, gnorm=0.894, train_wall=185, wall=4194 (progress_bar.py:262, log())
[2021-10-18 18:18:26]    INFO >> epoch 004:   1419 / 2527 loss=2.527, nll_loss=0.666, ppl=1.59, wps=8026.6, ups=2.41, wpb=3328.3, bsz=16, num_updates=9000, lr=5e-05, gnorm=0.877, train_wall=200, wall=4401 (progress_bar.py:262, log())
[2021-10-18 18:22:21]    INFO >> epoch 004:   1919 / 2527 loss=2.521, nll_loss=0.661, ppl=1.58, wps=7971.8, ups=2.13, wpb=3741.9, bsz=16, num_updates=9500, lr=5e-05, gnorm=0.86, train_wall=228, wall=4636 (progress_bar.py:262, log())
[2021-10-18 18:27:43]    INFO >> epoch 004:   2419 / 2527 loss=2.555, nll_loss=0.696, ppl=1.62, wps=7697, ups=1.55, wpb=4960.4, bsz=16, num_updates=10000, lr=5e-05, gnorm=0.835, train_wall=315, wall=4958 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 18:29:12]    INFO >> epoch 004 | loss 2.536 | nll_loss 0.676 | ppl 1.6 | wps 7157.9 | ups 1.95 | wpb 3672.5 | bsz 16 | num_updates 10108 | lr 5e-05 | gnorm 0.882 | train_wall 1167 | wall 5047 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 18:30:34]    INFO >> epoch 004 | valid on 'valid' subset | loss 2.541 | nll_loss 0.596 | ppl 1.51 | bleu 51.9521 | wps 2446.4 | wpb 6755.2 | bsz 31.4 | num_updates 10108 | best_bleu 52.0215 (progress_bar.py:269, print())
[2021-10-18 18:30:41]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 4 @ 10108 updates, score 51.952117) (writing took 7.751372 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 18:33:19]    INFO >> epoch 005:    392 / 2527 loss=2.517, nll_loss=0.654, ppl=1.57, wps=5111.2, ups=1.49, wpb=3432.3, bsz=16, num_updates=10500, lr=5e-05, gnorm=0.916, train_wall=231, wall=5294 (progress_bar.py:262, log())
[2021-10-18 18:36:32]    INFO >> epoch 005:    892 / 2527 loss=2.45, nll_loss=0.584, ppl=1.5, wps=7619.4, ups=2.6, wpb=2932.4, bsz=16, num_updates=11000, lr=5e-05, gnorm=0.882, train_wall=186, wall=5486 (progress_bar.py:262, log())
[2021-10-18 18:39:53]    INFO >> epoch 005:   1392 / 2527 loss=2.462, nll_loss=0.598, ppl=1.51, wps=8182.4, ups=2.48, wpb=3298.8, bsz=16, num_updates=11500, lr=4.9e-05, gnorm=0.872, train_wall=196, wall=5688 (progress_bar.py:262, log())
[2021-10-18 18:43:37]    INFO >> epoch 005:   1892 / 2527 loss=2.464, nll_loss=0.599, ppl=1.51, wps=8298.1, ups=2.23, wpb=3723.2, bsz=16, num_updates=12000, lr=4.9e-05, gnorm=0.839, train_wall=218, wall=5912 (progress_bar.py:262, log())
[2021-10-18 18:48:48]    INFO >> epoch 005:   2392 / 2527 loss=2.494, nll_loss=0.631, ppl=1.55, wps=7809.8, ups=1.61, wpb=4855.6, bsz=16, num_updates=12500, lr=4.9e-05, gnorm=0.831, train_wall=305, wall=6223 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 18:50:39]    INFO >> epoch 005 | loss 2.475 | nll_loss 0.611 | ppl 1.53 | wps 7211.3 | ups 1.96 | wpb 3672.5 | bsz 16 | num_updates 12635 | lr 4.9e-05 | gnorm 0.868 | train_wall 1158 | wall 6333 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 18:51:50]    INFO >> epoch 005 | valid on 'valid' subset | loss 2.542 | nll_loss 0.611 | ppl 1.53 | bleu 52.1399 | wps 2839.4 | wpb 6755.2 | bsz 31.4 | num_updates 12635 | best_bleu 52.1399 (progress_bar.py:269, print())
[2021-10-18 18:52:04]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 5 @ 12635 updates, score 52.139885) (writing took 13.034191 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 18:54:35]    INFO >> epoch 006:    365 / 2527 loss=2.469, nll_loss=0.603, ppl=1.52, wps=5180.2, ups=1.44, wpb=3594.6, bsz=16, num_updates=13000, lr=4.9e-05, gnorm=0.908, train_wall=247, wall=6570 (progress_bar.py:262, log())
[2021-10-18 18:57:55]    INFO >> epoch 006:    865 / 2527 loss=2.397, nll_loss=0.527, ppl=1.44, wps=7305.1, ups=2.51, wpb=2911.3, bsz=16, num_updates=13500, lr=4.9e-05, gnorm=0.879, train_wall=193, wall=6769 (progress_bar.py:262, log())
[2021-10-18 19:01:17]    INFO >> epoch 006:   1365 / 2527 loss=2.413, nll_loss=0.545, ppl=1.46, wps=8145.1, ups=2.47, wpb=3292.6, bsz=16, num_updates=14000, lr=4.9e-05, gnorm=0.879, train_wall=196, wall=6971 (progress_bar.py:262, log())
[2021-10-18 19:05:03]    INFO >> epoch 006:   1865 / 2527 loss=2.417, nll_loss=0.549, ppl=1.46, wps=8181.4, ups=2.21, wpb=3696.3, bsz=16, num_updates=14500, lr=4.9e-05, gnorm=0.831, train_wall=220, wall=7197 (progress_bar.py:262, log())
[2021-10-18 19:10:08]    INFO >> epoch 006:   2365 / 2527 loss=2.443, nll_loss=0.577, ppl=1.49, wps=7784.7, ups=1.64, wpb=4754.1, bsz=16, num_updates=15000, lr=4.9e-05, gnorm=0.826, train_wall=299, wall=7503 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 19:12:19]    INFO >> epoch 006 | loss 2.426 | nll_loss 0.558 | ppl 1.47 | wps 7136.3 | ups 1.94 | wpb 3672.5 | bsz 16 | num_updates 15162 | lr 4.9e-05 | gnorm 0.864 | train_wall 1174 | wall 7634 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 19:13:31]    INFO >> epoch 006 | valid on 'valid' subset | loss 2.555 | nll_loss 0.63 | ppl 1.55 | bleu 52.2903 | wps 2839 | wpb 6755.2 | bsz 31.4 | num_updates 15162 | best_bleu 52.2903 (progress_bar.py:269, print())
[2021-10-18 19:13:44]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 6 @ 15162 updates, score 52.290323) (writing took 12.977257 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 19:15:57]    INFO >> epoch 007:    338 / 2527 loss=2.428, nll_loss=0.559, ppl=1.47, wps=5388.1, ups=1.43, wpb=3766.8, bsz=16, num_updates=15500, lr=4.9e-05, gnorm=0.896, train_wall=249, wall=7852 (progress_bar.py:262, log())
[2021-10-18 19:19:04]    INFO >> epoch 007:    838 / 2527 loss=2.354, nll_loss=0.481, ppl=1.4, wps=7718.7, ups=2.68, wpb=2883.2, bsz=16, num_updates=16000, lr=4.9e-05, gnorm=0.881, train_wall=180, wall=8039 (progress_bar.py:262, log())
[2021-10-18 19:22:24]    INFO >> epoch 007:   1338 / 2527 loss=2.369, nll_loss=0.498, ppl=1.41, wps=8159.2, ups=2.5, wpb=3267.7, bsz=16, num_updates=16500, lr=4.9e-05, gnorm=0.86, train_wall=194, wall=8239 (progress_bar.py:262, log())
[2021-10-18 19:26:07]    INFO >> epoch 007:   1838 / 2527 loss=2.374, nll_loss=0.502, ppl=1.42, wps=8264.4, ups=2.25, wpb=3672.5, bsz=16, num_updates=17000, lr=4.9e-05, gnorm=0.808, train_wall=216, wall=8461 (progress_bar.py:262, log())
[2021-10-18 19:31:04]    INFO >> epoch 007:   2338 / 2527 loss=2.399, nll_loss=0.53, ppl=1.44, wps=7829.8, ups=1.68, wpb=4657.9, bsz=16, num_updates=17500, lr=4.9e-05, gnorm=0.814, train_wall=291, wall=8759 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 19:33:35]    INFO >> epoch 007 | loss 2.383 | nll_loss 0.512 | ppl 1.43 | wps 7272.4 | ups 1.98 | wpb 3672.5 | bsz 16 | num_updates 17689 | lr 4.9e-05 | gnorm 0.849 | train_wall 1149 | wall 8910 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 19:34:47]    INFO >> epoch 007 | valid on 'valid' subset | loss 2.575 | nll_loss 0.649 | ppl 1.57 | bleu 52.1943 | wps 2856.4 | wpb 6755.2 | bsz 31.4 | num_updates 17689 | best_bleu 52.2903 (progress_bar.py:269, print())
[2021-10-18 19:34:54]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 7 @ 17689 updates, score 52.194255) (writing took 7.341939 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 19:36:59]    INFO >> epoch 008:    311 / 2527 loss=2.392, nll_loss=0.521, ppl=1.44, wps=5521.9, ups=1.41, wpb=3915.7, bsz=16, num_updates=18000, lr=4.9e-05, gnorm=0.884, train_wall=260, wall=9113 (progress_bar.py:262, log())
[2021-10-18 19:40:08]    INFO >> epoch 008:    811 / 2527 loss=2.318, nll_loss=0.442, ppl=1.36, wps=7663.3, ups=2.65, wpb=2896.2, bsz=16, num_updates=18500, lr=4.9e-05, gnorm=0.853, train_wall=183, wall=9302 (progress_bar.py:262, log())
[2021-10-18 19:43:29]    INFO >> epoch 008:   1311 / 2527 loss=2.332, nll_loss=0.457, ppl=1.37, wps=8087.7, ups=2.49, wpb=3250.6, bsz=16, num_updates=19000, lr=4.9e-05, gnorm=0.833, train_wall=194, wall=9503 (progress_bar.py:262, log())
[2021-10-18 19:47:09]    INFO >> epoch 008:   1811 / 2527 loss=2.339, nll_loss=0.465, ppl=1.38, wps=8199.9, ups=2.26, wpb=3622.9, bsz=16, num_updates=19500, lr=4.9e-05, gnorm=0.821, train_wall=214, wall=9724 (progress_bar.py:262, log())
[2021-10-18 19:52:03]    INFO >> epoch 008:   2311 / 2527 loss=2.362, nll_loss=0.49, ppl=1.4, wps=7771.6, ups=1.7, wpb=4563.4, bsz=16, num_updates=20000, lr=4.9e-05, gnorm=0.815, train_wall=287, wall=10018 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 19:54:55]    INFO >> epoch 008 | loss 2.348 | nll_loss 0.475 | ppl 1.39 | wps 7254.5 | ups 1.98 | wpb 3672.5 | bsz 16 | num_updates 20216 | lr 4.9e-05 | gnorm 0.84 | train_wall 1157 | wall 10189 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 19:56:06]    INFO >> epoch 008 | valid on 'valid' subset | loss 2.588 | nll_loss 0.672 | ppl 1.59 | bleu 52.3195 | wps 2848.5 | wpb 6755.2 | bsz 31.4 | num_updates 20216 | best_bleu 52.3195 (progress_bar.py:269, print())
[2021-10-18 19:56:19]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 8 @ 20216 updates, score 52.319453) (writing took 12.778631 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 19:58:12]    INFO >> epoch 009:    284 / 2527 loss=2.364, nll_loss=0.492, ppl=1.41, wps=5554.7, ups=1.36, wpb=4096.9, bsz=16, num_updates=20500, lr=4.9e-05, gnorm=0.862, train_wall=269, wall=10387 (progress_bar.py:262, log())
[2021-10-18 20:01:19]    INFO >> epoch 009:    784 / 2527 loss=2.289, nll_loss=0.412, ppl=1.33, wps=7673.6, ups=2.68, wpb=2866.5, bsz=16, num_updates=21000, lr=4.9e-05, gnorm=0.834, train_wall=180, wall=10573 (progress_bar.py:262, log())
[2021-10-18 20:04:39]    INFO >> epoch 009:   1284 / 2527 loss=2.301, nll_loss=0.425, ppl=1.34, wps=8085.4, ups=2.5, wpb=3232.8, bsz=16, num_updates=21500, lr=4.9e-05, gnorm=0.825, train_wall=193, wall=10773 (progress_bar.py:262, log())
[2021-10-18 20:08:17]    INFO >> epoch 009:   1784 / 2527 loss=2.309, nll_loss=0.433, ppl=1.35, wps=8213.8, ups=2.28, wpb=3595.5, bsz=16, num_updates=22000, lr=4.9e-05, gnorm=0.788, train_wall=212, wall=10992 (progress_bar.py:262, log())
[2021-10-18 20:13:04]    INFO >> epoch 009:   2284 / 2527 loss=2.329, nll_loss=0.455, ppl=1.37, wps=7842.2, ups=1.75, wpb=4487.1, bsz=16, num_updates=22500, lr=4.9e-05, gnorm=0.795, train_wall=280, wall=11278 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 20:16:13]    INFO >> epoch 009 | loss 2.317 | nll_loss 0.442 | ppl 1.36 | wps 7258.4 | ups 1.98 | wpb 3672.5 | bsz 16 | num_updates 22743 | lr 4.9e-05 | gnorm 0.819 | train_wall 1153 | wall 11468 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 20:17:25]    INFO >> epoch 009 | valid on 'valid' subset | loss 2.601 | nll_loss 0.693 | ppl 1.62 | bleu 52.3864 | wps 2841 | wpb 6755.2 | bsz 31.4 | num_updates 22743 | best_bleu 52.3864 (progress_bar.py:269, print())
[2021-10-18 20:17:38]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 9 @ 22743 updates, score 52.386381) (writing took 12.634005 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 20:19:21]    INFO >> epoch 010:    257 / 2527 loss=2.337, nll_loss=0.463, ppl=1.38, wps=5575.6, ups=1.32, wpb=4208.2, bsz=16, num_updates=23000, lr=4.9e-05, gnorm=0.831, train_wall=278, wall=11656 (progress_bar.py:262, log())
[2021-10-18 20:22:28]    INFO >> epoch 010:    757 / 2527 loss=2.265, nll_loss=0.386, ppl=1.31, wps=7657.1, ups=2.67, wpb=2872, bsz=16, num_updates=23500, lr=4.9e-05, gnorm=0.816, train_wall=181, wall=11843 (progress_bar.py:262, log())
[2021-10-18 20:25:49]    INFO >> epoch 010:   1257 / 2527 loss=2.274, nll_loss=0.396, ppl=1.32, wps=7987.1, ups=2.5, wpb=3200.6, bsz=16, num_updates=24000, lr=4.9e-05, gnorm=0.82, train_wall=194, wall=12043 (progress_bar.py:262, log())
[2021-10-18 20:29:27]    INFO >> epoch 010:   1757 / 2527 loss=2.283, nll_loss=0.406, ppl=1.32, wps=8208.4, ups=2.29, wpb=3589.6, bsz=16, num_updates=24500, lr=4.9e-05, gnorm=0.775, train_wall=212, wall=12262 (progress_bar.py:262, log())
[2021-10-18 20:34:09]    INFO >> epoch 010:   2257 / 2527 loss=2.303, nll_loss=0.427, ppl=1.34, wps=7779.7, ups=1.77, wpb=4383.3, bsz=16, num_updates=25000, lr=4.9e-05, gnorm=0.791, train_wall=274, wall=12544 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 20:37:39]    INFO >> epoch 010 | loss 2.292 | nll_loss 0.415 | ppl 1.33 | wps 7216.9 | ups 1.97 | wpb 3672.5 | bsz 16 | num_updates 25270 | lr 4.9e-05 | gnorm 0.806 | train_wall 1158 | wall 12754 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 20:38:50]    INFO >> epoch 010 | valid on 'valid' subset | loss 2.624 | nll_loss 0.717 | ppl 1.64 | bleu 52.0237 | wps 2869.9 | wpb 6755.2 | bsz 31.4 | num_updates 25270 | best_bleu 52.3864 (progress_bar.py:269, print())
[2021-10-18 20:38:57]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 10 @ 25270 updates, score 52.023717) (writing took 7.250098 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 20:40:31]    INFO >> epoch 011:    230 / 2527 loss=2.315, nll_loss=0.44, ppl=1.36, wps=5724.8, ups=1.31, wpb=4370.9, bsz=16, num_updates=25500, lr=4.9e-05, gnorm=0.825, train_wall=288, wall=12926 (progress_bar.py:262, log())
[2021-10-18 20:43:37]    INFO >> epoch 011:    730 / 2527 loss=2.242, nll_loss=0.362, ppl=1.29, wps=7679, ups=2.69, wpb=2856.2, bsz=16, num_updates=26000, lr=4.9e-05, gnorm=0.825, train_wall=180, wall=13112 (progress_bar.py:262, log())
[2021-10-18 20:46:54]    INFO >> epoch 011:   1230 / 2527 loss=2.251, nll_loss=0.372, ppl=1.29, wps=8040.3, ups=2.54, wpb=3166.8, bsz=16, num_updates=26500, lr=4.9e-05, gnorm=0.794, train_wall=191, wall=13309 (progress_bar.py:262, log())
[2021-10-18 20:50:32]    INFO >> epoch 011:   1730 / 2527 loss=2.261, nll_loss=0.384, ppl=1.3, wps=8189.8, ups=2.29, wpb=3577.5, bsz=16, num_updates=27000, lr=4.9e-05, gnorm=0.763, train_wall=211, wall=13527 (progress_bar.py:262, log())
[2021-10-18 20:55:10]    INFO >> epoch 011:   2230 / 2527 loss=2.283, nll_loss=0.407, ppl=1.33, wps=7773.7, ups=1.8, wpb=4318.8, bsz=16, num_updates=27500, lr=4.9e-05, gnorm=0.783, train_wall=271, wall=13805 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 20:58:58]    INFO >> epoch 011 | loss 2.271 | nll_loss 0.393 | ppl 1.31 | wps 7255.3 | ups 1.98 | wpb 3672.5 | bsz 16 | num_updates 27797 | lr 4.9e-05 | gnorm 0.796 | train_wall 1158 | wall 14033 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 21:00:16]    INFO >> epoch 011 | valid on 'valid' subset | loss 2.634 | nll_loss 0.735 | ppl 1.66 | bleu 51.7122 | wps 2589.8 | wpb 6755.2 | bsz 31.4 | num_updates 27797 | best_bleu 52.3864 (progress_bar.py:269, print())
[2021-10-18 21:00:24]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 11 @ 27797 updates, score 51.712198) (writing took 7.708409 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 21:01:51]    INFO >> epoch 012:    203 / 2527 loss=2.296, nll_loss=0.421, ppl=1.34, wps=5596.4, ups=1.25, wpb=4492.5, bsz=16, num_updates=28000, lr=4.9e-05, gnorm=0.81, train_wall=300, wall=14206 (progress_bar.py:262, log())
[2021-10-18 21:05:03]    INFO >> epoch 012:    703 / 2527 loss=2.223, nll_loss=0.342, ppl=1.27, wps=7372.6, ups=2.61, wpb=2820.2, bsz=16, num_updates=28500, lr=4.9e-05, gnorm=0.787, train_wall=185, wall=14397 (progress_bar.py:262, log())
[2021-10-18 21:08:23]    INFO >> epoch 012:   1203 / 2527 loss=2.232, nll_loss=0.352, ppl=1.28, wps=7924.9, ups=2.5, wpb=3172.3, bsz=16, num_updates=29000, lr=4.9e-05, gnorm=0.77, train_wall=194, wall=14598 (progress_bar.py:262, log())
[2021-10-18 21:12:00]    INFO >> epoch 012:   1703 / 2527 loss=2.244, nll_loss=0.366, ppl=1.29, wps=8177.3, ups=2.3, wpb=3558.9, bsz=16, num_updates=29500, lr=4.9e-05, gnorm=0.757, train_wall=211, wall=14815 (progress_bar.py:262, log())
[2021-10-18 21:16:33]    INFO >> epoch 012:   2203 / 2527 loss=2.263, nll_loss=0.386, ppl=1.31, wps=7800.6, ups=1.83, wpb=4252.8, bsz=16, num_updates=30000, lr=4.9e-05, gnorm=0.76, train_wall=266, wall=15088 (progress_bar.py:262, log())
[2021-10-18 21:16:33]    INFO >> epoch 012 | loss 2.243 | nll_loss 0.364 | ppl 1.29 | wps 7066.8 | ups 2.09 | wpb 3384.1 | bsz 16 | num_updates 30000 | lr 4.9e-05 | gnorm 0.773 | train_wall 932 | wall 15088 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-18 21:17:50]    INFO >> epoch 012 | valid on 'valid' subset | loss 2.648 | nll_loss 0.755 | ppl 1.69 | bleu 51.5625 | wps 2624.2 | wpb 6755.2 | bsz 31.4 | num_updates 30000 | best_bleu 52.3864 (progress_bar.py:269, print())
[2021-10-18 21:17:58]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 12 @ 30000 updates, score 51.562513) (writing took 7.553987 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-10-18 21:17:58]    INFO >> done training in 15172.1 seconds (train.py:290, single_main())
