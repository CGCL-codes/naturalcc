nohup: ignoring input
Using backend: pytorch
[2021-10-19 01:42:02]    INFO >> Load arguments in /home/wanyao/yang/naturalcc-dev/run/translation/plbart/config/avatar/topk1/o2o/python-java.yml (train.py:309, cli_main())
[2021-10-19 01:42:02]    INFO >> {'criterion': 'label_smoothed_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 500, 'log_format': 'simple', 'tensorboard_logdir': '', 'memory_efficient_fp16': 1, 'fp16_no_flatten_grads': 1, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'empty_cache_freq': 0, 'task': 'bart_finetune', 'seed': 1234, 'cpu': 0, 'fp16': 0, 'fp16_opt_level': '01', 'bf16': 0, 'memory_efficient_bf16': 0, 'server_ip': '', 'server_port': ''}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 4, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'pipeline_model_parallel': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1, 'block_momentum': 0.875, 'block_lr': 1, 'use_nbm': 0, 'average_sync': 0}, 'task': {'data': '/mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap', 'source_lang': 'python', 'target_lang': 'java', 'load_alignments': 0, 'left_pad_source': 0, 'left_pad_target': 0, 'max_source_positions': 511, 'max_target_positions': 511, 'upsample_primary': 1, 'truncate_source': 1, 'truncate_target': 1, 'append_eos_to_target': 1, 'eval_bleu': 1, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': True, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_args': None, 'eval_bleu_print_samples': 0, 'eval_with_sacrebleu': 1}, 'model': {'arch': 'fairseq_transformer', 'offset_positions_by_padding': 1, 'pooler_dropout': 0.1, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.1, 'encoder_positional_embeddings': 0, 'encoder_learned_pos': 1, 'encoder_max_relative_len': 0, 'encoder_embed_path': 0, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_layers': 6, 'encoder_attention_heads': 12, 'encoder_normalize_before': 0, 'decoder_embed_path': '', 'decoder_positional_embeddings': 0, 'decoder_learned_pos': 1, 'decoder_max_relative_len': 0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_attention_heads': 12, 'decoder_normalize_before': 0, 'no_decoder_final_norm': 0, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.1, 'adaptive_softmax_factor': 0.0, 'share_decoder_input_output_embed': 1, 'decoder_out_embed_bias': 1, 'share_all_embeddings': 1, 'adaptive_input': 0, 'adaptive_input_factor': 0.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': 0, 'tie_adaptive_proj': 0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 1, 'no_scale_embedding': 0, 'no_token_positional_embeddings': 0, 'encoder_dropout_in': 0.1, 'encoder_dropout_out': 0.1, 'decoder_dropout_in': 0.1, 'decoder_dropout_out': 0.1, 'max_source_positions': 1024, 'max_target_positions': 1024, 'multihead_attention_version': 'ncc', 'encoder_position_encoding_version': 'ncc_learned', 'decoder_position_encoding_version': 'ncc_learned'}, 'optimization': {'max_epoch': 0, 'max_update': 30000, 'clip_norm': 0.0, 'update_freq': [4], 'lrs': [5e-05], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1500, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'sentence_avg': 0, 'label_smoothing': 0.1, 'adam': {'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': 0}}, 'checkpoint': {'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': 1, 'patience': 10, 'save_dir': '/mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints', 'should_continue': 0, 'model_name_or_path': None, 'cache_dir': None, 'logging_steps': 500, 'save_steps': 2000, 'save_total_limit': 2, 'overwrite_output_dir': 0, 'overwrite_cache': 0, 'init_checkpoint': '/mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt'}, 'eval': {'path': '/mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt', 'remove_bpe': 'sentencepiece', 'quiet': 1, 'results_path': None, 'model_overrides': '{}', 'topk': 5, 'max_sentences': 4, 'beam': 5, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 500, 'min_len': 1, 'match_source_len': 0, 'no_early_stop': 1, 'unnormalized': 0, 'no_beamable_mm': 0, 'lenpen': 1, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': 0, 'score_reference': 0, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': 0, 'sampling_topk': -1, 'sampling_topp': -1, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 0, 'print_step': 0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': 0, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': 0, 'retain_iter_history': 0, 'decoding_format': None, 'nltk_bleu': 1, 'rouge': 1}} (train.py:311, cli_main())
[2021-10-19 01:42:02]    INFO >> single GPU training... (train.py:340, cli_main())
[2021-10-19 01:42:02]    INFO >> [python] dictionary: 50005 types (bart_finetune.py:128, setup_task())
[2021-10-19 01:42:02]    INFO >> [java] dictionary: 50005 types (bart_finetune.py:129, setup_task())
[2021-10-19 01:42:05]    INFO >> truncate python/valid.code_tokens to 511 (bart_finetune.py:72, load_langpair_dataset())
[2021-10-19 01:42:07]    INFO >> truncate java/valid.code_tokens to 511 (bart_finetune.py:88, load_langpair_dataset())
[2021-10-19 01:42:12]    INFO >> Restore parameters from /mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt (train.py:228, single_main())
[2021-10-19 01:42:12]    INFO >> FairseqTransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
) (train.py:231, single_main())
[2021-10-19 01:42:12]    INFO >> model fairseq_transformer, criterion LabelSmoothedCrossEntropyCriterion (train.py:232, single_main())
[2021-10-19 01:42:12]    INFO >> num. model params: 139220736 (num. trained: 139220736) (train.py:233, single_main())
[2021-10-19 01:42:24]    INFO >> training on 1 GPUs (train.py:240, single_main())
[2021-10-19 01:42:24]    INFO >> max tokens per GPU = None and max sentences per GPU = 4 (train.py:241, single_main())
[2021-10-19 01:42:24]    INFO >> no existing checkpoint found /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (ncc_trainers.py:270, load_checkpoint())
[2021-10-19 01:42:24]    INFO >> loading train data for epoch 1 (ncc_trainers.py:285, get_train_iterator())
[2021-10-19 01:42:24]    INFO >> truncate python/train.code_tokens to 511 (bart_finetune.py:72, load_langpair_dataset())
[2021-10-19 01:42:24]    INFO >> truncate java/train.code_tokens to 511 (bart_finetune.py:88, load_langpair_dataset())
[2021-10-19 01:42:24]    INFO >> NOTE: your device may support faster training with fp16 (ncc_trainers.py:155, _setup_optimizer())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[2021-10-19 01:48:53]    INFO >> epoch 001 | loss 4.519 | nll_loss 2.51 | ppl 5.69 | wps 3322.1 | ups 0.98 | wpb 3388.8 | bsz 16 | num_updates 373 | lr 1.2e-05 | gnorm 14.913 | train_wall 375 | wall 389 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 01:59:28]    INFO >> epoch 001 | valid on 'valid' subset | loss 3.023 | nll_loss 0.873 | ppl 1.83 | bleu 55.7422 | wps 289.2 | wpb 6755.2 | bsz 31.4 | num_updates 373 (progress_bar.py:269, print())
[2021-10-19 01:59:34]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 1 @ 373 updates, score 55.742215) (writing took 6.238649 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 02:01:13]    INFO >> epoch 002:    127 / 373 loss=4.282, nll_loss=2.289, ppl=4.89, wps=1387, ups=0.45, wpb=3111.3, bsz=16, num_updates=500, lr=1.7e-05, gnorm=11.6, train_wall=463, wall=1129 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 02:06:04]    INFO >> epoch 002 | loss 2.888 | nll_loss 0.967 | ppl 1.95 | wps 1226.1 | ups 0.36 | wpb 3388.8 | bsz 16 | num_updates 746 | lr 2.5e-05 | gnorm 1.382 | train_wall 375 | wall 1420 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 02:16:00]    INFO >> epoch 002 | valid on 'valid' subset | loss 2.738 | nll_loss 0.722 | ppl 1.65 | bleu 60.2385 | wps 309 | wpb 6755.2 | bsz 31.4 | num_updates 746 | best_bleu 60.2385 (progress_bar.py:269, print())
[2021-10-19 02:19:28]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 2 @ 746 updates, score 60.238527) (writing took 207.541959 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 02:23:02]    INFO >> epoch 003:    254 / 373 loss=2.785, nll_loss=0.874, ppl=1.83, wps=1267.5, ups=0.38, wpb=3319.5, bsz=16, num_updates=1000, lr=3.3e-05, gnorm=1.18, train_wall=490, wall=2439 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 02:25:57]    INFO >> epoch 003 | loss 2.716 | nll_loss 0.817 | ppl 1.76 | wps 1059.6 | ups 0.31 | wpb 3388.8 | bsz 16 | num_updates 1119 | lr 3.7e-05 | gnorm 1.121 | train_wall 375 | wall 2613 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 02:36:05]    INFO >> epoch 003 | valid on 'valid' subset | loss 2.668 | nll_loss 0.665 | ppl 1.59 | bleu 61.428 | wps 302.2 | wpb 6755.2 | bsz 31.4 | num_updates 1119 | best_bleu 61.428 (progress_bar.py:269, print())
[2021-10-19 02:38:08]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 3 @ 1119 updates, score 61.428007) (writing took 122.247166 seconds) (checkpoint_utils.py:79, save_checkpoint())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 02:44:37]    INFO >> epoch 004 | loss 2.629 | nll_loss 0.739 | ppl 1.67 | wps 1128 | ups 0.33 | wpb 3388.8 | bsz 16 | num_updates 1492 | lr 5e-05 | gnorm 1.024 | train_wall 376 | wall 3734 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 02:54:58]    INFO >> epoch 004 | valid on 'valid' subset | loss 2.612 | nll_loss 0.642 | ppl 1.56 | bleu 62.336 | wps 296.3 | wpb 6755.2 | bsz 31.4 | num_updates 1492 | best_bleu 62.336 (progress_bar.py:269, print())
[2021-10-19 02:56:24]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 4 @ 1492 updates, score 62.336045) (writing took 86.182450 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 02:56:40]    INFO >> epoch 005:      8 / 373 loss=2.631, nll_loss=0.738, ppl=1.67, wps=922.6, ups=0.25, wpb=3722.2, bsz=16, num_updates=1500, lr=5e-05, gnorm=0.999, train_wall=555, wall=4456 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 03:02:54]    INFO >> epoch 005 | loss 2.565 | nll_loss 0.68 | ppl 1.6 | wps 1152.8 | ups 0.34 | wpb 3388.8 | bsz 16 | num_updates 1865 | lr 5e-05 | gnorm 0.965 | train_wall 376 | wall 4830 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 03:12:54]    INFO >> epoch 005 | valid on 'valid' subset | loss 2.602 | nll_loss 0.628 | ppl 1.55 | bleu 62.4166 | wps 305.2 | wpb 6755.2 | bsz 31.4 | num_updates 1865 | best_bleu 62.4166 (progress_bar.py:269, print())
[2021-10-19 03:15:04]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 5 @ 1865 updates, score 62.416636) (writing took 130.141954 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 03:16:49]    INFO >> epoch 006:    135 / 373 loss=2.58, nll_loss=0.696, ppl=1.62, wps=1287.1, ups=0.41, wpb=3112.4, bsz=16, num_updates=2000, lr=5e-05, gnorm=1.003, train_wall=463, wall=5665 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 03:21:33]    INFO >> epoch 006 | loss 2.514 | nll_loss 0.629 | ppl 1.55 | wps 1129.2 | ups 0.33 | wpb 3388.8 | bsz 16 | num_updates 2238 | lr 5e-05 | gnorm 0.918 | train_wall 375 | wall 5950 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 03:30:41]    INFO >> epoch 006 | valid on 'valid' subset | loss 2.588 | nll_loss 0.632 | ppl 1.55 | bleu 62.2484 | wps 337.5 | wpb 6755.2 | bsz 31.4 | num_updates 2238 | best_bleu 62.4166 (progress_bar.py:269, print())
[2021-10-19 03:48:25]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 6 @ 2238 updates, score 62.24842) (writing took 1063.547833 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 03:52:10]    INFO >> epoch 007:    262 / 373 loss=2.485, nll_loss=0.599, ppl=1.51, wps=786.8, ups=0.24, wpb=3337.6, bsz=16, num_updates=2500, lr=5e-05, gnorm=0.87, train_wall=493, wall=7786 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 03:54:55]    INFO >> epoch 007 | loss 2.474 | nll_loss 0.59 | ppl 1.5 | wps 631.4 | ups 0.19 | wpb 3388.8 | bsz 16 | num_updates 2611 | lr 5e-05 | gnorm 0.877 | train_wall 376 | wall 7952 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:04:27]    INFO >> epoch 007 | valid on 'valid' subset | loss 2.587 | nll_loss 0.633 | ppl 1.55 | bleu 62.0096 | wps 322 | wpb 6755.2 | bsz 31.4 | num_updates 2611 | best_bleu 62.4166 (progress_bar.py:269, print())
[2021-10-19 04:05:13]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 7 @ 2611 updates, score 62.009569) (writing took 46.502649 seconds) (checkpoint_utils.py:79, save_checkpoint())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:11:43]    INFO >> epoch 008 | loss 2.44 | nll_loss 0.555 | ppl 1.47 | wps 1254.2 | ups 0.37 | wpb 3388.8 | bsz 16 | num_updates 2984 | lr 5e-05 | gnorm 0.868 | train_wall 376 | wall 8960 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:21:21]    INFO >> epoch 008 | valid on 'valid' subset | loss 2.583 | nll_loss 0.636 | ppl 1.55 | bleu 62.0438 | wps 317.5 | wpb 6755.2 | bsz 31.4 | num_updates 2984 | best_bleu 62.4166 (progress_bar.py:269, print())
[2021-10-19 04:22:51]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 8 @ 2984 updates, score 62.043842) (writing took 89.820230 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 04:23:12]    INFO >> epoch 009:     16 / 373 loss=2.44, nll_loss=0.553, ppl=1.47, wps=991.5, ups=0.27, wpb=3692.1, bsz=16, num_updates=3000, lr=5e-05, gnorm=0.849, train_wall=551, wall=9648 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:29:20]    INFO >> epoch 009 | loss 2.412 | nll_loss 0.526 | ppl 1.44 | wps 1195.8 | ups 0.35 | wpb 3388.8 | bsz 16 | num_updates 3357 | lr 5e-05 | gnorm 0.853 | train_wall 375 | wall 10017 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:39:16]    INFO >> epoch 009 | valid on 'valid' subset | loss 2.596 | nll_loss 0.636 | ppl 1.55 | bleu 62.9364 | wps 307.9 | wpb 6755.2 | bsz 31.4 | num_updates 3357 | best_bleu 62.9364 (progress_bar.py:269, print())
[2021-10-19 04:40:44]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 9 @ 3357 updates, score 62.936428) (writing took 87.775677 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 04:42:35]    INFO >> epoch 010:    143 / 373 loss=2.422, nll_loss=0.536, ppl=1.45, wps=1342.9, ups=0.43, wpb=3123.5, bsz=16, num_updates=3500, lr=5e-05, gnorm=0.885, train_wall=463, wall=10811 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:47:13]    INFO >> epoch 010 | loss 2.386 | nll_loss 0.498 | ppl 1.41 | wps 1178.6 | ups 0.35 | wpb 3388.8 | bsz 16 | num_updates 3730 | lr 5e-05 | gnorm 0.83 | train_wall 375 | wall 11089 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:57:24]    INFO >> epoch 010 | valid on 'valid' subset | loss 2.593 | nll_loss 0.65 | ppl 1.57 | bleu 63.0136 | wps 300 | wpb 6755.2 | bsz 31.4 | num_updates 3730 | best_bleu 63.0136 (progress_bar.py:269, print())
[2021-10-19 04:58:53]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 10 @ 3730 updates, score 63.013601) (writing took 88.507018 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 05:02:43]    INFO >> epoch 011:    270 / 373 loss=2.366, nll_loss=0.477, ppl=1.39, wps=1390.5, ups=0.41, wpb=3360.3, bsz=16, num_updates=4000, lr=5e-05, gnorm=0.808, train_wall=493, wall=12019 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 05:05:18]    INFO >> epoch 011 | loss 2.363 | nll_loss 0.474 | ppl 1.39 | wps 1164.5 | ups 0.34 | wpb 3388.8 | bsz 16 | num_updates 4103 | lr 5e-05 | gnorm 0.828 | train_wall 372 | wall 12175 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 05:15:38]    INFO >> epoch 011 | valid on 'valid' subset | loss 2.601 | nll_loss 0.651 | ppl 1.57 | bleu 62.8421 | wps 295.8 | wpb 6755.2 | bsz 31.4 | num_updates 4103 | best_bleu 63.0136 (progress_bar.py:269, print())
[2021-10-19 05:16:26]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 11 @ 4103 updates, score 62.842107) (writing took 48.234993 seconds) (checkpoint_utils.py:79, save_checkpoint())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 05:22:56]    INFO >> epoch 012 | loss 2.341 | nll_loss 0.45 | ppl 1.37 | wps 1194.6 | ups 0.35 | wpb 3388.8 | bsz 16 | num_updates 4476 | lr 5e-05 | gnorm 0.827 | train_wall 376 | wall 13233 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 05:33:16]    INFO >> epoch 012 | valid on 'valid' subset | loss 2.592 | nll_loss 0.658 | ppl 1.58 | bleu 63.2531 | wps 296.1 | wpb 6755.2 | bsz 31.4 | num_updates 4476 | best_bleu 63.2531 (progress_bar.py:269, print())
[2021-10-19 05:35:16]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_best.pt (epoch 12 @ 4476 updates, score 63.253109) (writing took 120.363921 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 05:35:42]    INFO >> epoch 013:     24 / 373 loss=2.342, nll_loss=0.451, ppl=1.37, wps=925.5, ups=0.25, wpb=3663.6, bsz=16, num_updates=4500, lr=5e-05, gnorm=0.813, train_wall=547, wall=13999 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 05:41:45]    INFO >> epoch 013 | loss 2.322 | nll_loss 0.43 | ppl 1.35 | wps 1119.4 | ups 0.33 | wpb 3388.8 | bsz 16 | num_updates 4849 | lr 5e-05 | gnorm 0.824 | train_wall 376 | wall 14362 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 05:52:07]    INFO >> epoch 013 | valid on 'valid' subset | loss 2.593 | nll_loss 0.655 | ppl 1.58 | bleu 62.7655 | wps 295.2 | wpb 6755.2 | bsz 31.4 | num_updates 4849 | best_bleu 63.2531 (progress_bar.py:269, print())
[2021-10-19 05:53:32]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 13 @ 4849 updates, score 62.765457) (writing took 84.597997 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 05:55:29]    INFO >> epoch 014:    151 / 373 loss=2.329, nll_loss=0.436, ppl=1.35, wps=1319.5, ups=0.42, wpb=3132, bsz=16, num_updates=5000, lr=5e-05, gnorm=0.86, train_wall=465, wall=15185 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:00:01]    INFO >> epoch 014 | loss 2.304 | nll_loss 0.411 | ppl 1.33 | wps 1153.4 | ups 0.34 | wpb 3388.8 | bsz 16 | num_updates 5222 | lr 5e-05 | gnorm 0.819 | train_wall 376 | wall 15458 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:10:15]    INFO >> epoch 014 | valid on 'valid' subset | loss 2.595 | nll_loss 0.67 | ppl 1.59 | bleu 62.2465 | wps 299.1 | wpb 6755.2 | bsz 31.4 | num_updates 5222 | best_bleu 63.2531 (progress_bar.py:269, print())
[2021-10-19 06:11:00]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 14 @ 5222 updates, score 62.246451) (writing took 45.288490 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 06:15:05]    INFO >> epoch 015:    278 / 373 loss=2.288, nll_loss=0.393, ppl=1.31, wps=1437.8, ups=0.43, wpb=3380.8, bsz=16, num_updates=5500, lr=5e-05, gnorm=0.801, train_wall=501, wall=16361 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:17:24]    INFO >> epoch 015 | loss 2.287 | nll_loss 0.392 | ppl 1.31 | wps 1212.2 | ups 0.36 | wpb 3388.8 | bsz 16 | num_updates 5595 | lr 5e-05 | gnorm 0.826 | train_wall 370 | wall 16501 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:27:54]    INFO >> epoch 015 | valid on 'valid' subset | loss 2.618 | nll_loss 0.679 | ppl 1.6 | bleu 62.5687 | wps 291.7 | wpb 6755.2 | bsz 31.4 | num_updates 5595 | best_bleu 63.2531 (progress_bar.py:269, print())
[2021-10-19 06:29:31]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 15 @ 5595 updates, score 62.568727) (writing took 97.185756 seconds) (checkpoint_utils.py:79, save_checkpoint())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:36:00]    INFO >> epoch 016 | loss 2.272 | nll_loss 0.376 | ppl 1.3 | wps 1133 | ups 0.33 | wpb 3388.8 | bsz 16 | num_updates 5968 | lr 5e-05 | gnorm 0.794 | train_wall 375 | wall 17616 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:46:13]    INFO >> epoch 016 | valid on 'valid' subset | loss 2.626 | nll_loss 0.697 | ppl 1.62 | bleu 62.7272 | wps 298.2 | wpb 6755.2 | bsz 31.4 | num_updates 5968 | best_bleu 63.2531 (progress_bar.py:269, print())
[2021-10-19 06:46:58]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 16 @ 5968 updates, score 62.727244) (writing took 44.975738 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 06:47:29]    INFO >> epoch 017:     32 / 373 loss=2.274, nll_loss=0.379, ppl=1.3, wps=933.9, ups=0.26, wpb=3632.2, bsz=16, num_updates=6000, lr=5e-05, gnorm=0.788, train_wall=535, wall=18306 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:53:28]    INFO >> epoch 017 | loss 2.258 | nll_loss 0.361 | ppl 1.28 | wps 1206.2 | ups 0.36 | wpb 3388.8 | bsz 16 | num_updates 6341 | lr 5e-05 | gnorm 0.807 | train_wall 376 | wall 18664 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:03:41]    INFO >> epoch 017 | valid on 'valid' subset | loss 2.647 | nll_loss 0.72 | ppl 1.65 | bleu 62.7422 | wps 299.6 | wpb 6755.2 | bsz 31.4 | num_updates 6341 | best_bleu 63.2531 (progress_bar.py:269, print())
[2021-10-19 07:04:26]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 17 @ 6341 updates, score 62.742199) (writing took 45.286071 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 07:06:30]    INFO >> epoch 018:    159 / 373 loss=2.26, nll_loss=0.363, ppl=1.29, wps=1377.9, ups=0.44, wpb=3142.5, bsz=16, num_updates=6500, lr=5e-05, gnorm=0.827, train_wall=466, wall=19446 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:10:56]    INFO >> epoch 018 | loss 2.244 | nll_loss 0.346 | ppl 1.27 | wps 1205.6 | ups 0.36 | wpb 3388.8 | bsz 16 | num_updates 6714 | lr 5e-05 | gnorm 0.796 | train_wall 376 | wall 19712 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:21:10]    INFO >> epoch 018 | valid on 'valid' subset | loss 2.65 | nll_loss 0.73 | ppl 1.66 | bleu 62.8033 | wps 299.1 | wpb 6755.2 | bsz 31.4 | num_updates 6714 | best_bleu 63.2531 (progress_bar.py:269, print())
[2021-10-19 07:21:55]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 18 @ 6714 updates, score 62.803315) (writing took 44.935883 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 07:26:09]    INFO >> epoch 019:    286 / 373 loss=2.232, nll_loss=0.333, ppl=1.26, wps=1442.9, ups=0.42, wpb=3404.1, bsz=16, num_updates=7000, lr=5e-05, gnorm=0.775, train_wall=505, wall=20626 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:28:25]    INFO >> epoch 019 | loss 2.232 | nll_loss 0.333 | ppl 1.26 | wps 1205.5 | ups 0.36 | wpb 3388.8 | bsz 16 | num_updates 7087 | lr 5e-05 | gnorm 0.791 | train_wall 376 | wall 20761 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:38:26]    INFO >> epoch 019 | valid on 'valid' subset | loss 2.661 | nll_loss 0.739 | ppl 1.67 | bleu 62.2935 | wps 305.5 | wpb 6755.2 | bsz 31.4 | num_updates 7087 | best_bleu 63.2531 (progress_bar.py:269, print())
[2021-10-19 07:39:40]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 19 @ 7087 updates, score 62.293471) (writing took 73.669094 seconds) (checkpoint_utils.py:79, save_checkpoint())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:46:10]    INFO >> epoch 020 | loss 2.22 | nll_loss 0.321 | ppl 1.25 | wps 1186.9 | ups 0.35 | wpb 3388.8 | bsz 16 | num_updates 7460 | lr 5e-05 | gnorm 0.771 | train_wall 376 | wall 21826 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:56:34]    INFO >> epoch 020 | valid on 'valid' subset | loss 2.677 | nll_loss 0.765 | ppl 1.7 | bleu 63.1459 | wps 294.5 | wpb 6755.2 | bsz 31.4 | num_updates 7460 | best_bleu 63.2531 (progress_bar.py:269, print())
[2021-10-19 07:57:19]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 20 @ 7460 updates, score 63.145906) (writing took 45.263865 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 07:57:55]    INFO >> epoch 021:     40 / 373 loss=2.222, nll_loss=0.323, ppl=1.25, wps=943.6, ups=0.26, wpb=3597.2, bsz=16, num_updates=7500, lr=5e-05, gnorm=0.774, train_wall=536, wall=22532 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 08:03:47]    INFO >> epoch 021 | loss 2.21 | nll_loss 0.31 | ppl 1.24 | wps 1195.8 | ups 0.35 | wpb 3388.8 | bsz 16 | num_updates 7833 | lr 5e-05 | gnorm 0.773 | train_wall 374 | wall 22883 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 08:14:36]    INFO >> epoch 021 | valid on 'valid' subset | loss 2.705 | nll_loss 0.789 | ppl 1.73 | bleu 61.0789 | wps 283.3 | wpb 6755.2 | bsz 31.4 | num_updates 7833 | best_bleu 63.2531 (progress_bar.py:269, print())
[2021-10-19 08:15:21]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 21 @ 7833 updates, score 61.078891) (writing took 44.982213 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 08:17:30]    INFO >> epoch 022:    167 / 373 loss=2.212, nll_loss=0.312, ppl=1.24, wps=1343.1, ups=0.43, wpb=3156, bsz=16, num_updates=8000, lr=5e-05, gnorm=0.793, train_wall=465, wall=23707 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 08:21:50]    INFO >> epoch 022 | loss 2.2 | nll_loss 0.3 | ppl 1.23 | wps 1166.6 | ups 0.34 | wpb 3388.8 | bsz 16 | num_updates 8206 | lr 5e-05 | gnorm 0.762 | train_wall 375 | wall 23967 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 08:32:13]    INFO >> epoch 022 | valid on 'valid' subset | loss 2.718 | nll_loss 0.811 | ppl 1.75 | bleu 62.9263 | wps 295 | wpb 6755.2 | bsz 31.4 | num_updates 8206 | best_bleu 63.2531 (progress_bar.py:269, print())
[2021-10-19 08:32:58]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/python-java/checkpoints/checkpoint_last.pt (epoch 22 @ 8206 updates, score 62.926337) (writing took 45.269161 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 08:32:58]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:178, should_stop_early())
[2021-10-19 08:32:58]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:278, single_main())
[2021-10-19 08:32:58]    INFO >> done training in 24634.5 seconds (train.py:290, single_main())
