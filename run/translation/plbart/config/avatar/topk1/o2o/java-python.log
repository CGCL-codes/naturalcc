nohup: ignoring input
Using backend: pytorch
[2021-10-19 01:41:54]    INFO >> Load arguments in /home/wanyao/yang/naturalcc-dev/run/translation/plbart/config/avatar/topk1/o2o/java-python.yml (train.py:309, cli_main())
[2021-10-19 01:41:54]    INFO >> {'criterion': 'label_smoothed_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 500, 'log_format': 'simple', 'tensorboard_logdir': '', 'memory_efficient_fp16': 1, 'fp16_no_flatten_grads': 1, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'empty_cache_freq': 0, 'task': 'bart_finetune', 'seed': 1234, 'cpu': 0, 'fp16': 0, 'fp16_opt_level': '01', 'bf16': 0, 'memory_efficient_bf16': 0, 'server_ip': '', 'server_port': ''}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 4, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'pipeline_model_parallel': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1, 'block_momentum': 0.875, 'block_lr': 1, 'use_nbm': 0, 'average_sync': 0}, 'task': {'data': '/mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap', 'source_lang': 'java', 'target_lang': 'python', 'load_alignments': 0, 'left_pad_source': 0, 'left_pad_target': 0, 'max_source_positions': 511, 'max_target_positions': 511, 'upsample_primary': 1, 'truncate_source': 1, 'truncate_target': 1, 'append_eos_to_target': 1, 'eval_bleu': 1, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': True, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_args': None, 'eval_bleu_print_samples': 0, 'eval_with_sacrebleu': 1}, 'model': {'arch': 'fairseq_transformer', 'offset_positions_by_padding': 1, 'pooler_dropout': 0.1, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.1, 'encoder_positional_embeddings': 0, 'encoder_learned_pos': 1, 'encoder_max_relative_len': 0, 'encoder_embed_path': 0, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_layers': 6, 'encoder_attention_heads': 12, 'encoder_normalize_before': 0, 'decoder_embed_path': '', 'decoder_positional_embeddings': 0, 'decoder_learned_pos': 1, 'decoder_max_relative_len': 0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_attention_heads': 12, 'decoder_normalize_before': 0, 'no_decoder_final_norm': 0, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.1, 'adaptive_softmax_factor': 0.0, 'share_decoder_input_output_embed': 1, 'decoder_out_embed_bias': 1, 'share_all_embeddings': 1, 'adaptive_input': 0, 'adaptive_input_factor': 0.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': 0, 'tie_adaptive_proj': 0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 1, 'no_scale_embedding': 0, 'no_token_positional_embeddings': 0, 'encoder_dropout_in': 0.1, 'encoder_dropout_out': 0.1, 'decoder_dropout_in': 0.1, 'decoder_dropout_out': 0.1, 'max_source_positions': 1024, 'max_target_positions': 1024, 'multihead_attention_version': 'ncc', 'encoder_position_encoding_version': 'ncc_learned', 'decoder_position_encoding_version': 'ncc_learned'}, 'optimization': {'max_epoch': 0, 'max_update': 30000, 'clip_norm': 0.0, 'update_freq': [4], 'lrs': [5e-05], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1500, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'sentence_avg': 0, 'label_smoothing': 0.1, 'adam': {'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': 0}}, 'checkpoint': {'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': 1, 'patience': 10, 'save_dir': '/mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints', 'should_continue': 0, 'model_name_or_path': None, 'cache_dir': None, 'logging_steps': 500, 'save_steps': 2000, 'save_total_limit': 2, 'overwrite_output_dir': 0, 'overwrite_cache': 0, 'init_checkpoint': '/mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt'}, 'eval': {'path': '/mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt', 'remove_bpe': 'sentencepiece', 'quiet': 1, 'results_path': None, 'model_overrides': '{}', 'topk': 5, 'max_sentences': 4, 'beam': 5, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 500, 'min_len': 1, 'match_source_len': 0, 'no_early_stop': 1, 'unnormalized': 0, 'no_beamable_mm': 0, 'lenpen': 1, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': 0, 'score_reference': 0, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': 0, 'sampling_topk': -1, 'sampling_topp': -1, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 0, 'print_step': 0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': 0, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': 0, 'retain_iter_history': 0, 'decoding_format': None, 'nltk_bleu': 1, 'rouge': 1}} (train.py:311, cli_main())
[2021-10-19 01:41:54]    INFO >> single GPU training... (train.py:340, cli_main())
[2021-10-19 01:41:54]    INFO >> [java] dictionary: 50005 types (bart_finetune.py:128, setup_task())
[2021-10-19 01:41:54]    INFO >> [python] dictionary: 50005 types (bart_finetune.py:129, setup_task())
[2021-10-19 01:41:57]    INFO >> truncate java/valid.code_tokens to 511 (bart_finetune.py:72, load_langpair_dataset())
[2021-10-19 01:41:59]    INFO >> truncate python/valid.code_tokens to 511 (bart_finetune.py:88, load_langpair_dataset())
[2021-10-19 01:42:04]    INFO >> Restore parameters from /mnt/wanyao/ncc_data/clcdsa/plbart/checkpoint_11_100000.pt (train.py:228, single_main())
[2021-10-19 01:42:04]    INFO >> FairseqTransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
) (train.py:231, single_main())
[2021-10-19 01:42:04]    INFO >> model fairseq_transformer, criterion LabelSmoothedCrossEntropyCriterion (train.py:232, single_main())
[2021-10-19 01:42:04]    INFO >> num. model params: 139220736 (num. trained: 139220736) (train.py:233, single_main())
[2021-10-19 01:42:15]    INFO >> training on 1 GPUs (train.py:240, single_main())
[2021-10-19 01:42:15]    INFO >> max tokens per GPU = None and max sentences per GPU = 4 (train.py:241, single_main())
[2021-10-19 01:42:15]    INFO >> no existing checkpoint found /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (ncc_trainers.py:270, load_checkpoint())
[2021-10-19 01:42:15]    INFO >> loading train data for epoch 1 (ncc_trainers.py:285, get_train_iterator())
[2021-10-19 01:42:15]    INFO >> truncate java/train.code_tokens to 511 (bart_finetune.py:72, load_langpair_dataset())
[2021-10-19 01:42:15]    INFO >> truncate python/train.code_tokens to 511 (bart_finetune.py:88, load_langpair_dataset())
[2021-10-19 01:42:15]    INFO >> NOTE: your device may support faster training with fp16 (ncc_trainers.py:155, _setup_optimizer())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[2021-10-19 01:48:48]    INFO >> epoch 001 | loss 4.452 | nll_loss 2.446 | ppl 5.45 | wps 3026.4 | ups 0.97 | wpb 3115.7 | bsz 16 | num_updates 373 | lr 1.2e-05 | gnorm 16.282 | train_wall 379 | wall 393 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 01:59:25]    INFO >> epoch 001 | valid on 'valid' subset | loss 3.025 | nll_loss 0.896 | ppl 1.86 | bleu 51.3922 | wps 267.5 | wpb 6157.8 | bsz 31.4 | num_updates 373 (progress_bar.py:269, print())
[2021-10-19 01:59:31]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 1 @ 373 updates, score 51.392224) (writing took 6.614164 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 02:01:03]    INFO >> epoch 002:    127 / 373 loss=4.26, nll_loss=2.27, ppl=4.82, wps=1248.7, ups=0.45, wpb=2795.7, bsz=16, num_updates=500, lr=1.7e-05, gnorm=12.716, train_wall=460, wall=1128 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 02:06:02]    INFO >> epoch 002 | loss 2.923 | nll_loss 1.018 | ppl 2.03 | wps 1124.4 | ups 0.36 | wpb 3115.7 | bsz 16 | num_updates 746 | lr 2.5e-05 | gnorm 1.554 | train_wall 376 | wall 1427 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 02:15:40]    INFO >> epoch 002 | valid on 'valid' subset | loss 2.76 | nll_loss 0.728 | ppl 1.66 | bleu 58.0757 | wps 293.8 | wpb 6157.8 | bsz 31.4 | num_updates 746 | best_bleu 58.0757 (progress_bar.py:269, print())
[2021-10-19 02:18:41]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 2 @ 746 updates, score 58.075664) (writing took 180.674798 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 02:22:12]    INFO >> epoch 003:    254 / 373 loss=2.803, nll_loss=0.905, ppl=1.87, wps=1214.2, ups=0.39, wpb=3082.8, bsz=16, num_updates=1000, lr=3.3e-05, gnorm=1.278, train_wall=495, wall=2397 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 02:25:13]    INFO >> epoch 003 | loss 2.76 | nll_loss 0.875 | ppl 1.83 | wps 1009.9 | ups 0.32 | wpb 3115.7 | bsz 16 | num_updates 1119 | lr 3.7e-05 | gnorm 1.238 | train_wall 377 | wall 2578 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 02:35:18]    INFO >> epoch 003 | valid on 'valid' subset | loss 2.677 | nll_loss 0.682 | ppl 1.6 | bleu 60.3665 | wps 280.2 | wpb 6157.8 | bsz 31.4 | num_updates 1119 | best_bleu 60.3665 (progress_bar.py:269, print())
[2021-10-19 02:37:20]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 3 @ 1119 updates, score 60.366515) (writing took 122.734717 seconds) (checkpoint_utils.py:79, save_checkpoint())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 02:43:52]    INFO >> epoch 004 | loss 2.678 | nll_loss 0.8 | ppl 1.74 | wps 1038 | ups 0.33 | wpb 3115.7 | bsz 16 | num_updates 1492 | lr 5e-05 | gnorm 1.139 | train_wall 378 | wall 3697 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 02:54:12]    INFO >> epoch 004 | valid on 'valid' subset | loss 2.641 | nll_loss 0.652 | ppl 1.57 | bleu 60.3396 | wps 272.6 | wpb 6157.8 | bsz 31.4 | num_updates 1492 | best_bleu 60.3665 (progress_bar.py:269, print())
[2021-10-19 02:54:57]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 4 @ 1492 updates, score 60.33963) (writing took 45.396788 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 02:55:12]    INFO >> epoch 005:      8 / 373 loss=2.71, nll_loss=0.833, ppl=1.78, wps=872.9, ups=0.25, wpb=3456.4, bsz=16, num_updates=1500, lr=5e-05, gnorm=1.118, train_wall=562, wall=4377 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 03:01:30]    INFO >> epoch 005 | loss 2.613 | nll_loss 0.737 | ppl 1.67 | wps 1099.2 | ups 0.35 | wpb 3115.7 | bsz 16 | num_updates 1865 | lr 5e-05 | gnorm 1.058 | train_wall 378 | wall 4755 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 03:11:27]    INFO >> epoch 005 | valid on 'valid' subset | loss 2.612 | nll_loss 0.652 | ppl 1.57 | bleu 61.5275 | wps 282.2 | wpb 6157.8 | bsz 31.4 | num_updates 1865 | best_bleu 61.5275 (progress_bar.py:269, print())
[2021-10-19 03:12:54]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 5 @ 1865 updates, score 61.527515) (writing took 87.037906 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 03:14:30]    INFO >> epoch 006:    135 / 373 loss=2.619, nll_loss=0.744, ppl=1.68, wps=1205.9, ups=0.43, wpb=2793.4, bsz=16, num_updates=2000, lr=5e-05, gnorm=1.1, train_wall=458, wall=5535 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 03:19:25]    INFO >> epoch 006 | loss 2.561 | nll_loss 0.685 | ppl 1.61 | wps 1080.3 | ups 0.35 | wpb 3115.7 | bsz 16 | num_updates 2238 | lr 5e-05 | gnorm 1.009 | train_wall 377 | wall 5830 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 03:30:07]    INFO >> epoch 006 | valid on 'valid' subset | loss 2.608 | nll_loss 0.641 | ppl 1.56 | bleu 55.8856 | wps 262.5 | wpb 6157.8 | bsz 31.4 | num_updates 2238 | best_bleu 61.5275 (progress_bar.py:269, print())
[2021-10-19 03:48:19]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 6 @ 2238 updates, score 55.88561) (writing took 1092.252566 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 03:52:01]    INFO >> epoch 007:    262 / 373 loss=2.511, nll_loss=0.633, ppl=1.55, wps=690, ups=0.22, wpb=3105.8, bsz=16, num_updates=2500, lr=5e-05, gnorm=0.948, train_wall=500, wall=7786 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 03:54:52]    INFO >> epoch 007 | loss 2.518 | nll_loss 0.642 | ppl 1.56 | wps 546.5 | ups 0.18 | wpb 3115.7 | bsz 16 | num_updates 2611 | lr 5e-05 | gnorm 0.976 | train_wall 378 | wall 7957 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:05:13]    INFO >> epoch 007 | valid on 'valid' subset | loss 2.61 | nll_loss 0.651 | ppl 1.57 | bleu 60.6352 | wps 271.3 | wpb 6157.8 | bsz 31.4 | num_updates 2611 | best_bleu 61.5275 (progress_bar.py:269, print())
[2021-10-19 04:05:58]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 7 @ 2611 updates, score 60.635178) (writing took 44.767644 seconds) (checkpoint_utils.py:79, save_checkpoint())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:12:30]    INFO >> epoch 008 | loss 2.483 | nll_loss 0.605 | ppl 1.52 | wps 1098.5 | ups 0.35 | wpb 3115.7 | bsz 16 | num_updates 2984 | lr 5e-05 | gnorm 0.976 | train_wall 378 | wall 9015 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:23:19]    INFO >> epoch 008 | valid on 'valid' subset | loss 2.596 | nll_loss 0.644 | ppl 1.56 | bleu 56.7186 | wps 260.4 | wpb 6157.8 | bsz 31.4 | num_updates 2984 | best_bleu 61.5275 (progress_bar.py:269, print())
[2021-10-19 04:24:03]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 8 @ 2984 updates, score 56.718619) (writing took 44.018422 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 04:24:23]    INFO >> epoch 009:     16 / 373 loss=2.513, nll_loss=0.636, ppl=1.55, wps=882.8, ups=0.26, wpb=3429, bsz=16, num_updates=3000, lr=5e-05, gnorm=0.973, train_wall=559, wall=9728 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:30:35]    INFO >> epoch 009 | loss 2.452 | nll_loss 0.572 | ppl 1.49 | wps 1071.5 | ups 0.34 | wpb 3115.7 | bsz 16 | num_updates 3357 | lr 5e-05 | gnorm 0.948 | train_wall 378 | wall 10100 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:41:35]    INFO >> epoch 009 | valid on 'valid' subset | loss 2.61 | nll_loss 0.654 | ppl 1.57 | bleu 55.0487 | wps 254.2 | wpb 6157.8 | bsz 31.4 | num_updates 3357 | best_bleu 61.5275 (progress_bar.py:269, print())
[2021-10-19 04:42:20]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 9 @ 3357 updates, score 55.04873) (writing took 45.285773 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 04:44:03]    INFO >> epoch 010:    143 / 373 loss=2.448, nll_loss=0.567, ppl=1.48, wps=1187.7, ups=0.42, wpb=2803.5, bsz=16, num_updates=3500, lr=5e-05, gnorm=0.984, train_wall=459, wall=10908 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:48:51]    INFO >> epoch 010 | loss 2.424 | nll_loss 0.542 | ppl 1.46 | wps 1059.7 | ups 0.34 | wpb 3115.7 | bsz 16 | num_updates 3730 | lr 5e-05 | gnorm 0.923 | train_wall 377 | wall 11196 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 04:59:46]    INFO >> epoch 010 | valid on 'valid' subset | loss 2.591 | nll_loss 0.655 | ppl 1.57 | bleu 55.7699 | wps 257.7 | wpb 6157.8 | bsz 31.4 | num_updates 3730 | best_bleu 61.5275 (progress_bar.py:269, print())
[2021-10-19 05:00:32]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 10 @ 3730 updates, score 55.769935) (writing took 45.447038 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 05:04:23]    INFO >> epoch 011:    270 / 373 loss=2.387, nll_loss=0.503, ppl=1.42, wps=1283.8, ups=0.41, wpb=3131.4, bsz=16, num_updates=4000, lr=5e-05, gnorm=0.88, train_wall=504, wall=12128 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 05:07:04]    INFO >> epoch 011 | loss 2.398 | nll_loss 0.514 | ppl 1.43 | wps 1063.3 | ups 0.34 | wpb 3115.7 | bsz 16 | num_updates 4103 | lr 5e-05 | gnorm 0.926 | train_wall 379 | wall 12289 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 05:17:02]    INFO >> epoch 011 | valid on 'valid' subset | loss 2.598 | nll_loss 0.654 | ppl 1.57 | bleu 61.1823 | wps 280.7 | wpb 6157.8 | bsz 31.4 | num_updates 4103 | best_bleu 61.5275 (progress_bar.py:269, print())
[2021-10-19 05:17:46]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 11 @ 4103 updates, score 61.182327) (writing took 44.095893 seconds) (checkpoint_utils.py:79, save_checkpoint())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 05:24:17]    INFO >> epoch 012 | loss 2.373 | nll_loss 0.487 | ppl 1.4 | wps 1125.1 | ups 0.36 | wpb 3115.7 | bsz 16 | num_updates 4476 | lr 5e-05 | gnorm 0.898 | train_wall 377 | wall 13322 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 05:34:06]    INFO >> epoch 012 | valid on 'valid' subset | loss 2.604 | nll_loss 0.669 | ppl 1.59 | bleu 61.3675 | wps 285.7 | wpb 6157.8 | bsz 31.4 | num_updates 4476 | best_bleu 61.5275 (progress_bar.py:269, print())
[2021-10-19 05:35:26]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 12 @ 4476 updates, score 61.367514) (writing took 79.891679 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 05:35:51]    INFO >> epoch 013:     24 / 373 loss=2.402, nll_loss=0.518, ppl=1.43, wps=897.1, ups=0.26, wpb=3387.6, bsz=16, num_updates=4500, lr=5e-05, gnorm=0.923, train_wall=553, wall=14016 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 05:41:58]    INFO >> epoch 013 | loss 2.352 | nll_loss 0.465 | ppl 1.38 | wps 1095.8 | ups 0.35 | wpb 3115.7 | bsz 16 | num_updates 4849 | lr 5e-05 | gnorm 0.91 | train_wall 378 | wall 14383 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 05:52:09]    INFO >> epoch 013 | valid on 'valid' subset | loss 2.628 | nll_loss 0.681 | ppl 1.6 | bleu 62.5201 | wps 275.2 | wpb 6157.8 | bsz 31.4 | num_updates 4849 | best_bleu 62.5201 (progress_bar.py:269, print())
[2021-10-19 05:54:18]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_best.pt (epoch 13 @ 4849 updates, score 62.520079) (writing took 128.773105 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 05:56:07]    INFO >> epoch 014:    151 / 373 loss=2.342, nll_loss=0.453, ppl=1.37, wps=1158.1, ups=0.41, wpb=2816.9, bsz=16, num_updates=5000, lr=5e-05, gnorm=0.922, train_wall=460, wall=15232 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:00:49]    INFO >> epoch 014 | loss 2.33 | nll_loss 0.44 | ppl 1.36 | wps 1026.9 | ups 0.33 | wpb 3115.7 | bsz 16 | num_updates 5222 | lr 5e-05 | gnorm 0.887 | train_wall 377 | wall 15515 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:10:56]    INFO >> epoch 014 | valid on 'valid' subset | loss 2.632 | nll_loss 0.707 | ppl 1.63 | bleu 61.7565 | wps 277.6 | wpb 6157.8 | bsz 31.4 | num_updates 5222 | best_bleu 62.5201 (progress_bar.py:269, print())
[2021-10-19 06:11:40]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 14 @ 5222 updates, score 61.756486) (writing took 43.836735 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 06:15:41]    INFO >> epoch 015:    278 / 373 loss=2.303, nll_loss=0.411, ppl=1.33, wps=1342.7, ups=0.43, wpb=3152.5, bsz=16, num_updates=5500, lr=5e-05, gnorm=0.86, train_wall=508, wall=16406 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:18:07]    INFO >> epoch 015 | loss 2.312 | nll_loss 0.42 | ppl 1.34 | wps 1120.1 | ups 0.36 | wpb 3115.7 | bsz 16 | num_updates 5595 | lr 5e-05 | gnorm 0.904 | train_wall 373 | wall 16552 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:28:23]    INFO >> epoch 015 | valid on 'valid' subset | loss 2.642 | nll_loss 0.71 | ppl 1.64 | bleu 61.5684 | wps 274.4 | wpb 6157.8 | bsz 31.4 | num_updates 5595 | best_bleu 62.5201 (progress_bar.py:269, print())
[2021-10-19 06:30:02]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 15 @ 5595 updates, score 61.568428) (writing took 98.547565 seconds) (checkpoint_utils.py:79, save_checkpoint())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:36:34]    INFO >> epoch 016 | loss 2.294 | nll_loss 0.4 | ppl 1.32 | wps 1050.2 | ups 0.34 | wpb 3115.7 | bsz 16 | num_updates 5968 | lr 5e-05 | gnorm 0.896 | train_wall 378 | wall 17659 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:46:54]    INFO >> epoch 016 | valid on 'valid' subset | loss 2.653 | nll_loss 0.729 | ppl 1.66 | bleu 61.9673 | wps 271.5 | wpb 6157.8 | bsz 31.4 | num_updates 5968 | best_bleu 62.5201 (progress_bar.py:269, print())
[2021-10-19 06:47:38]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 16 @ 5968 updates, score 61.967348) (writing took 44.317371 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 06:48:09]    INFO >> epoch 017:     32 / 373 loss=2.319, nll_loss=0.426, ppl=1.34, wps=860.3, ups=0.26, wpb=3351.3, bsz=16, num_updates=6000, lr=5e-05, gnorm=0.921, train_wall=543, wall=18354 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 06:54:12]    INFO >> epoch 017 | loss 2.278 | nll_loss 0.383 | ppl 1.3 | wps 1098.5 | ups 0.35 | wpb 3115.7 | bsz 16 | num_updates 6341 | lr 5e-05 | gnorm 0.899 | train_wall 379 | wall 18717 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:04:34]    INFO >> epoch 017 | valid on 'valid' subset | loss 2.666 | nll_loss 0.747 | ppl 1.68 | bleu 61.8156 | wps 270.7 | wpb 6157.8 | bsz 31.4 | num_updates 6341 | best_bleu 62.5201 (progress_bar.py:269, print())
[2021-10-19 07:05:18]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 17 @ 6341 updates, score 61.815623) (writing took 43.827741 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 07:07:14]    INFO >> epoch 018:    159 / 373 loss=2.267, nll_loss=0.371, ppl=1.29, wps=1238.6, ups=0.44, wpb=2835.9, bsz=16, num_updates=6500, lr=5e-05, gnorm=0.901, train_wall=462, wall=19499 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:11:49]    INFO >> epoch 018 | loss 2.263 | nll_loss 0.367 | ppl 1.29 | wps 1099.7 | ups 0.35 | wpb 3115.7 | bsz 16 | num_updates 6714 | lr 5e-05 | gnorm 0.87 | train_wall 376 | wall 19774 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:22:21]    INFO >> epoch 018 | valid on 'valid' subset | loss 2.687 | nll_loss 0.761 | ppl 1.69 | bleu 60.4808 | wps 267.8 | wpb 6157.8 | bsz 31.4 | num_updates 6714 | best_bleu 62.5201 (progress_bar.py:269, print())
[2021-10-19 07:23:05]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 18 @ 6714 updates, score 60.480756) (writing took 44.252036 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 07:27:17]    INFO >> epoch 019:    286 / 373 loss=2.241, nll_loss=0.344, ppl=1.27, wps=1320.1, ups=0.42, wpb=3177.1, bsz=16, num_updates=7000, lr=5e-05, gnorm=0.826, train_wall=511, wall=20702 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:29:38]    INFO >> epoch 019 | loss 2.249 | nll_loss 0.351 | ppl 1.28 | wps 1086.8 | ups 0.35 | wpb 3115.7 | bsz 16 | num_updates 7087 | lr 5e-05 | gnorm 0.869 | train_wall 379 | wall 20843 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:40:09]    INFO >> epoch 019 | valid on 'valid' subset | loss 2.699 | nll_loss 0.783 | ppl 1.72 | bleu 58.8892 | wps 267.8 | wpb 6157.8 | bsz 31.4 | num_updates 7087 | best_bleu 62.5201 (progress_bar.py:269, print())
[2021-10-19 07:40:53]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 19 @ 7087 updates, score 58.889169) (writing took 44.164778 seconds) (checkpoint_utils.py:79, save_checkpoint())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:47:25]    INFO >> epoch 020 | loss 2.237 | nll_loss 0.338 | ppl 1.26 | wps 1088.8 | ups 0.35 | wpb 3115.7 | bsz 16 | num_updates 7460 | lr 5e-05 | gnorm 0.865 | train_wall 378 | wall 21910 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 07:58:07]    INFO >> epoch 020 | valid on 'valid' subset | loss 2.715 | nll_loss 0.806 | ppl 1.75 | bleu 58.9792 | wps 263.7 | wpb 6157.8 | bsz 31.4 | num_updates 7460 | best_bleu 62.5201 (progress_bar.py:269, print())
[2021-10-19 07:58:53]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 20 @ 7460 updates, score 58.979199) (writing took 45.085233 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 07:59:27]    INFO >> epoch 021:     40 / 373 loss=2.258, nll_loss=0.36, ppl=1.28, wps=856, ups=0.26, wpb=3304.5, bsz=16, num_updates=7500, lr=5e-05, gnorm=0.904, train_wall=542, wall=22632 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 08:05:24]    INFO >> epoch 021 | loss 2.224 | nll_loss 0.325 | ppl 1.25 | wps 1077 | ups 0.35 | wpb 3115.7 | bsz 16 | num_updates 7833 | lr 5e-05 | gnorm 0.87 | train_wall 378 | wall 22989 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 08:15:54]    INFO >> epoch 021 | valid on 'valid' subset | loss 2.725 | nll_loss 0.82 | ppl 1.76 | bleu 59.8434 | wps 268.5 | wpb 6157.8 | bsz 31.4 | num_updates 7833 | best_bleu 62.5201 (progress_bar.py:269, print())
[2021-10-19 08:16:38]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 21 @ 7833 updates, score 59.843389) (writing took 44.320413 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 08:18:41]    INFO >> epoch 022:    167 / 373 loss=2.214, nll_loss=0.315, ppl=1.24, wps=1238.9, ups=0.43, wpb=2858.1, bsz=16, num_updates=8000, lr=5e-05, gnorm=0.861, train_wall=464, wall=23786 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 08:23:11]    INFO >> epoch 022 | loss 2.213 | nll_loss 0.314 | ppl 1.24 | wps 1090.1 | ups 0.35 | wpb 3115.7 | bsz 16 | num_updates 8206 | lr 5e-05 | gnorm 0.853 | train_wall 378 | wall 24056 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 08:33:29]    INFO >> epoch 022 | valid on 'valid' subset | loss 2.754 | nll_loss 0.844 | ppl 1.8 | bleu 59.803 | wps 274 | wpb 6157.8 | bsz 31.4 | num_updates 8206 | best_bleu 62.5201 (progress_bar.py:269, print())
[2021-10-19 08:34:13]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 22 @ 8206 updates, score 59.803001) (writing took 44.154990 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 08:38:35]    INFO >> epoch 023:    294 / 373 loss=2.196, nll_loss=0.297, ppl=1.23, wps=1340.5, ups=0.42, wpb=3202.3, bsz=16, num_updates=8500, lr=5e-05, gnorm=0.802, train_wall=516, wall=24980 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 08:40:45]    INFO >> epoch 023 | loss 2.203 | nll_loss 0.302 | ppl 1.23 | wps 1102.1 | ups 0.35 | wpb 3115.7 | bsz 16 | num_updates 8579 | lr 5e-05 | gnorm 0.838 | train_wall 378 | wall 25110 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-10-19 08:50:46]    INFO >> epoch 023 | valid on 'valid' subset | loss 2.742 | nll_loss 0.84 | ppl 1.79 | bleu 61.2222 | wps 281.6 | wpb 6157.8 | bsz 31.4 | num_updates 8579 | best_bleu 62.5201 (progress_bar.py:269, print())
[2021-10-19 08:51:30]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top1/o2o/vanilla/data-mmap/plbart/java-python/checkpoints/checkpoint_last.pt (epoch 23 @ 8579 updates, score 61.222249) (writing took 44.519212 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-10-19 08:51:30]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:178, should_stop_early())
[2021-10-19 08:51:30]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:278, single_main())
[2021-10-19 08:51:30]    INFO >> done training in 25755.2 seconds (train.py:290, single_main())
