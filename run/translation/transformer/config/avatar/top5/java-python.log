nohup: ignoring input
Using backend: pytorch
[2021-11-14 16:55:25]    INFO >> Load arguments in /home/wanyao/yang/naturalcc-dev/run/translation/transformer/config/avatar/top5/java-python.yml (train.py:291, cli_main())
[2021-11-14 16:55:25]    INFO >> {'criterion': 'label_smoothed_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 500, 'log_format': 'simple', 'tensorboard_logdir': '', 'memory_efficient_fp16': 0, 'fp16_no_flatten_grads': 1, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'empty_cache_freq': 0, 'task': 'plbart_translation', 'seed': 1234, 'cpu': 0, 'fp16': 0, 'fp16_opt_level': '01', 'bf16': 0, 'memory_efficient_bf16': 0, 'server_ip': '', 'server_port': '', 'amp': 1, 'amp_batch_retries': 2, 'amp_init_scale': '2 ** 7', 'amp_scale_window': None}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 4, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 16, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'pipeline_model_parallel': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1, 'block_momentum': 0.875, 'block_lr': 1, 'use_nbm': 0, 'average_sync': 0}, 'task': {'data': '/mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap', 'source_lang': 'java', 'target_lang': 'python', 'load_alignments': 0, 'left_pad_source': 0, 'left_pad_target': 0, 'max_source_positions': 512, 'max_target_positions': 512, 'upsample_primary': 1, 'truncate_source': 1, 'truncate_target': 1, 'append_eos_to_target': 1, 'eval_bleu': 1, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': True, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_args': None, 'eval_bleu_print_samples': 0, 'eval_with_sacrebleu': 1}, 'model': {'arch': 'fairseq_transformer', 'offset_positions_by_padding': 1, 'pooler_dropout': 0.1, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.1, 'encoder_positional_embeddings': 0, 'encoder_learned_pos': 1, 'encoder_max_relative_len': 0, 'encoder_embed_path': 0, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_layers': 6, 'encoder_attention_heads': 12, 'encoder_normalize_before': 0, 'decoder_embed_path': '', 'decoder_positional_embeddings': 0, 'decoder_learned_pos': 1, 'decoder_max_relative_len': 0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_attention_heads': 12, 'decoder_normalize_before': 0, 'no_decoder_final_norm': 0, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.1, 'adaptive_softmax_factor': 0.0, 'share_decoder_input_output_embed': 1, 'decoder_out_embed_bias': 1, 'share_all_embeddings': 1, 'adaptive_input': 0, 'adaptive_input_factor': 0.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': 0, 'tie_adaptive_proj': 0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 1, 'no_scale_embedding': 0, 'no_token_positional_embeddings': 0, 'encoder_dropout_in': 0.1, 'encoder_dropout_out': 0.1, 'decoder_dropout_in': 0.1, 'decoder_dropout_out': 0.1, 'max_source_positions': 1024, 'max_target_positions': 1024, 'multihead_attention_version': 'ncc', 'encoder_position_encoding_version': 'ncc_learned', 'decoder_position_encoding_version': 'ncc_learned'}, 'optimization': {'max_epoch': 100, 'max_update': 0, 'clip_norm': 25.0, 'update_freq': [1], 'lrs': [5e-05], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1500, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'sentence_avg': 0, 'label_smoothing': 0.1, 'adam': {'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': 0}}, 'checkpoint': {'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': 1, 'patience': 10, 'save_dir': '/mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints', 'should_continue': 0, 'model_name_or_path': None, 'cache_dir': None, 'logging_steps': 500, 'save_steps': 2000, 'save_total_limit': 2, 'overwrite_output_dir': 0, 'overwrite_cache': 0}, 'eval': {'path': '/mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_best.pt', 'remove_bpe': 'sentencepiece', 'quiet': 1, 'results_path': None, 'model_overrides': '{}', 'topk': 5, 'max_sentences': 2, 'beam': 5, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 500, 'min_len': 1, 'match_source_len': 0, 'no_early_stop': 1, 'unnormalized': 0, 'no_beamable_mm': 0, 'lenpen': 1, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': 0, 'score_reference': 0, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': 0, 'sampling_topk': -1, 'sampling_topp': -1, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 0, 'print_step': 0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': 0, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': 0, 'retain_iter_history': 0, 'decoding_format': None, 'nltk_bleu': 1, 'rouge': 1}} (train.py:293, cli_main())
[2021-11-14 16:55:25]    INFO >> single GPU training... (train.py:322, cli_main())
[2021-11-14 16:55:25]    INFO >> [java] dictionary: 50005 types (plbart_translation.py:135, setup_task())
[2021-11-14 16:55:25]    INFO >> [python] dictionary: 50005 types (plbart_translation.py:136, setup_task())
[2021-11-14 16:55:28]    INFO >> truncate java/valid.code_tokens to 512 (plbart_translation.py:72, load_langpair_dataset())
[2021-11-14 16:55:32]    INFO >> truncate python/valid.code_tokens to 512 (plbart_translation.py:88, load_langpair_dataset())
[2021-11-14 16:55:35]    INFO >> FairseqTransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
) (train.py:213, single_main())
[2021-11-14 16:55:35]    INFO >> model fairseq_transformer, criterion LabelSmoothedCrossEntropyCriterion (train.py:214, single_main())
[2021-11-14 16:55:35]    INFO >> num. model params: 139220736 (num. trained: 139220736) (train.py:215, single_main())
[2021-11-14 16:55:44]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:542, pretty_print_cuda_env_list())
[2021-11-14 16:55:44]    INFO >> rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                     (utils.py:544, pretty_print_cuda_env_list())
[2021-11-14 16:55:44]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:550, pretty_print_cuda_env_list())
[2021-11-14 16:55:44]    INFO >> training on 1 GPUs (train.py:222, single_main())
[2021-11-14 16:55:44]    INFO >> max tokens per GPU = None and max sentences per GPU = 4 (train.py:223, single_main())
[2021-11-14 16:55:44]    INFO >> no existing checkpoint found /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())
[2021-11-14 16:55:44]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())
[2021-11-14 16:55:44]    INFO >> truncate java/train.code_tokens to 512 (plbart_translation.py:72, load_langpair_dataset())
[2021-11-14 16:55:44]    INFO >> truncate python/train.code_tokens to 512 (plbart_translation.py:88, load_langpair_dataset())
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[2021-11-14 16:55:53]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 16:55:53]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 16:55:55]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 16:55:55]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 16:58:08]    INFO >> epoch 001:    501 / 10106 loss=79.716, nll_loss=72.064, ppl=4.93591e+21, wps=2358.1, ups=3.68, wpb=640.6, bsz=4, num_updates=500, lr=1.7e-05, gnorm=123.639, clip=100, loss_scale=16, train_wall=134, gb_free=29.3, wall=144 (progress_bar.py:260, log())
[2021-11-14 17:00:26]    INFO >> epoch 001:   1001 / 10106 loss=46.797, nll_loss=37.151, ppl=1.52576e+11, wps=2438.3, ups=3.62, wpb=673.6, bsz=4, num_updates=1000, lr=3.3e-05, gnorm=45.707, clip=98.8, loss_scale=16, train_wall=137, gb_free=28.8, wall=282 (progress_bar.py:260, log())
[2021-11-14 17:02:42]    INFO >> epoch 001:   1501 / 10106 loss=39.89, nll_loss=30.511, ppl=1.53035e+09, wps=2392, ups=3.68, wpb=650.5, bsz=4, num_updates=1500, lr=5e-05, gnorm=32.682, clip=86, loss_scale=16, train_wall=135, gb_free=28.5, wall=418 (progress_bar.py:260, log())
[2021-11-14 17:04:39]    INFO >> epoch 001:   2001 / 10106 loss=35.488, nll_loss=26.675, ppl=1.07174e+08, wps=3024.3, ups=4.3, wpb=702.7, bsz=4, num_updates=2000, lr=5e-05, gnorm=26.769, clip=59.4, loss_scale=16, train_wall=115, gb_free=28.9, wall=534 (progress_bar.py:260, log())
[2021-11-14 17:06:08]    INFO >> epoch 001:   2501 / 10106 loss=31.147, nll_loss=22.696, ppl=6.79489e+06, wps=3733.5, ups=5.62, wpb=664.7, bsz=4, num_updates=2500, lr=5e-05, gnorm=27.165, clip=57.2, loss_scale=32, train_wall=88, gb_free=28.4, wall=623 (progress_bar.py:260, log())
[2021-11-14 17:07:38]    INFO >> epoch 001:   3001 / 10106 loss=27.992, nll_loss=20.205, ppl=1.20839e+06, wps=3981.2, ups=5.53, wpb=719.8, bsz=4, num_updates=3000, lr=5e-05, gnorm=34.671, clip=92.8, loss_scale=32, train_wall=89, gb_free=28.1, wall=714 (progress_bar.py:260, log())
[2021-11-14 17:08:24]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 17:09:31]    INFO >> epoch 001:   3501 / 10106 loss=24.04, nll_loss=16.981, ppl=129341, wps=3304.8, ups=4.41, wpb=749.2, bsz=4, num_updates=3500, lr=5e-05, gnorm=35.802, clip=95.6, loss_scale=16, train_wall=112, gb_free=27.5, wall=827 (progress_bar.py:260, log())
[2021-11-14 17:11:53]    INFO >> epoch 001:   4001 / 10106 loss=21.196, nll_loss=14.701, ppl=26638, wps=2423.4, ups=3.53, wpb=686.6, bsz=4, num_updates=4000, lr=5e-05, gnorm=31.491, clip=89.4, loss_scale=16, train_wall=141, gb_free=26.1, wall=969 (progress_bar.py:260, log())
[2021-11-14 17:14:10]    INFO >> epoch 001:   4501 / 10106 loss=19.131, nll_loss=13.061, ppl=8547.31, wps=2427.5, ups=3.64, wpb=666.1, bsz=4, num_updates=4500, lr=5e-05, gnorm=29.268, clip=77.6, loss_scale=16, train_wall=136, gb_free=29.4, wall=1106 (progress_bar.py:260, log())
[2021-11-14 17:16:33]    INFO >> epoch 001:   5001 / 10106 loss=18.202, nll_loss=12.605, ppl=6230.2, wps=2450.5, ups=3.49, wpb=701.6, bsz=4, num_updates=5000, lr=5e-05, gnorm=27.521, clip=62.2, loss_scale=16, train_wall=142, gb_free=29.4, wall=1249 (progress_bar.py:260, log())
[2021-11-14 17:18:20]    INFO >> epoch 001:   5501 / 10106 loss=16.525, nll_loss=11.24, ppl=2419.19, wps=3212, ups=4.69, wpb=685, bsz=4, num_updates=5500, lr=5e-05, gnorm=26.398, clip=55.8, loss_scale=32, train_wall=106, gb_free=29.1, wall=1356 (progress_bar.py:260, log())
[2021-11-14 17:19:53]    INFO >> epoch 001:   6001 / 10106 loss=15.649, nll_loss=10.628, ppl=1582.02, wps=3472.1, ups=5.41, wpb=642.3, bsz=4, num_updates=6000, lr=5e-05, gnorm=26.289, clip=49.6, loss_scale=32, train_wall=91, gb_free=29.3, wall=1448 (progress_bar.py:260, log())
[2021-11-14 17:21:22]    INFO >> epoch 001:   6501 / 10106 loss=14.616, nll_loss=9.821, ppl=904.54, wps=3904.4, ups=5.58, wpb=699.5, bsz=4, num_updates=6500, lr=5e-05, gnorm=23.907, clip=33.6, loss_scale=32, train_wall=89, gb_free=26.3, wall=1538 (progress_bar.py:260, log())
[2021-11-14 17:23:20]    INFO >> epoch 001:   7001 / 10106 loss=13.617, nll_loss=9.023, ppl=520.2, wps=2747, ups=4.25, wpb=646, bsz=4, num_updates=7000, lr=5e-05, gnorm=22.809, clip=22.2, loss_scale=32, train_wall=116, gb_free=29, wall=1656 (progress_bar.py:260, log())
[2021-11-14 17:23:59]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 17:25:35]    INFO >> epoch 001:   7501 / 10106 loss=13.005, nll_loss=8.639, ppl=398.7, wps=2365.9, ups=3.69, wpb=641.9, bsz=4, num_updates=7500, lr=5e-05, gnorm=22.378, clip=20, loss_scale=16, train_wall=134, gb_free=28.5, wall=1791 (progress_bar.py:260, log())
[2021-11-14 17:27:52]    INFO >> epoch 001:   8001 / 10106 loss=12.236, nll_loss=8.126, ppl=279.37, wps=2404.2, ups=3.65, wpb=658.8, bsz=4, num_updates=8000, lr=5e-05, gnorm=20.716, clip=10, loss_scale=16, train_wall=136, gb_free=27.9, wall=1928 (progress_bar.py:260, log())
[2021-11-14 17:30:10]    INFO >> epoch 001:   8501 / 10106 loss=11.722, nll_loss=7.85, ppl=230.75, wps=2375, ups=3.64, wpb=652, bsz=4, num_updates=8500, lr=5e-05, gnorm=20.795, clip=12, loss_scale=16, train_wall=136, gb_free=28.5, wall=2065 (progress_bar.py:260, log())
[2021-11-14 17:31:48]    INFO >> epoch 001:   9001 / 10106 loss=11.089, nll_loss=7.419, ppl=171.12, wps=3395.2, ups=5.09, wpb=667.2, bsz=4, num_updates=9000, lr=5e-05, gnorm=19.356, clip=9, loss_scale=16, train_wall=97, gb_free=26.9, wall=2164 (progress_bar.py:260, log())
[2021-11-14 17:33:19]    INFO >> epoch 001:   9501 / 10106 loss=10.512, nll_loss=7.015, ppl=129.32, wps=3758.6, ups=5.51, wpb=682.4, bsz=4, num_updates=9500, lr=5e-05, gnorm=18.127, clip=4, loss_scale=32, train_wall=90, gb_free=28.8, wall=2255 (progress_bar.py:260, log())
[2021-11-14 17:34:49]    INFO >> epoch 001:  10001 / 10106 loss=10.152, nll_loss=6.791, ppl=110.73, wps=3902.6, ups=5.53, wpb=705.5, bsz=4, num_updates=10000, lr=5e-05, gnorm=17.462, clip=4.8, loss_scale=32, train_wall=89, gb_free=29.4, wall=2345 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 17:35:24]    INFO >> epoch 001 | loss 23.374 | nll_loss 17.419 | ppl 175246 | wps 2888 | ups 4.26 | wpb 677.8 | bsz 4 | num_updates 10105 | lr 5e-05 | gnorm 31.494 | clip 51.5 | loss_scale 32 | train_wall 2333 | gb_free 29.3 | wall 2380 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 17:45:49]    INFO >> epoch 001 | valid on 'valid' subset | loss 8.352 | nll_loss 5.034 | ppl 32.75 | bleu 2.64653 | wps 280.8 | wpb 3153 | bsz 16 | num_updates 10105 (progress_bar.py:269, print())
[2021-11-14 17:45:54]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_best.pt (epoch 1 @ 10105 updates, score 2.646527) (writing took 5.827864 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 17:46:47]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 17:46:47]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 17:46:57]    INFO >> epoch 002:    396 / 10106 loss=9.669, nll_loss=6.409, ppl=84.95, wps=477.5, ups=0.69, wpb=695.1, bsz=4, num_updates=10500, lr=5e-05, gnorm=17.182, clip=3.8, loss_scale=16, train_wall=73, gb_free=29, wall=3073 (progress_bar.py:260, log())
[2021-11-14 17:48:23]    INFO >> epoch 002:    896 / 10106 loss=9.224, nll_loss=6.103, ppl=68.73, wps=3956.9, ups=5.78, wpb=684.5, bsz=4, num_updates=11000, lr=5e-05, gnorm=16.083, clip=1.8, loss_scale=16, train_wall=85, gb_free=29.1, wall=3159 (progress_bar.py:260, log())
[2021-11-14 17:50:43]    INFO >> epoch 002:   1396 / 10106 loss=8.794, nll_loss=5.802, ppl=55.78, wps=2351.4, ups=3.58, wpb=656.2, bsz=4, num_updates=11500, lr=4.9e-05, gnorm=15.95, clip=2.4, loss_scale=16, train_wall=138, gb_free=29.2, wall=3299 (progress_bar.py:260, log())
[2021-11-14 17:53:02]    INFO >> epoch 002:   1896 / 10106 loss=8.534, nll_loss=5.686, ppl=51.47, wps=2357.8, ups=3.59, wpb=657.3, bsz=4, num_updates=12000, lr=4.9e-05, gnorm=15.416, clip=1.6, loss_scale=16, train_wall=138, gb_free=29.3, wall=3438 (progress_bar.py:260, log())
[2021-11-14 17:55:04]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 17:55:23]    INFO >> epoch 002:   2396 / 10106 loss=8.12, nll_loss=5.37, ppl=41.36, wps=2336.4, ups=3.56, wpb=656.1, bsz=4, num_updates=12500, lr=4.9e-05, gnorm=14.799, clip=1.6, loss_scale=16, train_wall=139, gb_free=29.3, wall=3579 (progress_bar.py:260, log())
[2021-11-14 17:57:17]    INFO >> epoch 002:   2896 / 10106 loss=7.884, nll_loss=5.253, ppl=38.13, wps=2959.7, ups=4.37, wpb=676.5, bsz=4, num_updates=13000, lr=4.9e-05, gnorm=14.156, clip=1, loss_scale=16, train_wall=113, gb_free=29.4, wall=3693 (progress_bar.py:260, log())
[2021-11-14 17:58:48]    INFO >> epoch 002:   3396 / 10106 loss=7.52, nll_loss=4.975, ppl=31.45, wps=3669.5, ups=5.53, wpb=663.7, bsz=4, num_updates=13500, lr=4.9e-05, gnorm=13.618, clip=0.8, loss_scale=16, train_wall=89, gb_free=29, wall=3783 (progress_bar.py:260, log())
[2021-11-14 18:00:21]    INFO >> epoch 002:   3896 / 10106 loss=7.343, nll_loss=4.905, ppl=29.96, wps=3879.1, ups=5.33, wpb=727.9, bsz=4, num_updates=14000, lr=4.9e-05, gnorm=12.701, clip=0.4, loss_scale=16, train_wall=93, gb_free=28.4, wall=3877 (progress_bar.py:260, log())
[2021-11-14 18:02:11]    INFO >> epoch 002:   4396 / 10106 loss=7.134, nll_loss=4.781, ppl=27.49, wps=3064.3, ups=4.57, wpb=671, bsz=4, num_updates=14500, lr=4.9e-05, gnorm=12.513, clip=0.8, loss_scale=32, train_wall=108, gb_free=28.6, wall=3987 (progress_bar.py:260, log())
[2021-11-14 18:04:31]    INFO >> epoch 002:   4896 / 10106 loss=6.843, nll_loss=4.544, ppl=23.34, wps=2481.2, ups=3.56, wpb=696.5, bsz=4, num_updates=15000, lr=4.9e-05, gnorm=12.228, clip=0.2, loss_scale=32, train_wall=139, gb_free=28.6, wall=4127 (progress_bar.py:260, log())
[2021-11-14 18:06:54]    INFO >> epoch 002:   5396 / 10106 loss=6.66, nll_loss=4.42, ppl=21.41, wps=2452.8, ups=3.5, wpb=700.9, bsz=4, num_updates=15500, lr=4.9e-05, gnorm=11.568, clip=0.2, loss_scale=32, train_wall=142, gb_free=28.2, wall=4270 (progress_bar.py:260, log())
[2021-11-14 18:09:12]    INFO >> epoch 002:   5896 / 10106 loss=6.413, nll_loss=4.217, ppl=18.59, wps=2386.8, ups=3.63, wpb=658.3, bsz=4, num_updates=16000, lr=4.9e-05, gnorm=11.352, clip=0, loss_scale=32, train_wall=137, gb_free=28.3, wall=4408 (progress_bar.py:260, log())
[2021-11-14 18:10:49]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:10:53]    INFO >> epoch 002:   6396 / 10106 loss=6.205, nll_loss=4.058, ppl=16.65, wps=3368.8, ups=4.97, wpb=677.3, bsz=4, num_updates=16500, lr=4.9e-05, gnorm=10.635, clip=0.2, loss_scale=32, train_wall=99, gb_free=29.3, wall=4508 (progress_bar.py:260, log())
[2021-11-14 18:11:24]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:11:24]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 18:12:21]    INFO >> epoch 002:   6897 / 10106 loss=6.069, nll_loss=3.973, ppl=15.7, wps=3779.8, ups=5.66, wpb=667.6, bsz=4, num_updates=17000, lr=4.9e-05, gnorm=10.515, clip=0.2, loss_scale=16, train_wall=87, gb_free=28.2, wall=4597 (progress_bar.py:260, log())
[2021-11-14 18:13:50]    INFO >> epoch 002:   7397 / 10106 loss=5.939, nll_loss=3.906, ppl=14.99, wps=3751.1, ups=5.6, wpb=670.1, bsz=4, num_updates=17500, lr=4.9e-05, gnorm=10.289, clip=0.4, loss_scale=16, train_wall=88, gb_free=28.6, wall=4686 (progress_bar.py:260, log())
[2021-11-14 18:14:11]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:14:11]    INFO >> AMP: overflow detected, setting scale to to 4.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:15:47]    INFO >> epoch 002:   7897 / 10106 loss=5.783, nll_loss=3.805, ppl=13.97, wps=2954.4, ups=4.28, wpb=689.5, bsz=4, num_updates=18000, lr=4.9e-05, gnorm=9.912, clip=0.2, loss_scale=4, train_wall=115, gb_free=29.3, wall=4803 (progress_bar.py:260, log())
[2021-11-14 18:18:06]    INFO >> epoch 002:   8397 / 10106 loss=5.496, nll_loss=3.571, ppl=11.89, wps=2431.5, ups=3.59, wpb=677.2, bsz=4, num_updates=18500, lr=4.9e-05, gnorm=9.218, clip=0, loss_scale=4, train_wall=138, gb_free=29, wall=4942 (progress_bar.py:260, log())
[2021-11-14 18:20:26]    INFO >> epoch 002:   8897 / 10106 loss=5.434, nll_loss=3.568, ppl=11.86, wps=2343.1, ups=3.58, wpb=653.9, bsz=4, num_updates=19000, lr=4.9e-05, gnorm=9.155, clip=0, loss_scale=4, train_wall=138, gb_free=27.4, wall=5081 (progress_bar.py:260, log())
[2021-11-14 18:22:48]    INFO >> epoch 002:   9397 / 10106 loss=5.306, nll_loss=3.485, ppl=11.2, wps=2507.3, ups=3.52, wpb=712.6, bsz=4, num_updates=19500, lr=4.9e-05, gnorm=8.563, clip=0, loss_scale=4, train_wall=141, gb_free=28.7, wall=5224 (progress_bar.py:260, log())
[2021-11-14 18:24:23]    INFO >> epoch 002:   9897 / 10106 loss=5.141, nll_loss=3.341, ppl=10.13, wps=3523.7, ups=5.23, wpb=673.6, bsz=4, num_updates=20000, lr=4.9e-05, gnorm=8.376, clip=0, loss_scale=8, train_wall=95, gb_free=28.7, wall=5319 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 18:25:16]    INFO >> epoch 002 | loss 6.9 | nll_loss 4.557 | ppl 23.54 | wps 2288.3 | ups 3.38 | wpb 677.7 | bsz 4 | num_updates 20209 | lr 4.9e-05 | gnorm 12.086 | clip 0.7 | loss_scale 8 | train_wall 2316 | gb_free 26.7 | wall 5372 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 18:28:24]    INFO >> epoch 002 | valid on 'valid' subset | loss 4.682 | nll_loss 2.875 | ppl 7.34 | bleu 3.30304 | wps 930.4 | wpb 3153 | bsz 16 | num_updates 20209 | best_bleu 3.30304 (progress_bar.py:269, print())
[2021-11-14 18:28:36]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_best.pt (epoch 2 @ 20209 updates, score 3.303042) (writing took 11.664832 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 18:29:45]    INFO >> epoch 003:    291 / 10106 loss=5.1, nll_loss=3.325, ppl=10.02, wps=1084.9, ups=1.56, wpb=697.6, bsz=4, num_updates=20500, lr=4.9e-05, gnorm=8.513, clip=0, loss_scale=8, train_wall=99, gb_free=26.7, wall=5641 (progress_bar.py:260, log())
[2021-11-14 18:31:13]    INFO >> epoch 003:    791 / 10106 loss=4.973, nll_loss=3.208, ppl=9.24, wps=3729, ups=5.65, wpb=660.1, bsz=4, num_updates=21000, lr=4.9e-05, gnorm=8.414, clip=0, loss_scale=8, train_wall=87, gb_free=27.3, wall=5729 (progress_bar.py:260, log())
[2021-11-14 18:32:46]    INFO >> epoch 003:   1291 / 10106 loss=4.921, nll_loss=3.176, ppl=9.04, wps=3747.8, ups=5.4, wpb=693.6, bsz=4, num_updates=21500, lr=4.9e-05, gnorm=8.11, clip=0, loss_scale=8, train_wall=91, gb_free=29.3, wall=5822 (progress_bar.py:260, log())
[2021-11-14 18:34:20]    INFO >> epoch 003:   1791 / 10106 loss=4.853, nll_loss=3.116, ppl=8.67, wps=3648.8, ups=5.31, wpb=687.5, bsz=4, num_updates=22000, lr=4.9e-05, gnorm=8.132, clip=0, loss_scale=16, train_wall=93, gb_free=28.9, wall=5916 (progress_bar.py:260, log())
[2021-11-14 18:34:55]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:34:55]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 18:35:37]    INFO >> epoch 003:   2292 / 10106 loss=4.836, nll_loss=3.107, ppl=8.62, wps=4506.4, ups=6.52, wpb=690.7, bsz=4, num_updates=22500, lr=4.9e-05, gnorm=8.354, clip=0.4, loss_scale=8, train_wall=76, gb_free=29.4, wall=5993 (progress_bar.py:260, log())
[2021-11-14 18:36:45]    INFO >> epoch 003:   2792 / 10106 loss=4.777, nll_loss=3.058, ppl=8.33, wps=4856.7, ups=7.37, wpb=659.4, bsz=4, num_updates=23000, lr=4.9e-05, gnorm=8.136, clip=0, loss_scale=8, train_wall=67, gb_free=29, wall=6060 (progress_bar.py:260, log())
[2021-11-14 18:37:55]    INFO >> epoch 003:   3292 / 10106 loss=4.774, nll_loss=3.064, ppl=8.36, wps=4873.4, ups=7.12, wpb=684.5, bsz=4, num_updates=23500, lr=4.9e-05, gnorm=8.21, clip=0.4, loss_scale=8, train_wall=69, gb_free=29.1, wall=6131 (progress_bar.py:260, log())
[2021-11-14 18:39:06]    INFO >> epoch 003:   3792 / 10106 loss=4.688, nll_loss=2.976, ppl=7.87, wps=4768.8, ups=7, wpb=681.7, bsz=4, num_updates=24000, lr=4.9e-05, gnorm=8.004, clip=0.2, loss_scale=8, train_wall=70, gb_free=27.2, wall=6202 (progress_bar.py:260, log())
[2021-11-14 18:40:11]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:40:16]    INFO >> epoch 003:   4292 / 10106 loss=4.69, nll_loss=2.989, ppl=7.94, wps=4833.3, ups=7.17, wpb=673.7, bsz=4, num_updates=24500, lr=4.9e-05, gnorm=7.991, clip=0, loss_scale=8, train_wall=69, gb_free=28.8, wall=6272 (progress_bar.py:260, log())
[2021-11-14 18:42:26]    INFO >> epoch 003:   4792 / 10106 loss=4.663, nll_loss=2.965, ppl=7.81, wps=2643.2, ups=3.84, wpb=688.3, bsz=4, num_updates=25000, lr=4.9e-05, gnorm=8.064, clip=0.2, loss_scale=8, train_wall=129, gb_free=29.1, wall=6402 (progress_bar.py:260, log())
[2021-11-14 18:44:49]    INFO >> epoch 003:   5292 / 10106 loss=4.649, nll_loss=2.956, ppl=7.76, wps=2476.7, ups=3.51, wpb=706.5, bsz=4, num_updates=25500, lr=4.9e-05, gnorm=7.891, clip=0.2, loss_scale=8, train_wall=141, gb_free=28.8, wall=6545 (progress_bar.py:260, log())
[2021-11-14 18:47:10]    INFO >> epoch 003:   5792 / 10106 loss=4.596, nll_loss=2.9, ppl=7.46, wps=2507.9, ups=3.54, wpb=709, bsz=4, num_updates=26000, lr=4.9e-05, gnorm=7.814, clip=0.4, loss_scale=8, train_wall=140, gb_free=28.9, wall=6686 (progress_bar.py:260, log())
[2021-11-14 18:49:16]    INFO >> epoch 003:   6292 / 10106 loss=4.525, nll_loss=2.823, ppl=7.08, wps=2486.7, ups=3.97, wpb=626.3, bsz=4, num_updates=26500, lr=4.9e-05, gnorm=7.753, clip=0, loss_scale=16, train_wall=125, gb_free=27.6, wall=6812 (progress_bar.py:260, log())
[2021-11-14 18:49:57]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:49:58]    INFO >> AMP: overflow detected, setting scale to to 4.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:49:58]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 18:50:47]    INFO >> epoch 003:   6793 / 10106 loss=4.527, nll_loss=2.831, ppl=7.12, wps=3758.8, ups=5.49, wpb=685.2, bsz=4, num_updates=27000, lr=4.9e-05, gnorm=7.647, clip=0, loss_scale=4, train_wall=90, gb_free=28.6, wall=6903 (progress_bar.py:260, log())
[2021-11-14 18:52:20]    INFO >> epoch 003:   7293 / 10106 loss=4.539, nll_loss=2.851, ppl=7.21, wps=3701.2, ups=5.37, wpb=688.6, bsz=4, num_updates=27500, lr=4.9e-05, gnorm=7.563, clip=0, loss_scale=4, train_wall=92, gb_free=28.8, wall=6996 (progress_bar.py:260, log())
[2021-11-14 18:53:51]    INFO >> epoch 003:   7793 / 10106 loss=4.498, nll_loss=2.806, ppl=7, wps=3729.1, ups=5.5, wpb=678.3, bsz=4, num_updates=28000, lr=4.9e-05, gnorm=7.409, clip=0, loss_scale=4, train_wall=90, gb_free=28.7, wall=7087 (progress_bar.py:260, log())
[2021-11-14 18:56:08]    INFO >> epoch 003:   8293 / 10106 loss=4.432, nll_loss=2.734, ppl=6.65, wps=2416.1, ups=3.65, wpb=662.2, bsz=4, num_updates=28500, lr=4.9e-05, gnorm=7.477, clip=0, loss_scale=4, train_wall=136, gb_free=28.6, wall=7224 (progress_bar.py:260, log())
[2021-11-14 18:58:25]    INFO >> epoch 003:   8793 / 10106 loss=4.462, nll_loss=2.773, ppl=6.83, wps=2397, ups=3.65, wpb=657, bsz=4, num_updates=29000, lr=4.9e-05, gnorm=7.221, clip=0, loss_scale=8, train_wall=136, gb_free=28.9, wall=7361 (progress_bar.py:260, log())
[2021-11-14 19:00:48]    INFO >> epoch 003:   9293 / 10106 loss=4.459, nll_loss=2.772, ppl=6.83, wps=2431, ups=3.5, wpb=695.3, bsz=4, num_updates=29500, lr=4.9e-05, gnorm=7.268, clip=0, loss_scale=8, train_wall=142, gb_free=29.3, wall=7504 (progress_bar.py:260, log())
[2021-11-14 19:02:52]    INFO >> epoch 003:   9793 / 10106 loss=4.454, nll_loss=2.769, ppl=6.82, wps=2697.6, ups=4.04, wpb=668.4, bsz=4, num_updates=30000, lr=4.9e-05, gnorm=7.173, clip=0, loss_scale=8, train_wall=123, gb_free=29.2, wall=7628 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 19:04:02]    INFO >> epoch 003 | loss 4.655 | nll_loss 2.946 | ppl 7.71 | wps 2944.1 | ups 4.34 | wpb 677.7 | bsz 4 | num_updates 30313 | lr 4.9e-05 | gnorm 7.821 | clip 0.1 | loss_scale 8 | train_wall 2081 | gb_free 29.3 | wall 7698 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 19:17:00]    INFO >> epoch 003 | valid on 'valid' subset | loss 4.311 | nll_loss 2.591 | ppl 6.03 | bleu 10.4661 | wps 217.4 | wpb 3153 | bsz 16 | num_updates 30313 | best_bleu 10.4661 (progress_bar.py:269, print())
[2021-11-14 19:17:12]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_best.pt (epoch 3 @ 30313 updates, score 10.466067) (writing took 11.743948 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 19:17:54]    INFO >> epoch 004:    187 / 10106 loss=4.389, nll_loss=2.694, ppl=6.47, wps=356.3, ups=0.55, wpb=642.4, bsz=4, num_updates=30500, lr=4.9e-05, gnorm=7.229, clip=0.2, loss_scale=8, train_wall=88, gb_free=29.3, wall=8530 (progress_bar.py:260, log())
[2021-11-14 19:18:50]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 19:18:50]    INFO >> AMP: overflow detected, setting scale to to 4.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 19:19:24]    INFO >> epoch 004:    687 / 10106 loss=4.343, nll_loss=2.646, ppl=6.26, wps=3820.2, ups=5.53, wpb=691.2, bsz=4, num_updates=31000, lr=4.9e-05, gnorm=6.975, clip=0.4, loss_scale=4, train_wall=89, gb_free=29.4, wall=8620 (progress_bar.py:260, log())
[2021-11-14 19:21:28]    INFO >> epoch 004:   1187 / 10106 loss=4.334, nll_loss=2.636, ppl=6.21, wps=2684.2, ups=4.03, wpb=665.3, bsz=4, num_updates=31500, lr=4.8e-05, gnorm=6.893, clip=0.2, loss_scale=4, train_wall=123, gb_free=29.3, wall=8744 (progress_bar.py:260, log())
[2021-11-14 19:22:58]    INFO >> epoch 004:   1687 / 10106 loss=4.371, nll_loss=2.678, ppl=6.4, wps=3703.2, ups=5.55, wpb=667.5, bsz=4, num_updates=32000, lr=4.8e-05, gnorm=6.868, clip=0.2, loss_scale=4, train_wall=89, gb_free=29.3, wall=8834 (progress_bar.py:260, log())
[2021-11-14 19:24:30]    INFO >> epoch 004:   2187 / 10106 loss=4.341, nll_loss=2.645, ppl=6.26, wps=3682.3, ups=5.44, wpb=676.4, bsz=4, num_updates=32500, lr=4.8e-05, gnorm=6.862, clip=0, loss_scale=4, train_wall=91, gb_free=27.7, wall=8926 (progress_bar.py:260, log())
[2021-11-14 19:25:59]    INFO >> epoch 004:   2687 / 10106 loss=4.347, nll_loss=2.655, ppl=6.3, wps=3683.1, ups=5.62, wpb=655.2, bsz=4, num_updates=33000, lr=4.8e-05, gnorm=6.695, clip=0, loss_scale=8, train_wall=88, gb_free=28.5, wall=9015 (progress_bar.py:260, log())
[2021-11-14 19:27:23]    INFO >> epoch 004:   3187 / 10106 loss=4.306, nll_loss=2.61, ppl=6.1, wps=4063.9, ups=5.96, wpb=681.3, bsz=4, num_updates=33500, lr=4.8e-05, gnorm=6.502, clip=0, loss_scale=8, train_wall=83, gb_free=29.3, wall=9099 (progress_bar.py:260, log())
[2021-11-14 19:28:34]    INFO >> epoch 004:   3687 / 10106 loss=4.231, nll_loss=2.525, ppl=5.76, wps=4773.4, ups=7.09, wpb=673.6, bsz=4, num_updates=34000, lr=4.8e-05, gnorm=6.469, clip=0, loss_scale=8, train_wall=70, gb_free=29.3, wall=9169 (progress_bar.py:260, log())
[2021-11-14 19:28:36]    INFO >> AMP: overflow detected, setting scale to to 4.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 19:28:36]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 19:29:42]    INFO >> epoch 004:   4188 / 10106 loss=4.293, nll_loss=2.596, ppl=6.05, wps=4991.6, ups=7.32, wpb=682.2, bsz=4, num_updates=34500, lr=4.8e-05, gnorm=6.493, clip=0.2, loss_scale=4, train_wall=67, gb_free=29.3, wall=9238 (progress_bar.py:260, log())
[2021-11-14 19:30:47]    INFO >> epoch 004:   4688 / 10106 loss=4.323, nll_loss=2.633, ppl=6.2, wps=5283.5, ups=7.68, wpb=688.3, bsz=4, num_updates=35000, lr=4.8e-05, gnorm=6.416, clip=0, loss_scale=4, train_wall=64, gb_free=28.1, wall=9303 (progress_bar.py:260, log())
[2021-11-14 19:31:54]    INFO >> epoch 004:   5188 / 10106 loss=4.285, nll_loss=2.589, ppl=6.02, wps=5018.9, ups=7.5, wpb=669.1, bsz=4, num_updates=35500, lr=4.8e-05, gnorm=6.339, clip=0, loss_scale=4, train_wall=66, gb_free=27.7, wall=9370 (progress_bar.py:260, log())
[2021-11-14 19:33:31]    INFO >> epoch 004:   5688 / 10106 loss=4.255, nll_loss=2.556, ppl=5.88, wps=3492.5, ups=5.13, wpb=680.9, bsz=4, num_updates=36000, lr=4.8e-05, gnorm=6.221, clip=0.2, loss_scale=4, train_wall=96, gb_free=28.4, wall=9467 (progress_bar.py:260, log())
[2021-11-14 19:35:52]    INFO >> epoch 004:   6188 / 10106 loss=4.249, nll_loss=2.551, ppl=5.86, wps=2431.6, ups=3.56, wpb=683.4, bsz=4, num_updates=36500, lr=4.8e-05, gnorm=6.167, clip=0, loss_scale=8, train_wall=139, gb_free=27.5, wall=9608 (progress_bar.py:260, log())
[2021-11-14 19:38:14]    INFO >> epoch 004:   6688 / 10106 loss=4.221, nll_loss=2.519, ppl=5.73, wps=2402.9, ups=3.51, wpb=684.2, bsz=4, num_updates=37000, lr=4.8e-05, gnorm=6.039, clip=0, loss_scale=8, train_wall=141, gb_free=27.1, wall=9750 (progress_bar.py:260, log())
[2021-11-14 19:40:33]    INFO >> epoch 004:   7188 / 10106 loss=4.239, nll_loss=2.539, ppl=5.81, wps=2401, ups=3.6, wpb=667.8, bsz=4, num_updates=37500, lr=4.8e-05, gnorm=6.091, clip=0, loss_scale=8, train_wall=138, gb_free=28, wall=9889 (progress_bar.py:260, log())
[2021-11-14 19:42:18]    INFO >> epoch 004:   7688 / 10106 loss=4.222, nll_loss=2.524, ppl=5.75, wps=3279.1, ups=4.78, wpb=686, bsz=4, num_updates=38000, lr=4.8e-05, gnorm=6.02, clip=0, loss_scale=8, train_wall=104, gb_free=27.4, wall=9994 (progress_bar.py:260, log())
[2021-11-14 19:43:48]    INFO >> epoch 004:   8188 / 10106 loss=4.186, nll_loss=2.482, ppl=5.59, wps=3714.1, ups=5.54, wpb=670.2, bsz=4, num_updates=38500, lr=4.8e-05, gnorm=5.956, clip=0, loss_scale=16, train_wall=89, gb_free=29.1, wall=10084 (progress_bar.py:260, log())
[2021-11-14 19:45:20]    INFO >> epoch 004:   8688 / 10106 loss=4.172, nll_loss=2.465, ppl=5.52, wps=3728, ups=5.45, wpb=684, bsz=4, num_updates=39000, lr=4.8e-05, gnorm=5.796, clip=0, loss_scale=16, train_wall=91, gb_free=29.2, wall=10176 (progress_bar.py:260, log())
[2021-11-14 19:47:21]    INFO >> epoch 004:   9188 / 10106 loss=4.198, nll_loss=2.496, ppl=5.64, wps=2897.9, ups=4.12, wpb=703.4, bsz=4, num_updates=39500, lr=4.8e-05, gnorm=5.732, clip=0, loss_scale=16, train_wall=120, gb_free=29.3, wall=10297 (progress_bar.py:260, log())
[2021-11-14 19:47:56]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 19:49:37]    INFO >> epoch 004:   9688 / 10106 loss=4.185, nll_loss=2.478, ppl=5.57, wps=2447.9, ups=3.68, wpb=664.4, bsz=4, num_updates=40000, lr=4.8e-05, gnorm=5.975, clip=0.2, loss_scale=8, train_wall=134, gb_free=29.3, wall=10433 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 19:51:50]    INFO >> epoch 004 | loss 4.266 | nll_loss 2.567 | ppl 5.92 | wps 2388.4 | ups 3.52 | wpb 677.8 | bsz 4 | num_updates 40418 | lr 4.8e-05 | gnorm 6.337 | clip 0.1 | loss_scale 8 | train_wall 2032 | gb_free 28.8 | wall 10566 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 20:02:47]    INFO >> epoch 004 | valid on 'valid' subset | loss 4.126 | nll_loss 2.39 | ppl 5.24 | bleu 9.29998 | wps 256.1 | wpb 3153 | bsz 16 | num_updates 40418 | best_bleu 10.4661 (progress_bar.py:269, print())
[2021-11-14 20:02:55]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_last.pt (epoch 4 @ 40418 updates, score 9.299983) (writing took 7.489393 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 20:03:27]    INFO >> epoch 005:     82 / 10106 loss=4.182, nll_loss=2.478, ppl=5.57, wps=414.4, ups=0.6, wpb=688, bsz=4, num_updates=40500, lr=4.8e-05, gnorm=5.695, clip=0, loss_scale=8, train_wall=141, gb_free=28.9, wall=11263 (progress_bar.py:260, log())
[2021-11-14 20:05:48]    INFO >> epoch 005:    582 / 10106 loss=4.082, nll_loss=2.362, ppl=5.14, wps=2447.6, ups=3.53, wpb=692.7, bsz=4, num_updates=41000, lr=4.8e-05, gnorm=5.646, clip=0.2, loss_scale=8, train_wall=140, gb_free=26.5, wall=11404 (progress_bar.py:260, log())
[2021-11-14 20:07:46]    INFO >> epoch 005:   1082 / 10106 loss=4.157, nll_loss=2.449, ppl=5.46, wps=2976.7, ups=4.25, wpb=701.1, bsz=4, num_updates=41500, lr=4.8e-05, gnorm=5.429, clip=0, loss_scale=8, train_wall=117, gb_free=28.8, wall=11522 (progress_bar.py:260, log())
[2021-11-14 20:09:17]    INFO >> epoch 005:   1582 / 10106 loss=4.105, nll_loss=2.389, ppl=5.24, wps=3828, ups=5.53, wpb=691.9, bsz=4, num_updates=42000, lr=4.8e-05, gnorm=5.331, clip=0, loss_scale=16, train_wall=89, gb_free=25.8, wall=11612 (progress_bar.py:260, log())
[2021-11-14 20:10:49]    INFO >> epoch 005:   2082 / 10106 loss=4.092, nll_loss=2.374, ppl=5.19, wps=3797.8, ups=5.42, wpb=701.3, bsz=4, num_updates=42500, lr=4.8e-05, gnorm=5.273, clip=0, loss_scale=16, train_wall=91, gb_free=29.1, wall=11705 (progress_bar.py:260, log())
[2021-11-14 20:12:29]    INFO >> epoch 005:   2582 / 10106 loss=4.03, nll_loss=2.304, ppl=4.94, wps=3275.2, ups=5, wpb=655.6, bsz=4, num_updates=43000, lr=4.8e-05, gnorm=5.34, clip=0, loss_scale=16, train_wall=99, gb_free=28.4, wall=11805 (progress_bar.py:260, log())
[2021-11-14 20:13:54]    INFO >> epoch 005:   3082 / 10106 loss=4.047, nll_loss=2.325, ppl=5.01, wps=3742, ups=5.86, wpb=638.3, bsz=4, num_updates=43500, lr=4.8e-05, gnorm=5.302, clip=0, loss_scale=16, train_wall=84, gb_free=29.3, wall=11890 (progress_bar.py:260, log())
[2021-11-14 20:14:12]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:15:13]    INFO >> epoch 005:   3582 / 10106 loss=4.062, nll_loss=2.342, ppl=5.07, wps=4255.4, ups=6.37, wpb=668.2, bsz=4, num_updates=44000, lr=4.8e-05, gnorm=5.412, clip=0.2, loss_scale=8, train_wall=77, gb_free=27.7, wall=11969 (progress_bar.py:260, log())
[2021-11-14 20:16:24]    INFO >> epoch 005:   4082 / 10106 loss=4.07, nll_loss=2.353, ppl=5.11, wps=4824.6, ups=7.04, wpb=685.4, bsz=4, num_updates=44500, lr=4.8e-05, gnorm=5.215, clip=0, loss_scale=8, train_wall=70, gb_free=29, wall=12040 (progress_bar.py:260, log())
[2021-11-14 20:17:34]    INFO >> epoch 005:   4582 / 10106 loss=4.068, nll_loss=2.35, ppl=5.1, wps=4890.7, ups=7.15, wpb=684.1, bsz=4, num_updates=45000, lr=4.8e-05, gnorm=5.078, clip=0, loss_scale=8, train_wall=69, gb_free=28.6, wall=12110 (progress_bar.py:260, log())
[2021-11-14 20:18:42]    INFO >> epoch 005:   5082 / 10106 loss=4.063, nll_loss=2.344, ppl=5.08, wps=4865.1, ups=7.37, wpb=660, bsz=4, num_updates=45500, lr=4.8e-05, gnorm=5.12, clip=0, loss_scale=8, train_wall=67, gb_free=29.4, wall=12177 (progress_bar.py:260, log())
[2021-11-14 20:20:02]    INFO >> epoch 005:   5582 / 10106 loss=4.089, nll_loss=2.375, ppl=5.19, wps=4279.2, ups=6.24, wpb=685.9, bsz=4, num_updates=46000, lr=4.8e-05, gnorm=5.039, clip=0, loss_scale=16, train_wall=79, gb_free=28.9, wall=12258 (progress_bar.py:260, log())
[2021-11-14 20:21:37]    INFO >> epoch 005:   6082 / 10106 loss=4.062, nll_loss=2.344, ppl=5.08, wps=3586.9, ups=5.24, wpb=685.2, bsz=4, num_updates=46500, lr=4.8e-05, gnorm=4.994, clip=0, loss_scale=16, train_wall=94, gb_free=28.6, wall=12353 (progress_bar.py:260, log())
[2021-11-14 20:23:14]    INFO >> epoch 005:   6582 / 10106 loss=4, nll_loss=2.272, ppl=4.83, wps=3470.9, ups=5.19, wpb=669.3, bsz=4, num_updates=47000, lr=4.8e-05, gnorm=5.015, clip=0, loss_scale=16, train_wall=95, gb_free=29.4, wall=12450 (progress_bar.py:260, log())
[2021-11-14 20:24:51]    INFO >> epoch 005:   7082 / 10106 loss=3.968, nll_loss=2.236, ppl=4.71, wps=3423.3, ups=5.14, wpb=666.1, bsz=4, num_updates=47500, lr=4.8e-05, gnorm=4.915, clip=0, loss_scale=16, train_wall=96, gb_free=29.2, wall=12547 (progress_bar.py:260, log())
[2021-11-14 20:25:54]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:25:54]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 20:26:29]    INFO >> epoch 005:   7583 / 10106 loss=4.043, nll_loss=2.321, ppl=5, wps=3506.4, ups=5.09, wpb=689, bsz=4, num_updates=48000, lr=4.8e-05, gnorm=4.878, clip=0, loss_scale=16, train_wall=97, gb_free=29.4, wall=12645 (progress_bar.py:260, log())
[2021-11-14 20:28:06]    INFO >> epoch 005:   8083 / 10106 loss=4.005, nll_loss=2.278, ppl=4.85, wps=3414.8, ups=5.18, wpb=658.7, bsz=4, num_updates=48500, lr=4.8e-05, gnorm=4.857, clip=0, loss_scale=16, train_wall=95, gb_free=26.1, wall=12742 (progress_bar.py:260, log())
[2021-11-14 20:29:43]    INFO >> epoch 005:   8583 / 10106 loss=4.016, nll_loss=2.29, ppl=4.89, wps=3383, ups=5.14, wpb=658.8, bsz=4, num_updates=49000, lr=4.8e-05, gnorm=4.904, clip=0, loss_scale=16, train_wall=96, gb_free=29.3, wall=12839 (progress_bar.py:260, log())
[2021-11-14 20:31:24]    INFO >> epoch 005:   9083 / 10106 loss=3.994, nll_loss=2.265, ppl=4.81, wps=3455.7, ups=4.95, wpb=698.1, bsz=4, num_updates=49500, lr=4.8e-05, gnorm=4.756, clip=0, loss_scale=16, train_wall=100, gb_free=29.3, wall=12940 (progress_bar.py:260, log())
[2021-11-14 20:33:04]    INFO >> epoch 005:   9583 / 10106 loss=3.967, nll_loss=2.235, ppl=4.71, wps=3354.5, ups=5.01, wpb=669.2, bsz=4, num_updates=50000, lr=4.8e-05, gnorm=4.739, clip=0, loss_scale=32, train_wall=99, gb_free=27.5, wall=13040 (progress_bar.py:260, log())
[2021-11-14 20:34:05]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:34:31]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:34:31]    INFO >> AMP: overflow detected, setting scale to to 4.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:34:31]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 20:34:45]    INFO >> epoch 005:  10084 / 10106 loss=3.985, nll_loss=2.255, ppl=4.77, wps=3458.2, ups=4.95, wpb=698.1, bsz=4, num_updates=50500, lr=4.8e-05, gnorm=4.654, clip=0, loss_scale=4, train_wall=99, gb_free=27.9, wall=13141 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 20:35:04]    INFO >> epoch 005 | loss 4.047 | nll_loss 2.325 | ppl 5.01 | wps 2640.3 | ups 3.9 | wpb 677.8 | bsz 4 | num_updates 50522 | lr 4.8e-05 | gnorm 5.099 | clip 0 | loss_scale 4 | train_wall 1884 | gb_free 29.4 | wall 13159 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 20:45:02]    INFO >> epoch 005 | valid on 'valid' subset | loss 4.032 | nll_loss 2.317 | ppl 4.98 | bleu 12.6719 | wps 285.4 | wpb 3153 | bsz 16 | num_updates 50522 | best_bleu 12.6719 (progress_bar.py:269, print())
[2021-11-14 20:45:14]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_best.pt (epoch 5 @ 50522 updates, score 12.671929) (writing took 11.610056 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 20:46:53]    INFO >> epoch 006:    478 / 10106 loss=3.971, nll_loss=2.238, ppl=4.72, wps=454.9, ups=0.69, wpb=662.5, bsz=4, num_updates=51000, lr=4.8e-05, gnorm=4.659, clip=0, loss_scale=4, train_wall=95, gb_free=28.6, wall=13869 (progress_bar.py:260, log())
[2021-11-14 20:48:24]    INFO >> epoch 006:    978 / 10106 loss=3.913, nll_loss=2.172, ppl=4.51, wps=3642.8, ups=5.52, wpb=659.7, bsz=4, num_updates=51500, lr=4.7e-05, gnorm=4.609, clip=0, loss_scale=4, train_wall=89, gb_free=28, wall=13959 (progress_bar.py:260, log())
[2021-11-14 20:49:32]    INFO >> epoch 006:   1478 / 10106 loss=3.928, nll_loss=2.189, ppl=4.56, wps=5091.1, ups=7.34, wpb=693.6, bsz=4, num_updates=52000, lr=4.7e-05, gnorm=4.568, clip=0.2, loss_scale=4, train_wall=67, gb_free=27.6, wall=14027 (progress_bar.py:260, log())
[2021-11-14 20:50:40]    INFO >> epoch 006:   1978 / 10106 loss=4.001, nll_loss=2.269, ppl=4.82, wps=5099.1, ups=7.3, wpb=698.3, bsz=4, num_updates=52500, lr=4.7e-05, gnorm=4.582, clip=0.2, loss_scale=8, train_wall=67, gb_free=29.3, wall=14096 (progress_bar.py:260, log())
[2021-11-14 20:51:47]    INFO >> epoch 006:   2478 / 10106 loss=3.873, nll_loss=2.127, ppl=4.37, wps=5120.5, ups=7.42, wpb=689.8, bsz=4, num_updates=53000, lr=4.7e-05, gnorm=4.381, clip=0, loss_scale=8, train_wall=66, gb_free=27.7, wall=14163 (progress_bar.py:260, log())
[2021-11-14 20:52:56]    INFO >> epoch 006:   2978 / 10106 loss=3.958, nll_loss=2.226, ppl=4.68, wps=4961.6, ups=7.33, wpb=676.7, bsz=4, num_updates=53500, lr=4.7e-05, gnorm=4.467, clip=0.2, loss_scale=8, train_wall=67, gb_free=28, wall=14231 (progress_bar.py:260, log())
[2021-11-14 20:54:03]    INFO >> epoch 006:   3478 / 10106 loss=3.929, nll_loss=2.192, ppl=4.57, wps=5022.1, ups=7.4, wpb=678.8, bsz=4, num_updates=54000, lr=4.7e-05, gnorm=4.404, clip=0, loss_scale=8, train_wall=67, gb_free=29.1, wall=14299 (progress_bar.py:260, log())
[2021-11-14 20:55:30]    INFO >> epoch 006:   3978 / 10106 loss=3.88, nll_loss=2.133, ppl=4.39, wps=3776, ups=5.78, wpb=653.8, bsz=4, num_updates=54500, lr=4.7e-05, gnorm=4.342, clip=0, loss_scale=16, train_wall=86, gb_free=28.1, wall=14386 (progress_bar.py:260, log())
[2021-11-14 20:55:34]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:57:08]    INFO >> epoch 006:   4478 / 10106 loss=3.887, nll_loss=2.145, ppl=4.42, wps=3412, ups=5.09, wpb=670.7, bsz=4, num_updates=55000, lr=4.7e-05, gnorm=4.319, clip=0, loss_scale=8, train_wall=97, gb_free=27.5, wall=14484 (progress_bar.py:260, log())
[2021-11-14 20:58:45]    INFO >> epoch 006:   4978 / 10106 loss=3.909, nll_loss=2.169, ppl=4.5, wps=3537, ups=5.17, wpb=683.7, bsz=4, num_updates=55500, lr=4.7e-05, gnorm=4.266, clip=0, loss_scale=8, train_wall=96, gb_free=28.2, wall=14581 (progress_bar.py:260, log())
[2021-11-14 21:00:22]    INFO >> epoch 006:   5478 / 10106 loss=3.867, nll_loss=2.122, ppl=4.35, wps=3509.4, ups=5.15, wpb=680.9, bsz=4, num_updates=56000, lr=4.7e-05, gnorm=4.237, clip=0, loss_scale=8, train_wall=96, gb_free=29.2, wall=14678 (progress_bar.py:260, log())
[2021-11-14 21:02:01]    INFO >> epoch 006:   5978 / 10106 loss=3.898, nll_loss=2.156, ppl=4.46, wps=3419.4, ups=5.05, wpb=676.7, bsz=4, num_updates=56500, lr=4.7e-05, gnorm=4.231, clip=0, loss_scale=8, train_wall=98, gb_free=28.9, wall=14777 (progress_bar.py:260, log())
[2021-11-14 21:02:38]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 21:03:39]    INFO >> epoch 006:   6478 / 10106 loss=3.892, nll_loss=2.15, ppl=4.44, wps=3583.9, ups=5.09, wpb=704.1, bsz=4, num_updates=57000, lr=4.7e-05, gnorm=4.195, clip=0.4, loss_scale=8, train_wall=97, gb_free=27.4, wall=14875 (progress_bar.py:260, log())
[2021-11-14 21:05:17]    INFO >> epoch 006:   6978 / 10106 loss=3.819, nll_loss=2.067, ppl=4.19, wps=3612.8, ups=5.12, wpb=705.9, bsz=4, num_updates=57500, lr=4.7e-05, gnorm=3.982, clip=0, loss_scale=8, train_wall=97, gb_free=26.9, wall=14972 (progress_bar.py:260, log())
[2021-11-14 21:06:54]    INFO >> epoch 006:   7478 / 10106 loss=3.92, nll_loss=2.181, ppl=4.53, wps=3610.2, ups=5.16, wpb=700, bsz=4, num_updates=58000, lr=4.7e-05, gnorm=4.189, clip=0, loss_scale=8, train_wall=96, gb_free=29.3, wall=15069 (progress_bar.py:260, log())
[2021-11-14 21:08:31]    INFO >> epoch 006:   7978 / 10106 loss=3.84, nll_loss=2.091, ppl=4.26, wps=3467.5, ups=5.11, wpb=678.2, bsz=4, num_updates=58500, lr=4.7e-05, gnorm=4.07, clip=0, loss_scale=8, train_wall=97, gb_free=28.7, wall=15167 (progress_bar.py:260, log())
[2021-11-14 21:10:08]    INFO >> epoch 006:   8478 / 10106 loss=3.85, nll_loss=2.102, ppl=4.29, wps=3420.1, ups=5.18, wpb=660.2, bsz=4, num_updates=59000, lr=4.7e-05, gnorm=4.043, clip=0, loss_scale=16, train_wall=96, gb_free=28.8, wall=15264 (progress_bar.py:260, log())
[2021-11-14 21:11:45]    INFO >> epoch 006:   8978 / 10106 loss=3.854, nll_loss=2.107, ppl=4.31, wps=3549.8, ups=5.15, wpb=689.6, bsz=4, num_updates=59500, lr=4.7e-05, gnorm=3.982, clip=0, loss_scale=16, train_wall=96, gb_free=28.9, wall=15361 (progress_bar.py:260, log())
[2021-11-14 21:13:20]    INFO >> epoch 006:   9478 / 10106 loss=3.805, nll_loss=2.051, ppl=4.14, wps=3422.6, ups=5.25, wpb=651.6, bsz=4, num_updates=60000, lr=4.7e-05, gnorm=3.992, clip=0, loss_scale=16, train_wall=94, gb_free=29.3, wall=15456 (progress_bar.py:260, log())
[2021-11-14 21:14:57]    INFO >> epoch 006:   9978 / 10106 loss=3.793, nll_loss=2.038, ppl=4.11, wps=3249.6, ups=5.14, wpb=631.8, bsz=4, num_updates=60500, lr=4.7e-05, gnorm=3.996, clip=0, loss_scale=16, train_wall=96, gb_free=28.1, wall=15553 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 21:15:38]    INFO >> epoch 006 | loss 3.889 | nll_loss 2.146 | ppl 4.43 | wps 2814.5 | ups 4.15 | wpb 677.9 | bsz 4 | num_updates 60628 | lr 4.7e-05 | gnorm 4.269 | clip 0 | loss_scale 16 | train_wall 1780 | gb_free 27.6 | wall 15594 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 21:24:46]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.935 | nll_loss 2.181 | ppl 4.53 | bleu 13.1657 | wps 311 | wpb 3153 | bsz 16 | num_updates 60628 | best_bleu 13.1657 (progress_bar.py:269, print())
[2021-11-14 21:24:57]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_best.pt (epoch 6 @ 60628 updates, score 13.165702) (writing took 11.091365 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 21:25:54]    INFO >> epoch 007:    372 / 10106 loss=3.794, nll_loss=2.039, ppl=4.11, wps=515.9, ups=0.76, wpb=677.8, bsz=4, num_updates=61000, lr=4.7e-05, gnorm=3.8, clip=0, loss_scale=32, train_wall=74, gb_free=28.3, wall=16210 (progress_bar.py:260, log())
[2021-11-14 21:27:04]    INFO >> epoch 007:    872 / 10106 loss=3.79, nll_loss=2.033, ppl=4.09, wps=5048.6, ups=7.18, wpb=703.2, bsz=4, num_updates=61500, lr=4.7e-05, gnorm=3.793, clip=0, loss_scale=32, train_wall=69, gb_free=27.8, wall=16280 (progress_bar.py:260, log())
[2021-11-14 21:27:07]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 21:27:07]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 21:28:13]    INFO >> epoch 007:   1373 / 10106 loss=3.783, nll_loss=2.024, ppl=4.07, wps=5067.7, ups=7.29, wpb=695.4, bsz=4, num_updates=62000, lr=4.7e-05, gnorm=3.783, clip=0, loss_scale=16, train_wall=68, gb_free=26.7, wall=16348 (progress_bar.py:260, log())
[2021-11-14 21:29:21]    INFO >> epoch 007:   1873 / 10106 loss=3.776, nll_loss=2.016, ppl=4.04, wps=4979.7, ups=7.28, wpb=684.1, bsz=4, num_updates=62500, lr=4.7e-05, gnorm=3.827, clip=0, loss_scale=16, train_wall=68, gb_free=29.3, wall=16417 (progress_bar.py:260, log())
[2021-11-14 21:30:40]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 21:30:56]    INFO >> epoch 007:   2373 / 10106 loss=3.797, nll_loss=2.039, ppl=4.11, wps=3741, ups=5.3, wpb=705.3, bsz=4, num_updates=63000, lr=4.7e-05, gnorm=3.807, clip=0, loss_scale=8, train_wall=93, gb_free=26.9, wall=16511 (progress_bar.py:260, log())
[2021-11-14 21:32:32]    INFO >> epoch 007:   2873 / 10106 loss=3.788, nll_loss=2.03, ppl=4.08, wps=3604.5, ups=5.18, wpb=695.5, bsz=4, num_updates=63500, lr=4.7e-05, gnorm=3.813, clip=0.2, loss_scale=8, train_wall=95, gb_free=29.3, wall=16608 (progress_bar.py:260, log())
[2021-11-14 21:34:06]    INFO >> epoch 007:   3373 / 10106 loss=3.722, nll_loss=1.955, ppl=3.88, wps=3485.2, ups=5.29, wpb=658.5, bsz=4, num_updates=64000, lr=4.7e-05, gnorm=3.721, clip=0, loss_scale=8, train_wall=93, gb_free=29.1, wall=16702 (progress_bar.py:260, log())
[2021-11-14 21:35:45]    INFO >> epoch 007:   3873 / 10106 loss=3.785, nll_loss=2.025, ppl=4.07, wps=3463.6, ups=5.09, wpb=680.1, bsz=4, num_updates=64500, lr=4.7e-05, gnorm=3.763, clip=0, loss_scale=8, train_wall=97, gb_free=28.1, wall=16800 (progress_bar.py:260, log())
[2021-11-14 21:37:25]    INFO >> epoch 007:   4373 / 10106 loss=3.745, nll_loss=1.981, ppl=3.95, wps=3381.8, ups=5, wpb=676.1, bsz=4, num_updates=65000, lr=4.7e-05, gnorm=3.687, clip=0, loss_scale=16, train_wall=99, gb_free=28.7, wall=16900 (progress_bar.py:260, log())
[2021-11-14 21:39:01]    INFO >> epoch 007:   4873 / 10106 loss=3.751, nll_loss=1.988, ppl=3.97, wps=3459.7, ups=5.21, wpb=664.4, bsz=4, num_updates=65500, lr=4.7e-05, gnorm=3.685, clip=0, loss_scale=16, train_wall=95, gb_free=28.9, wall=16996 (progress_bar.py:260, log())
[2021-11-14 21:40:12]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 21:40:40]    INFO >> epoch 007:   5373 / 10106 loss=3.816, nll_loss=2.06, ppl=4.17, wps=3627, ups=5.05, wpb=717.5, bsz=4, num_updates=66000, lr=4.7e-05, gnorm=3.737, clip=0.2, loss_scale=8, train_wall=98, gb_free=29.2, wall=17095 (progress_bar.py:260, log())
[2021-11-14 21:42:16]    INFO >> epoch 007:   5873 / 10106 loss=3.738, nll_loss=1.974, ppl=3.93, wps=3553.3, ups=5.18, wpb=685.6, bsz=4, num_updates=66500, lr=4.7e-05, gnorm=3.594, clip=0, loss_scale=8, train_wall=95, gb_free=29.3, wall=17192 (progress_bar.py:260, log())
[2021-11-14 21:43:50]    INFO >> epoch 007:   6373 / 10106 loss=3.706, nll_loss=1.936, ppl=3.83, wps=3408.3, ups=5.32, wpb=641.1, bsz=4, num_updates=67000, lr=4.7e-05, gnorm=3.565, clip=0, loss_scale=8, train_wall=93, gb_free=28.7, wall=17286 (progress_bar.py:260, log())
[2021-11-14 21:45:24]    INFO >> epoch 007:   6873 / 10106 loss=3.75, nll_loss=1.988, ppl=3.97, wps=3402.4, ups=5.31, wpb=641, bsz=4, num_updates=67500, lr=4.7e-05, gnorm=3.602, clip=0, loss_scale=8, train_wall=93, gb_free=29.3, wall=17380 (progress_bar.py:260, log())
[2021-11-14 21:47:04]    INFO >> epoch 007:   7373 / 10106 loss=3.717, nll_loss=1.951, ppl=3.87, wps=3372.5, ups=5.03, wpb=671, bsz=4, num_updates=68000, lr=4.7e-05, gnorm=3.501, clip=0, loss_scale=16, train_wall=98, gb_free=28.7, wall=17480 (progress_bar.py:260, log())
[2021-11-14 21:48:40]    INFO >> epoch 007:   7873 / 10106 loss=3.694, nll_loss=1.923, ppl=3.79, wps=3385.9, ups=5.21, wpb=650.3, bsz=4, num_updates=68500, lr=4.7e-05, gnorm=3.58, clip=0, loss_scale=16, train_wall=95, gb_free=28.2, wall=17576 (progress_bar.py:260, log())
[2021-11-14 21:50:16]    INFO >> epoch 007:   8373 / 10106 loss=3.734, nll_loss=1.972, ppl=3.92, wps=3560.6, ups=5.19, wpb=685.5, bsz=4, num_updates=69000, lr=4.7e-05, gnorm=3.53, clip=0, loss_scale=16, train_wall=95, gb_free=28, wall=17672 (progress_bar.py:260, log())
[2021-11-14 21:51:53]    INFO >> epoch 007:   8873 / 10106 loss=3.775, nll_loss=2.015, ppl=4.04, wps=3494.9, ups=5.15, wpb=678.3, bsz=4, num_updates=69500, lr=4.7e-05, gnorm=3.525, clip=0, loss_scale=16, train_wall=96, gb_free=29, wall=17769 (progress_bar.py:260, log())
[2021-11-14 21:53:30]    INFO >> epoch 007:   9373 / 10106 loss=3.723, nll_loss=1.958, ppl=3.89, wps=3439.2, ups=5.14, wpb=669, bsz=4, num_updates=70000, lr=4.7e-05, gnorm=3.481, clip=0, loss_scale=32, train_wall=96, gb_free=27.3, wall=17866 (progress_bar.py:260, log())
[2021-11-14 21:55:08]    INFO >> epoch 007:   9873 / 10106 loss=3.662, nll_loss=1.889, ppl=3.7, wps=3426.8, ups=5.12, wpb=669.1, bsz=4, num_updates=70500, lr=4.7e-05, gnorm=3.374, clip=0, loss_scale=32, train_wall=97, gb_free=28.6, wall=17964 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 21:56:10]    INFO >> epoch 007 | loss 3.751 | nll_loss 1.988 | ppl 3.97 | wps 2816.5 | ups 4.16 | wpb 677.8 | bsz 4 | num_updates 70733 | lr 4.7e-05 | gnorm 3.65 | clip 0 | loss_scale 32 | train_wall 1829 | gb_free 29 | wall 18025 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 22:02:28]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.868 | nll_loss 2.087 | ppl 4.25 | bleu 17.8128 | wps 450.6 | wpb 3153 | bsz 16 | num_updates 70733 | best_bleu 17.8128 (progress_bar.py:269, print())
[2021-11-14 22:02:40]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_best.pt (epoch 7 @ 70733 updates, score 17.812781) (writing took 11.653469 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 22:03:25]    INFO >> epoch 008:    267 / 10106 loss=3.688, nll_loss=1.916, ppl=3.77, wps=696.9, ups=1.01, wpb=692.6, bsz=4, num_updates=71000, lr=4.7e-05, gnorm=3.343, clip=0, loss_scale=32, train_wall=83, gb_free=28.4, wall=18461 (progress_bar.py:260, log())
[2021-11-14 22:04:34]    INFO >> epoch 008:    767 / 10106 loss=3.671, nll_loss=1.896, ppl=3.72, wps=5088.5, ups=7.27, wpb=700.1, bsz=4, num_updates=71500, lr=4.6e-05, gnorm=3.278, clip=0, loss_scale=32, train_wall=68, gb_free=28.5, wall=18530 (progress_bar.py:260, log())
[2021-11-14 22:05:56]    INFO >> epoch 008:   1267 / 10106 loss=3.653, nll_loss=1.877, ppl=3.67, wps=4133.5, ups=6.1, wpb=677.1, bsz=4, num_updates=72000, lr=4.6e-05, gnorm=3.223, clip=0, loss_scale=64, train_wall=81, gb_free=28.8, wall=18611 (progress_bar.py:260, log())
[2021-11-14 22:06:53]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:06:53]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 22:07:31]    INFO >> epoch 008:   1768 / 10106 loss=3.632, nll_loss=1.852, ppl=3.61, wps=3405.6, ups=5.25, wpb=648.9, bsz=4, num_updates=72500, lr=4.6e-05, gnorm=3.263, clip=0, loss_scale=32, train_wall=94, gb_free=28.2, wall=18707 (progress_bar.py:260, log())
[2021-11-14 22:08:42]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:09:08]    INFO >> epoch 008:   2268 / 10106 loss=3.649, nll_loss=1.872, ppl=3.66, wps=3468.9, ups=5.17, wpb=671.2, bsz=4, num_updates=73000, lr=4.6e-05, gnorm=3.316, clip=0, loss_scale=16, train_wall=95, gb_free=25.7, wall=18803 (progress_bar.py:260, log())
[2021-11-14 22:10:44]    INFO >> epoch 008:   2768 / 10106 loss=3.654, nll_loss=1.876, ppl=3.67, wps=3509.5, ups=5.17, wpb=679.2, bsz=4, num_updates=73500, lr=4.6e-05, gnorm=3.246, clip=0, loss_scale=16, train_wall=96, gb_free=29.4, wall=18900 (progress_bar.py:260, log())
[2021-11-14 22:12:20]    INFO >> epoch 008:   3268 / 10106 loss=3.64, nll_loss=1.86, ppl=3.63, wps=3419, ups=5.21, wpb=656.2, bsz=4, num_updates=74000, lr=4.6e-05, gnorm=3.316, clip=0, loss_scale=16, train_wall=95, gb_free=28.3, wall=18996 (progress_bar.py:260, log())
[2021-11-14 22:13:13]    INFO >> AMP: overflow detected, setting scale to to 8.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:13:57]    INFO >> epoch 008:   3768 / 10106 loss=3.642, nll_loss=1.863, ppl=3.64, wps=3481.8, ups=5.2, wpb=670, bsz=4, num_updates=74500, lr=4.6e-05, gnorm=3.316, clip=0, loss_scale=8, train_wall=95, gb_free=29.3, wall=19092 (progress_bar.py:260, log())
[2021-11-14 22:15:35]    INFO >> epoch 008:   4268 / 10106 loss=3.657, nll_loss=1.879, ppl=3.68, wps=3515.5, ups=5.06, wpb=694.2, bsz=4, num_updates=75000, lr=4.6e-05, gnorm=3.289, clip=0, loss_scale=8, train_wall=98, gb_free=29.1, wall=19191 (progress_bar.py:260, log())
[2021-11-14 22:17:13]    INFO >> epoch 008:   4768 / 10106 loss=3.675, nll_loss=1.902, ppl=3.74, wps=3514.1, ups=5.12, wpb=685.8, bsz=4, num_updates=75500, lr=4.6e-05, gnorm=3.291, clip=0.2, loss_scale=8, train_wall=97, gb_free=27.6, wall=19289 (progress_bar.py:260, log())
[2021-11-14 22:18:48]    INFO >> epoch 008:   5268 / 10106 loss=3.648, nll_loss=1.869, ppl=3.65, wps=3404.8, ups=5.24, wpb=649.5, bsz=4, num_updates=76000, lr=4.6e-05, gnorm=3.24, clip=0, loss_scale=8, train_wall=94, gb_free=29, wall=19384 (progress_bar.py:260, log())
[2021-11-14 22:20:25]    INFO >> epoch 008:   5768 / 10106 loss=3.646, nll_loss=1.867, ppl=3.65, wps=3629.9, ups=5.19, wpb=699.6, bsz=4, num_updates=76500, lr=4.6e-05, gnorm=3.127, clip=0, loss_scale=16, train_wall=95, gb_free=29.1, wall=19480 (progress_bar.py:260, log())
[2021-11-14 22:22:03]    INFO >> epoch 008:   6268 / 10106 loss=3.604, nll_loss=1.819, ppl=3.53, wps=3459.3, ups=5.1, wpb=678.5, bsz=4, num_updates=77000, lr=4.6e-05, gnorm=3.17, clip=0, loss_scale=16, train_wall=97, gb_free=26.1, wall=19579 (progress_bar.py:260, log())
[2021-11-14 22:23:42]    INFO >> epoch 008:   6768 / 10106 loss=3.647, nll_loss=1.869, ppl=3.65, wps=3476.9, ups=5.06, wpb=687.5, bsz=4, num_updates=77500, lr=4.6e-05, gnorm=3.153, clip=0, loss_scale=16, train_wall=98, gb_free=27.9, wall=19677 (progress_bar.py:260, log())
[2021-11-14 22:25:19]    INFO >> epoch 008:   7268 / 10106 loss=3.605, nll_loss=1.821, ppl=3.53, wps=3532.6, ups=5.14, wpb=687.2, bsz=4, num_updates=78000, lr=4.6e-05, gnorm=3.127, clip=0, loss_scale=16, train_wall=96, gb_free=29.3, wall=19775 (progress_bar.py:260, log())
[2021-11-14 22:26:57]    INFO >> epoch 008:   7768 / 10106 loss=3.63, nll_loss=1.85, ppl=3.6, wps=3518.5, ups=5.12, wpb=687.1, bsz=4, num_updates=78500, lr=4.6e-05, gnorm=3.103, clip=0, loss_scale=32, train_wall=97, gb_free=28.8, wall=19872 (progress_bar.py:260, log())
[2021-11-14 22:28:20]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:28:20]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 22:28:32]    INFO >> epoch 008:   8269 / 10106 loss=3.59, nll_loss=1.807, ppl=3.5, wps=3395.7, ups=5.26, wpb=645.6, bsz=4, num_updates=79000, lr=4.6e-05, gnorm=3.064, clip=0, loss_scale=16, train_wall=94, gb_free=29.3, wall=19967 (progress_bar.py:260, log())
[2021-11-14 22:30:06]    INFO >> epoch 008:   8769 / 10106 loss=3.601, nll_loss=1.816, ppl=3.52, wps=3458.9, ups=5.29, wpb=653.4, bsz=4, num_updates=79500, lr=4.6e-05, gnorm=3.126, clip=0, loss_scale=16, train_wall=93, gb_free=29.2, wall=20062 (progress_bar.py:260, log())
[2021-11-14 22:31:42]    INFO >> epoch 008:   9269 / 10106 loss=3.655, nll_loss=1.878, ppl=3.68, wps=3523.9, ups=5.2, wpb=678.1, bsz=4, num_updates=80000, lr=4.6e-05, gnorm=3.179, clip=0.2, loss_scale=16, train_wall=95, gb_free=25.7, wall=20158 (progress_bar.py:260, log())
[2021-11-14 22:33:22]    INFO >> epoch 008:   9769 / 10106 loss=3.608, nll_loss=1.826, ppl=3.55, wps=3513.1, ups=5, wpb=702, bsz=4, num_updates=80500, lr=4.6e-05, gnorm=3.036, clip=0, loss_scale=16, train_wall=99, gb_free=28.8, wall=20258 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 22:34:42]    INFO >> epoch 008 | loss 3.636 | nll_loss 1.856 | ppl 3.62 | wps 2961.5 | ups 4.37 | wpb 677.7 | bsz 4 | num_updates 80837 | lr 4.6e-05 | gnorm 3.2 | clip 0 | loss_scale 16 | train_wall 1877 | gb_free 29.3 | wall 20338 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 22:38:31]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.812 | nll_loss 2.068 | ppl 4.19 | bleu 12.5715 | wps 758.8 | wpb 3153 | bsz 16 | num_updates 80837 | best_bleu 17.8128 (progress_bar.py:269, print())
[2021-11-14 22:38:39]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_last.pt (epoch 8 @ 80837 updates, score 12.571502) (writing took 7.098736 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 22:39:08]    INFO >> epoch 009:    163 / 10106 loss=3.561, nll_loss=1.771, ppl=3.41, wps=1009.9, ups=1.44, wpb=699.2, bsz=4, num_updates=81000, lr=4.6e-05, gnorm=3.006, clip=0, loss_scale=32, train_wall=86, gb_free=29.3, wall=20604 (progress_bar.py:260, log())
[2021-11-14 22:40:17]    INFO >> epoch 009:    663 / 10106 loss=3.557, nll_loss=1.766, ppl=3.4, wps=5201.2, ups=7.31, wpb=712, bsz=4, num_updates=81500, lr=4.6e-05, gnorm=2.909, clip=0, loss_scale=32, train_wall=67, gb_free=28.7, wall=20673 (progress_bar.py:260, log())
[2021-11-14 22:41:45]    INFO >> epoch 009:   1163 / 10106 loss=3.518, nll_loss=1.721, ppl=3.3, wps=3717, ups=5.66, wpb=657.1, bsz=4, num_updates=82000, lr=4.6e-05, gnorm=2.93, clip=0, loss_scale=32, train_wall=87, gb_free=29.1, wall=20761 (progress_bar.py:260, log())
[2021-11-14 22:43:19]    INFO >> epoch 009:   1663 / 10106 loss=3.529, nll_loss=1.734, ppl=3.33, wps=3511.5, ups=5.31, wpb=661.6, bsz=4, num_updates=82500, lr=4.6e-05, gnorm=2.905, clip=0, loss_scale=32, train_wall=93, gb_free=29.2, wall=20855 (progress_bar.py:260, log())
[2021-11-14 22:44:55]    INFO >> epoch 009:   2163 / 10106 loss=3.545, nll_loss=1.75, ppl=3.36, wps=3446.4, ups=5.21, wpb=662.1, bsz=4, num_updates=83000, lr=4.6e-05, gnorm=2.952, clip=0, loss_scale=64, train_wall=95, gb_free=26.9, wall=20951 (progress_bar.py:260, log())
[2021-11-14 22:46:33]    INFO >> epoch 009:   2663 / 10106 loss=3.559, nll_loss=1.768, ppl=3.41, wps=3578.5, ups=5.13, wpb=697.2, bsz=4, num_updates=83500, lr=4.6e-05, gnorm=2.901, clip=0, loss_scale=64, train_wall=96, gb_free=28.9, wall=21049 (progress_bar.py:260, log())
[2021-11-14 22:47:26]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:48:10]    INFO >> epoch 009:   3163 / 10106 loss=3.508, nll_loss=1.71, ppl=3.27, wps=3360, ups=5.16, wpb=650.7, bsz=4, num_updates=84000, lr=4.6e-05, gnorm=2.894, clip=0, loss_scale=32, train_wall=96, gb_free=29.3, wall=21146 (progress_bar.py:260, log())
[2021-11-14 22:49:00]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:49:46]    INFO >> epoch 009:   3663 / 10106 loss=3.541, nll_loss=1.746, ppl=3.35, wps=3403.1, ups=5.21, wpb=653.1, bsz=4, num_updates=84500, lr=4.6e-05, gnorm=3.023, clip=0.2, loss_scale=16, train_wall=95, gb_free=28.7, wall=21241 (progress_bar.py:260, log())
[2021-11-14 22:51:23]    INFO >> epoch 009:   4163 / 10106 loss=3.571, nll_loss=1.782, ppl=3.44, wps=3455.2, ups=5.12, wpb=675.2, bsz=4, num_updates=85000, lr=4.6e-05, gnorm=3.014, clip=0, loss_scale=16, train_wall=97, gb_free=26, wall=21339 (progress_bar.py:260, log())
[2021-11-14 22:53:00]    INFO >> epoch 009:   4663 / 10106 loss=3.57, nll_loss=1.781, ppl=3.44, wps=3594.4, ups=5.18, wpb=694.3, bsz=4, num_updates=85500, lr=4.6e-05, gnorm=2.871, clip=0, loss_scale=16, train_wall=96, gb_free=28.1, wall=21436 (progress_bar.py:260, log())
[2021-11-14 22:54:37]    INFO >> epoch 009:   5163 / 10106 loss=3.539, nll_loss=1.745, ppl=3.35, wps=3457.3, ups=5.17, wpb=668.2, bsz=4, num_updates=86000, lr=4.6e-05, gnorm=2.903, clip=0, loss_scale=16, train_wall=96, gb_free=28.9, wall=21532 (progress_bar.py:260, log())
[2021-11-14 22:56:17]    INFO >> epoch 009:   5663 / 10106 loss=3.54, nll_loss=1.747, ppl=3.36, wps=3553.6, ups=5, wpb=711.1, bsz=4, num_updates=86500, lr=4.6e-05, gnorm=2.876, clip=0, loss_scale=32, train_wall=99, gb_free=28.6, wall=21632 (progress_bar.py:260, log())
[2021-11-14 22:56:39]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:56:39]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 22:57:56]    INFO >> epoch 009:   6164 / 10106 loss=3.567, nll_loss=1.777, ppl=3.43, wps=3536.3, ups=5.04, wpb=701.3, bsz=4, num_updates=87000, lr=4.6e-05, gnorm=2.889, clip=0, loss_scale=16, train_wall=98, gb_free=29.1, wall=21732 (progress_bar.py:260, log())
[2021-11-14 22:59:34]    INFO >> epoch 009:   6664 / 10106 loss=3.588, nll_loss=1.8, ppl=3.48, wps=3483.8, ups=5.1, wpb=683.3, bsz=4, num_updates=87500, lr=4.6e-05, gnorm=2.964, clip=0, loss_scale=16, train_wall=97, gb_free=29.1, wall=21830 (progress_bar.py:260, log())
[2021-11-14 23:01:11]    INFO >> epoch 009:   7164 / 10106 loss=3.536, nll_loss=1.742, ppl=3.35, wps=3461.5, ups=5.15, wpb=672.2, bsz=4, num_updates=88000, lr=4.6e-05, gnorm=2.883, clip=0, loss_scale=16, train_wall=96, gb_free=29.3, wall=21927 (progress_bar.py:260, log())
[2021-11-14 23:02:48]    INFO >> epoch 009:   7664 / 10106 loss=3.57, nll_loss=1.781, ppl=3.44, wps=3482.6, ups=5.15, wpb=676.1, bsz=4, num_updates=88500, lr=4.6e-05, gnorm=2.867, clip=0, loss_scale=16, train_wall=96, gb_free=29.3, wall=22024 (progress_bar.py:260, log())
[2021-11-14 23:04:24]    INFO >> epoch 009:   8164 / 10106 loss=3.53, nll_loss=1.736, ppl=3.33, wps=3627.7, ups=5.21, wpb=696.7, bsz=4, num_updates=89000, lr=4.6e-05, gnorm=2.783, clip=0, loss_scale=32, train_wall=95, gb_free=28.2, wall=22120 (progress_bar.py:260, log())
[2021-11-14 23:06:02]    INFO >> epoch 009:   8664 / 10106 loss=3.525, nll_loss=1.729, ppl=3.32, wps=3507.5, ups=5.09, wpb=689.3, bsz=4, num_updates=89500, lr=4.6e-05, gnorm=2.773, clip=0, loss_scale=32, train_wall=97, gb_free=28.5, wall=22218 (progress_bar.py:260, log())
[2021-11-14 23:07:38]    INFO >> epoch 009:   9164 / 10106 loss=3.51, nll_loss=1.714, ppl=3.28, wps=3393.5, ups=5.22, wpb=649.7, bsz=4, num_updates=90000, lr=4.6e-05, gnorm=2.782, clip=0, loss_scale=32, train_wall=95, gb_free=29.1, wall=22314 (progress_bar.py:260, log())
[2021-11-14 23:09:13]    INFO >> epoch 009:   9664 / 10106 loss=3.473, nll_loss=1.669, ppl=3.18, wps=3541.1, ups=5.25, wpb=674.1, bsz=4, num_updates=90500, lr=4.6e-05, gnorm=2.763, clip=0, loss_scale=32, train_wall=94, gb_free=28.2, wall=22409 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 23:10:47]    INFO >> epoch 009 | loss 3.538 | nll_loss 1.744 | ppl 3.35 | wps 3163.2 | ups 4.67 | wpb 677.8 | bsz 4 | num_updates 90942 | lr 4.6e-05 | gnorm 2.882 | clip 0 | loss_scale 64 | train_wall 1884 | gb_free 28.7 | wall 22503 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 23:14:57]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.764 | nll_loss 1.993 | ppl 3.98 | bleu 15.3971 | wps 698.7 | wpb 3153 | bsz 16 | num_updates 90942 | best_bleu 17.8128 (progress_bar.py:269, print())
[2021-11-14 23:15:03]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_last.pt (epoch 9 @ 90942 updates, score 15.397127) (writing took 6.655686 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 23:15:19]    INFO >> epoch 010:     58 / 10106 loss=3.465, nll_loss=1.664, ppl=3.17, wps=917.1, ups=1.37, wpb=671.1, bsz=4, num_updates=91000, lr=4.6e-05, gnorm=2.684, clip=0, loss_scale=64, train_wall=86, gb_free=28, wall=22775 (progress_bar.py:260, log())
[2021-11-14 23:16:29]    INFO >> epoch 010:    558 / 10106 loss=3.448, nll_loss=1.641, ppl=3.12, wps=4958, ups=7.17, wpb=691.8, bsz=4, num_updates=91500, lr=4.5e-05, gnorm=2.616, clip=0, loss_scale=64, train_wall=69, gb_free=29, wall=22845 (progress_bar.py:260, log())
[2021-11-14 23:18:05]    INFO >> epoch 010:   1058 / 10106 loss=3.401, nll_loss=1.588, ppl=3.01, wps=3415.4, ups=5.18, wpb=659.2, bsz=4, num_updates=92000, lr=4.5e-05, gnorm=2.636, clip=0, loss_scale=64, train_wall=95, gb_free=28.8, wall=22941 (progress_bar.py:260, log())
[2021-11-14 23:19:42]    INFO >> epoch 010:   1558 / 10106 loss=3.48, nll_loss=1.678, ppl=3.2, wps=3537.8, ups=5.17, wpb=684.9, bsz=4, num_updates=92500, lr=4.5e-05, gnorm=2.665, clip=0, loss_scale=64, train_wall=96, gb_free=28.7, wall=23038 (progress_bar.py:260, log())
[2021-11-14 23:21:19]    INFO >> epoch 010:   2058 / 10106 loss=3.442, nll_loss=1.634, ppl=3.1, wps=3523.4, ups=5.18, wpb=680.2, bsz=4, num_updates=93000, lr=4.5e-05, gnorm=2.64, clip=0, loss_scale=128, train_wall=95, gb_free=29.1, wall=23135 (progress_bar.py:260, log())
[2021-11-14 23:21:31]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 23:21:31]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 23:21:31]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 23:21:31]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 23:22:57]    INFO >> epoch 010:   2559 / 10106 loss=3.474, nll_loss=1.67, ppl=3.18, wps=3480.5, ups=5.07, wpb=686.9, bsz=4, num_updates=93500, lr=4.5e-05, gnorm=2.649, clip=0, loss_scale=16, train_wall=97, gb_free=26.2, wall=23233 (progress_bar.py:260, log())
[2021-11-14 23:24:37]    INFO >> epoch 010:   3059 / 10106 loss=3.502, nll_loss=1.702, ppl=3.25, wps=3655.6, ups=5.04, wpb=724.7, bsz=4, num_updates=94000, lr=4.5e-05, gnorm=2.687, clip=0, loss_scale=16, train_wall=98, gb_free=29.3, wall=23332 (progress_bar.py:260, log())
[2021-11-14 23:26:12]    INFO >> epoch 010:   3559 / 10106 loss=3.472, nll_loss=1.667, ppl=3.18, wps=3451.4, ups=5.24, wpb=658.4, bsz=4, num_updates=94500, lr=4.5e-05, gnorm=2.721, clip=0, loss_scale=16, train_wall=94, gb_free=28.6, wall=23428 (progress_bar.py:260, log())
[2021-11-14 23:27:47]    INFO >> epoch 010:   4059 / 10106 loss=3.467, nll_loss=1.662, ppl=3.16, wps=3386, ups=5.25, wpb=645.4, bsz=4, num_updates=95000, lr=4.5e-05, gnorm=2.748, clip=0, loss_scale=16, train_wall=94, gb_free=26.3, wall=23523 (progress_bar.py:260, log())
[2021-11-14 23:29:23]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 23:29:27]    INFO >> epoch 010:   4559 / 10106 loss=3.475, nll_loss=1.67, ppl=3.18, wps=3620, ups=5.03, wpb=719.1, bsz=4, num_updates=95500, lr=4.5e-05, gnorm=2.646, clip=0, loss_scale=16, train_wall=98, gb_free=29.3, wall=23622 (progress_bar.py:260, log())
[2021-11-14 23:31:04]    INFO >> epoch 010:   5059 / 10106 loss=3.476, nll_loss=1.674, ppl=3.19, wps=3376.3, ups=5.16, wpb=654.9, bsz=4, num_updates=96000, lr=4.5e-05, gnorm=2.693, clip=0, loss_scale=16, train_wall=96, gb_free=29.3, wall=23719 (progress_bar.py:260, log())
[2021-11-14 23:32:48]    INFO >> epoch 010:   5559 / 10106 loss=3.441, nll_loss=1.631, ppl=3.1, wps=3195.4, ups=4.79, wpb=667.5, bsz=4, num_updates=96500, lr=4.5e-05, gnorm=2.655, clip=0, loss_scale=16, train_wall=103, gb_free=28.9, wall=23824 (progress_bar.py:260, log())
[2021-11-14 23:34:27]    INFO >> epoch 010:   6059 / 10106 loss=3.46, nll_loss=1.655, ppl=3.15, wps=3532.9, ups=5.04, wpb=701.6, bsz=4, num_updates=97000, lr=4.5e-05, gnorm=2.634, clip=0, loss_scale=16, train_wall=98, gb_free=27.9, wall=23923 (progress_bar.py:260, log())
[2021-11-14 23:36:06]    INFO >> epoch 010:   6559 / 10106 loss=3.46, nll_loss=1.656, ppl=3.15, wps=3411.6, ups=5.06, wpb=674.4, bsz=4, num_updates=97500, lr=4.5e-05, gnorm=2.678, clip=0, loss_scale=32, train_wall=98, gb_free=28.5, wall=24022 (progress_bar.py:260, log())
[2021-11-14 23:37:44]    INFO >> epoch 010:   7059 / 10106 loss=3.441, nll_loss=1.635, ppl=3.11, wps=3494.9, ups=5.12, wpb=682.4, bsz=4, num_updates=98000, lr=4.5e-05, gnorm=2.632, clip=0, loss_scale=32, train_wall=97, gb_free=28.8, wall=24120 (progress_bar.py:260, log())
[2021-11-14 23:39:24]    INFO >> epoch 010:   7559 / 10106 loss=3.462, nll_loss=1.657, ppl=3.15, wps=3499.1, ups=4.98, wpb=702.3, bsz=4, num_updates=98500, lr=4.5e-05, gnorm=2.646, clip=0, loss_scale=32, train_wall=99, gb_free=28.1, wall=24220 (progress_bar.py:260, log())
[2021-11-14 23:41:00]    INFO >> epoch 010:   8059 / 10106 loss=3.422, nll_loss=1.612, ppl=3.06, wps=3420.4, ups=5.23, wpb=653.5, bsz=4, num_updates=99000, lr=4.5e-05, gnorm=2.613, clip=0, loss_scale=32, train_wall=94, gb_free=29.4, wall=24315 (progress_bar.py:260, log())
[2021-11-14 23:42:35]    INFO >> epoch 010:   8559 / 10106 loss=3.461, nll_loss=1.653, ppl=3.14, wps=3405.6, ups=5.23, wpb=650.8, bsz=4, num_updates=99500, lr=4.5e-05, gnorm=2.683, clip=0, loss_scale=64, train_wall=95, gb_free=28.5, wall=24411 (progress_bar.py:260, log())
[2021-11-14 23:44:00]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 23:44:10]    INFO >> epoch 010:   9059 / 10106 loss=3.395, nll_loss=1.58, ppl=2.99, wps=3421.7, ups=5.24, wpb=652.4, bsz=4, num_updates=100000, lr=4.5e-05, gnorm=2.576, clip=0, loss_scale=32, train_wall=94, gb_free=29, wall=24506 (progress_bar.py:260, log())
[2021-11-14 23:45:47]    INFO >> epoch 010:   9559 / 10106 loss=3.444, nll_loss=1.635, ppl=3.11, wps=3494.2, ups=5.2, wpb=671.8, bsz=4, num_updates=100500, lr=4.5e-05, gnorm=2.624, clip=0, loss_scale=32, train_wall=95, gb_free=27.5, wall=24602 (progress_bar.py:260, log())
[2021-11-14 23:46:55]    INFO >> epoch 010:  10059 / 10106 loss=3.465, nll_loss=1.661, ppl=3.16, wps=5170.8, ups=7.34, wpb=704.5, bsz=4, num_updates=101000, lr=4.5e-05, gnorm=2.517, clip=0, loss_scale=32, train_wall=67, gb_free=29.1, wall=24671 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 23:47:16]    INFO >> epoch 010 | loss 3.454 | nll_loss 1.648 | ppl 3.13 | wps 3129.4 | ups 4.62 | wpb 677.9 | bsz 4 | num_updates 101047 | lr 4.5e-05 | gnorm 2.647 | clip 0 | loss_scale 32 | train_wall 1888 | gb_free 29.1 | wall 24692 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 23:53:24]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.748 | nll_loss 1.986 | ppl 3.96 | bleu 16.2019 | wps 463.7 | wpb 3153 | bsz 16 | num_updates 101047 | best_bleu 17.8128 (progress_bar.py:269, print())
[2021-11-14 23:53:31]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_last.pt (epoch 10 @ 101047 updates, score 16.20191) (writing took 7.158139 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 23:55:05]    INFO >> epoch 011:    453 / 10106 loss=3.369, nll_loss=1.55, ppl=2.93, wps=676.5, ups=1.02, wpb=663.5, bsz=4, num_updates=101500, lr=4.5e-05, gnorm=2.523, clip=0, loss_scale=32, train_wall=92, gb_free=29, wall=25161 (progress_bar.py:260, log())
[2021-11-14 23:56:44]    INFO >> epoch 011:    953 / 10106 loss=3.408, nll_loss=1.594, ppl=3.02, wps=3497.8, ups=5.08, wpb=688.4, bsz=4, num_updates=102000, lr=4.5e-05, gnorm=2.514, clip=0, loss_scale=64, train_wall=97, gb_free=28.3, wall=25259 (progress_bar.py:260, log())
[2021-11-14 23:58:20]    INFO >> epoch 011:   1453 / 10106 loss=3.419, nll_loss=1.607, ppl=3.05, wps=3587.7, ups=5.18, wpb=692, bsz=4, num_updates=102500, lr=4.5e-05, gnorm=2.478, clip=0, loss_scale=64, train_wall=95, gb_free=28.5, wall=25356 (progress_bar.py:260, log())
[2021-11-14 23:59:57]    INFO >> epoch 011:   1953 / 10106 loss=3.341, nll_loss=1.516, ppl=2.86, wps=3462.2, ups=5.17, wpb=669.5, bsz=4, num_updates=103000, lr=4.5e-05, gnorm=2.492, clip=0, loss_scale=64, train_wall=96, gb_free=28, wall=25452 (progress_bar.py:260, log())
[2021-11-15 00:01:34]    INFO >> epoch 011:   2453 / 10106 loss=3.417, nll_loss=1.603, ppl=3.04, wps=3605.6, ups=5.16, wpb=699, bsz=4, num_updates=103500, lr=4.5e-05, gnorm=2.519, clip=0, loss_scale=64, train_wall=96, gb_free=27.2, wall=25549 (progress_bar.py:260, log())
[2021-11-15 00:03:12]    INFO >> epoch 011:   2953 / 10106 loss=3.367, nll_loss=1.547, ppl=2.92, wps=3246.2, ups=5.06, wpb=641.4, bsz=4, num_updates=104000, lr=4.5e-05, gnorm=2.529, clip=0, loss_scale=128, train_wall=98, gb_free=28.9, wall=25648 (progress_bar.py:260, log())
[2021-11-15 00:04:49]    INFO >> epoch 011:   3453 / 10106 loss=3.391, nll_loss=1.576, ppl=2.98, wps=3463.3, ups=5.19, wpb=667, bsz=4, num_updates=104500, lr=4.5e-05, gnorm=2.439, clip=0, loss_scale=128, train_wall=95, gb_free=29.4, wall=25745 (progress_bar.py:260, log())
[2021-11-15 00:05:13]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:05:13]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 00:06:25]    INFO >> epoch 011:   3954 / 10106 loss=3.384, nll_loss=1.569, ppl=2.97, wps=3488.9, ups=5.22, wpb=668.6, bsz=4, num_updates=105000, lr=4.5e-05, gnorm=2.452, clip=0, loss_scale=64, train_wall=95, gb_free=27.8, wall=25840 (progress_bar.py:260, log())
[2021-11-15 00:06:35]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:06:35]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:08:01]    INFO >> epoch 011:   4454 / 10106 loss=3.409, nll_loss=1.595, ppl=3.02, wps=3483.5, ups=5.2, wpb=669.9, bsz=4, num_updates=105500, lr=4.5e-05, gnorm=2.52, clip=0, loss_scale=16, train_wall=95, gb_free=29.4, wall=25937 (progress_bar.py:260, log())
[2021-11-15 00:09:38]    INFO >> epoch 011:   4954 / 10106 loss=3.37, nll_loss=1.552, ppl=2.93, wps=3501.6, ups=5.16, wpb=678.4, bsz=4, num_updates=106000, lr=4.5e-05, gnorm=2.515, clip=0, loss_scale=16, train_wall=96, gb_free=29.3, wall=26033 (progress_bar.py:260, log())
[2021-11-15 00:11:12]    INFO >> epoch 011:   5454 / 10106 loss=3.371, nll_loss=1.551, ppl=2.93, wps=3424.2, ups=5.31, wpb=644.5, bsz=4, num_updates=106500, lr=4.5e-05, gnorm=2.567, clip=0, loss_scale=16, train_wall=93, gb_free=29.3, wall=26128 (progress_bar.py:260, log())
[2021-11-15 00:12:48]    INFO >> epoch 011:   5954 / 10106 loss=3.413, nll_loss=1.6, ppl=3.03, wps=3717.7, ups=5.21, wpb=713.6, bsz=4, num_updates=107000, lr=4.5e-05, gnorm=2.495, clip=0, loss_scale=16, train_wall=95, gb_free=28.6, wall=26223 (progress_bar.py:260, log())
[2021-11-15 00:14:22]    INFO >> epoch 011:   6454 / 10106 loss=3.381, nll_loss=1.562, ppl=2.95, wps=3395, ups=5.3, wpb=640.7, bsz=4, num_updates=107500, lr=4.5e-05, gnorm=2.511, clip=0, loss_scale=32, train_wall=93, gb_free=29.1, wall=26318 (progress_bar.py:260, log())
[2021-11-15 00:15:58]    INFO >> epoch 011:   6954 / 10106 loss=3.375, nll_loss=1.557, ppl=2.94, wps=3577.6, ups=5.19, wpb=689.5, bsz=4, num_updates=108000, lr=4.5e-05, gnorm=2.476, clip=0, loss_scale=32, train_wall=95, gb_free=27.9, wall=26414 (progress_bar.py:260, log())
[2021-11-15 00:17:36]    INFO >> epoch 011:   7454 / 10106 loss=3.36, nll_loss=1.539, ppl=2.91, wps=3547.7, ups=5.12, wpb=693.4, bsz=4, num_updates=108500, lr=4.5e-05, gnorm=2.404, clip=0, loss_scale=32, train_wall=97, gb_free=29, wall=26512 (progress_bar.py:260, log())
[2021-11-15 00:19:13]    INFO >> epoch 011:   7954 / 10106 loss=3.37, nll_loss=1.554, ppl=2.94, wps=3611.7, ups=5.14, wpb=702.1, bsz=4, num_updates=109000, lr=4.5e-05, gnorm=2.42, clip=0, loss_scale=32, train_wall=96, gb_free=26.3, wall=26609 (progress_bar.py:260, log())
[2021-11-15 00:20:53]    INFO >> epoch 011:   8454 / 10106 loss=3.379, nll_loss=1.564, ppl=2.96, wps=3391.2, ups=5.02, wpb=675.5, bsz=4, num_updates=109500, lr=4.5e-05, gnorm=2.438, clip=0, loss_scale=64, train_wall=99, gb_free=29, wall=26709 (progress_bar.py:260, log())
[2021-11-15 00:21:24]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:21:24]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 00:22:16]    INFO >> epoch 011:   8955 / 10106 loss=3.389, nll_loss=1.573, ppl=2.98, wps=4014.6, ups=6.03, wpb=665.9, bsz=4, num_updates=110000, lr=4.5e-05, gnorm=2.436, clip=0, loss_scale=32, train_wall=82, gb_free=28.9, wall=26792 (progress_bar.py:260, log())
[2021-11-15 00:23:24]    INFO >> epoch 011:   9455 / 10106 loss=3.368, nll_loss=1.55, ppl=2.93, wps=4985.4, ups=7.32, wpb=680.9, bsz=4, num_updates=110500, lr=4.5e-05, gnorm=2.423, clip=0, loss_scale=32, train_wall=67, gb_free=29.3, wall=26860 (progress_bar.py:260, log())
[2021-11-15 00:24:32]    INFO >> epoch 011:   9955 / 10106 loss=3.404, nll_loss=1.59, ppl=3.01, wps=5086.8, ups=7.37, wpb=690.3, bsz=4, num_updates=111000, lr=4.5e-05, gnorm=2.447, clip=0, loss_scale=32, train_wall=67, gb_free=29.4, wall=26928 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 00:25:08]    INFO >> epoch 011 | loss 3.385 | nll_loss 1.568 | ppl 2.96 | wps 3014.1 | ups 4.45 | wpb 677.7 | bsz 4 | num_updates 111151 | lr 4.5e-05 | gnorm 2.483 | clip 0 | loss_scale 32 | train_wall 1853 | gb_free 29 | wall 26964 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 00:30:26]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.72 | nll_loss 1.966 | ppl 3.91 | bleu 14.5471 | wps 543.5 | wpb 3153 | bsz 16 | num_updates 111151 | best_bleu 17.8128 (progress_bar.py:269, print())
[2021-11-15 00:30:33]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_last.pt (epoch 11 @ 111151 updates, score 14.547059) (writing took 7.117718 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 00:31:51]    INFO >> epoch 012:    349 / 10106 loss=3.371, nll_loss=1.55, ppl=2.93, wps=817, ups=1.14, wpb=716.6, bsz=4, num_updates=111500, lr=4.4e-05, gnorm=2.495, clip=0.2, loss_scale=32, train_wall=89, gb_free=29.2, wall=27366 (progress_bar.py:260, log())
[2021-11-15 00:33:27]    INFO >> epoch 012:    849 / 10106 loss=3.331, nll_loss=1.503, ppl=2.83, wps=3539.1, ups=5.17, wpb=684.4, bsz=4, num_updates=112000, lr=4.4e-05, gnorm=2.367, clip=0, loss_scale=64, train_wall=96, gb_free=28, wall=27463 (progress_bar.py:260, log())
[2021-11-15 00:34:57]    INFO >> epoch 012:   1349 / 10106 loss=3.339, nll_loss=1.515, ppl=2.86, wps=3827.1, ups=5.6, wpb=683.9, bsz=4, num_updates=112500, lr=4.4e-05, gnorm=2.36, clip=0, loss_scale=64, train_wall=88, gb_free=29.3, wall=27552 (progress_bar.py:260, log())
[2021-11-15 00:36:18]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:36:33]    INFO >> epoch 012:   1849 / 10106 loss=3.297, nll_loss=1.466, ppl=2.76, wps=3476.4, ups=5.2, wpb=669.1, bsz=4, num_updates=113000, lr=4.4e-05, gnorm=2.361, clip=0, loss_scale=32, train_wall=95, gb_free=29.1, wall=27649 (progress_bar.py:260, log())
[2021-11-15 00:38:11]    INFO >> epoch 012:   2349 / 10106 loss=3.321, nll_loss=1.494, ppl=2.82, wps=3399, ups=5.08, wpb=669.6, bsz=4, num_updates=113500, lr=4.4e-05, gnorm=2.379, clip=0, loss_scale=32, train_wall=97, gb_free=27.9, wall=27747 (progress_bar.py:260, log())
[2021-11-15 00:39:48]    INFO >> epoch 012:   2849 / 10106 loss=3.343, nll_loss=1.519, ppl=2.87, wps=3462.7, ups=5.18, wpb=668.9, bsz=4, num_updates=114000, lr=4.4e-05, gnorm=2.41, clip=0, loss_scale=32, train_wall=96, gb_free=26.2, wall=27844 (progress_bar.py:260, log())
[2021-11-15 00:41:29]    INFO >> epoch 012:   3349 / 10106 loss=3.284, nll_loss=1.452, ppl=2.74, wps=3303.8, ups=4.93, wpb=669.7, bsz=4, num_updates=114500, lr=4.4e-05, gnorm=2.373, clip=0, loss_scale=32, train_wall=100, gb_free=29.4, wall=27945 (progress_bar.py:260, log())
[2021-11-15 00:43:09]    INFO >> epoch 012:   3849 / 10106 loss=3.338, nll_loss=1.516, ppl=2.86, wps=3442.4, ups=5.01, wpb=686.8, bsz=4, num_updates=115000, lr=4.4e-05, gnorm=2.343, clip=0, loss_scale=64, train_wall=99, gb_free=29.3, wall=28045 (progress_bar.py:260, log())
[2021-11-15 00:44:45]    INFO >> epoch 012:   4349 / 10106 loss=3.303, nll_loss=1.475, ppl=2.78, wps=3474.9, ups=5.23, wpb=664.4, bsz=4, num_updates=115500, lr=4.4e-05, gnorm=2.343, clip=0, loss_scale=64, train_wall=95, gb_free=28.5, wall=28141 (progress_bar.py:260, log())
[2021-11-15 00:46:21]    INFO >> epoch 012:   4849 / 10106 loss=3.338, nll_loss=1.515, ppl=2.86, wps=3543.5, ups=5.22, wpb=679.5, bsz=4, num_updates=116000, lr=4.4e-05, gnorm=2.341, clip=0, loss_scale=64, train_wall=95, gb_free=29.3, wall=28236 (progress_bar.py:260, log())
[2021-11-15 00:46:29]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:47:58]    INFO >> epoch 012:   5349 / 10106 loss=3.328, nll_loss=1.502, ppl=2.83, wps=3557.9, ups=5.15, wpb=690.3, bsz=4, num_updates=116500, lr=4.4e-05, gnorm=2.358, clip=0, loss_scale=32, train_wall=96, gb_free=28.1, wall=28333 (progress_bar.py:260, log())
[2021-11-15 00:49:36]    INFO >> epoch 012:   5849 / 10106 loss=3.313, nll_loss=1.487, ppl=2.8, wps=3496.7, ups=5.09, wpb=687, bsz=4, num_updates=117000, lr=4.4e-05, gnorm=2.336, clip=0, loss_scale=32, train_wall=97, gb_free=27.4, wall=28432 (progress_bar.py:260, log())
[2021-11-15 00:51:15]    INFO >> epoch 012:   6349 / 10106 loss=3.366, nll_loss=1.546, ppl=2.92, wps=3664.5, ups=5.03, wpb=729.2, bsz=4, num_updates=117500, lr=4.4e-05, gnorm=2.338, clip=0, loss_scale=32, train_wall=98, gb_free=29.1, wall=28531 (progress_bar.py:260, log())
[2021-11-15 00:52:52]    INFO >> epoch 012:   6849 / 10106 loss=3.323, nll_loss=1.5, ppl=2.83, wps=3597.8, ups=5.19, wpb=693.6, bsz=4, num_updates=118000, lr=4.4e-05, gnorm=2.334, clip=0, loss_scale=32, train_wall=95, gb_free=28.4, wall=28628 (progress_bar.py:260, log())
[2021-11-15 00:54:31]    INFO >> epoch 012:   7349 / 10106 loss=3.323, nll_loss=1.498, ppl=2.82, wps=3249.3, ups=5.04, wpb=644.6, bsz=4, num_updates=118500, lr=4.4e-05, gnorm=2.361, clip=0, loss_scale=64, train_wall=98, gb_free=29.3, wall=28727 (progress_bar.py:260, log())
[2021-11-15 00:55:27]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:55:27]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 00:56:06]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:56:06]    INFO >> epoch 012:   7850 / 10106 loss=3.326, nll_loss=1.499, ppl=2.83, wps=3462.9, ups=5.27, wpb=657.2, bsz=4, num_updates=119000, lr=4.4e-05, gnorm=2.367, clip=0, loss_scale=16, train_wall=94, gb_free=27.4, wall=28822 (progress_bar.py:260, log())
[2021-11-15 00:57:28]    INFO >> epoch 012:   8350 / 10106 loss=3.322, nll_loss=1.497, ppl=2.82, wps=4126.4, ups=6.09, wpb=677.7, bsz=4, num_updates=119500, lr=4.4e-05, gnorm=2.384, clip=0, loss_scale=16, train_wall=81, gb_free=29.2, wall=28904 (progress_bar.py:260, log())
[2021-11-15 00:58:38]    INFO >> epoch 012:   8850 / 10106 loss=3.299, nll_loss=1.471, ppl=2.77, wps=4518, ups=7.18, wpb=629.3, bsz=4, num_updates=120000, lr=4.4e-05, gnorm=2.38, clip=0, loss_scale=16, train_wall=69, gb_free=25.6, wall=28973 (progress_bar.py:260, log())
[2021-11-15 00:59:45]    INFO >> epoch 012:   9350 / 10106 loss=3.312, nll_loss=1.487, ppl=2.8, wps=4822.9, ups=7.45, wpb=647, bsz=4, num_updates=120500, lr=4.4e-05, gnorm=2.369, clip=0, loss_scale=16, train_wall=66, gb_free=28.9, wall=29040 (progress_bar.py:260, log())
[2021-11-15 01:00:53]    INFO >> epoch 012:   9850 / 10106 loss=3.349, nll_loss=1.525, ppl=2.88, wps=5256.2, ups=7.3, wpb=720.5, bsz=4, num_updates=121000, lr=4.4e-05, gnorm=2.394, clip=0, loss_scale=32, train_wall=68, gb_free=26.9, wall=29109 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 01:01:43]    INFO >> epoch 012 | loss 3.326 | nll_loss 1.5 | ppl 2.83 | wps 3119.4 | ups 4.6 | wpb 677.8 | bsz 4 | num_updates 121256 | lr 4.4e-05 | gnorm 2.363 | clip 0 | loss_scale 32 | train_wall 1826 | gb_free 27.1 | wall 29159 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 01:08:12]    INFO >> epoch 012 | valid on 'valid' subset | loss 3.692 | nll_loss 1.932 | ppl 3.82 | bleu 13.4217 | wps 437.4 | wpb 3153 | bsz 16 | num_updates 121256 | best_bleu 17.8128 (progress_bar.py:269, print())
[2021-11-15 01:08:19]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_last.pt (epoch 12 @ 121256 updates, score 13.421716) (writing took 7.133815 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 01:09:15]    INFO >> epoch 013:    244 / 10106 loss=3.284, nll_loss=1.453, ppl=2.74, wps=689, ups=1, wpb=691.3, bsz=4, num_updates=121500, lr=4.4e-05, gnorm=2.258, clip=0, loss_scale=32, train_wall=82, gb_free=29.2, wall=29611 (progress_bar.py:260, log())
[2021-11-15 01:10:51]    INFO >> epoch 013:    744 / 10106 loss=3.27, nll_loss=1.434, ppl=2.7, wps=3501.7, ups=5.18, wpb=675.6, bsz=4, num_updates=122000, lr=4.4e-05, gnorm=2.28, clip=0, loss_scale=32, train_wall=95, gb_free=29.3, wall=29707 (progress_bar.py:260, log())
[2021-11-15 01:12:29]    INFO >> epoch 013:   1244 / 10106 loss=3.277, nll_loss=1.444, ppl=2.72, wps=3512.6, ups=5.14, wpb=683.2, bsz=4, num_updates=122500, lr=4.4e-05, gnorm=2.281, clip=0, loss_scale=32, train_wall=96, gb_free=29.3, wall=29804 (progress_bar.py:260, log())
[2021-11-15 01:14:05]    INFO >> epoch 013:   1744 / 10106 loss=3.248, nll_loss=1.411, ppl=2.66, wps=3483.1, ups=5.17, wpb=673.3, bsz=4, num_updates=123000, lr=4.4e-05, gnorm=2.281, clip=0, loss_scale=64, train_wall=96, gb_free=27.9, wall=29901 (progress_bar.py:260, log())
[2021-11-15 01:15:42]    INFO >> epoch 013:   2244 / 10106 loss=3.256, nll_loss=1.421, ppl=2.68, wps=3414.3, ups=5.19, wpb=658, bsz=4, num_updates=123500, lr=4.4e-05, gnorm=2.26, clip=0, loss_scale=64, train_wall=95, gb_free=29.3, wall=29997 (progress_bar.py:260, log())
[2021-11-15 01:17:21]    INFO >> epoch 013:   2744 / 10106 loss=3.233, nll_loss=1.393, ppl=2.63, wps=3292.2, ups=5.03, wpb=654, bsz=4, num_updates=124000, lr=4.4e-05, gnorm=2.281, clip=0, loss_scale=64, train_wall=98, gb_free=26, wall=30097 (progress_bar.py:260, log())
[2021-11-15 01:18:16]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 01:18:57]    INFO >> epoch 013:   3244 / 10106 loss=3.285, nll_loss=1.453, ppl=2.74, wps=3540.5, ups=5.19, wpb=682.5, bsz=4, num_updates=124500, lr=4.4e-05, gnorm=2.285, clip=0, loss_scale=32, train_wall=95, gb_free=29, wall=30193 (progress_bar.py:260, log())
[2021-11-15 01:20:35]    INFO >> epoch 013:   3744 / 10106 loss=3.277, nll_loss=1.445, ppl=2.72, wps=3584.5, ups=5.11, wpb=701.4, bsz=4, num_updates=125000, lr=4.4e-05, gnorm=2.242, clip=0, loss_scale=32, train_wall=97, gb_free=29.1, wall=30291 (progress_bar.py:260, log())
[2021-11-15 01:22:11]    INFO >> epoch 013:   4244 / 10106 loss=3.284, nll_loss=1.451, ppl=2.73, wps=3652.2, ups=5.2, wpb=703, bsz=4, num_updates=125500, lr=4.4e-05, gnorm=2.267, clip=0, loss_scale=32, train_wall=95, gb_free=28, wall=30387 (progress_bar.py:260, log())
[2021-11-15 01:23:47]    INFO >> epoch 013:   4744 / 10106 loss=3.286, nll_loss=1.453, ppl=2.74, wps=3498.1, ups=5.22, wpb=669.6, bsz=4, num_updates=126000, lr=4.4e-05, gnorm=2.331, clip=0, loss_scale=32, train_wall=95, gb_free=28.8, wall=30483 (progress_bar.py:260, log())
[2021-11-15 01:25:23]    INFO >> epoch 013:   5244 / 10106 loss=3.263, nll_loss=1.428, ppl=2.69, wps=3445.7, ups=5.19, wpb=663.9, bsz=4, num_updates=126500, lr=4.4e-05, gnorm=2.249, clip=0, loss_scale=64, train_wall=95, gb_free=28.6, wall=30579 (progress_bar.py:260, log())
[2021-11-15 01:26:58]    INFO >> epoch 013:   5744 / 10106 loss=3.249, nll_loss=1.414, ppl=2.66, wps=3311, ups=5.29, wpb=626.4, bsz=4, num_updates=127000, lr=4.4e-05, gnorm=2.28, clip=0, loss_scale=64, train_wall=94, gb_free=28.6, wall=30674 (progress_bar.py:260, log())
[2021-11-15 01:28:37]    INFO >> epoch 013:   6244 / 10106 loss=3.292, nll_loss=1.463, ppl=2.76, wps=3539.3, ups=5.08, wpb=697.2, bsz=4, num_updates=127500, lr=4.4e-05, gnorm=2.238, clip=0, loss_scale=64, train_wall=98, gb_free=28.3, wall=30772 (progress_bar.py:260, log())
[2021-11-15 01:30:13]    INFO >> epoch 013:   6744 / 10106 loss=3.263, nll_loss=1.427, ppl=2.69, wps=3504.8, ups=5.18, wpb=676.6, bsz=4, num_updates=128000, lr=4.4e-05, gnorm=2.284, clip=0, loss_scale=64, train_wall=95, gb_free=29.2, wall=30869 (progress_bar.py:260, log())
[2021-11-15 01:30:49]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 01:30:49]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 01:31:50]    INFO >> epoch 013:   7245 / 10106 loss=3.266, nll_loss=1.432, ppl=2.7, wps=3617.1, ups=5.13, wpb=704.8, bsz=4, num_updates=128500, lr=4.4e-05, gnorm=2.222, clip=0, loss_scale=32, train_wall=96, gb_free=28.2, wall=30966 (progress_bar.py:260, log())
[2021-11-15 01:33:00]    INFO >> epoch 013:   7745 / 10106 loss=3.341, nll_loss=1.515, ppl=2.86, wps=5058.6, ups=7.24, wpb=699.1, bsz=4, num_updates=129000, lr=4.4e-05, gnorm=2.288, clip=0, loss_scale=32, train_wall=68, gb_free=28.5, wall=31035 (progress_bar.py:260, log())
[2021-11-15 01:34:07]    INFO >> epoch 013:   8245 / 10106 loss=3.282, nll_loss=1.449, ppl=2.73, wps=4972.1, ups=7.42, wpb=670, bsz=4, num_updates=129500, lr=4.4e-05, gnorm=2.316, clip=0, loss_scale=32, train_wall=66, gb_free=29, wall=31103 (progress_bar.py:260, log())
[2021-11-15 01:35:15]    INFO >> epoch 013:   8745 / 10106 loss=3.252, nll_loss=1.416, ppl=2.67, wps=4866.3, ups=7.37, wpb=660.5, bsz=4, num_updates=130000, lr=4.4e-05, gnorm=2.232, clip=0, loss_scale=32, train_wall=67, gb_free=29.3, wall=31171 (progress_bar.py:260, log())
[2021-11-15 01:36:26]    INFO >> epoch 013:   9245 / 10106 loss=3.283, nll_loss=1.454, ppl=2.74, wps=4765.4, ups=7.01, wpb=679.5, bsz=4, num_updates=130500, lr=4.4e-05, gnorm=2.224, clip=0, loss_scale=64, train_wall=70, gb_free=29.3, wall=31242 (progress_bar.py:260, log())
[2021-11-15 01:37:34]    INFO >> epoch 013:   9745 / 10106 loss=3.252, nll_loss=1.417, ppl=2.67, wps=4975.3, ups=7.39, wpb=673, bsz=4, num_updates=131000, lr=4.4e-05, gnorm=2.24, clip=0, loss_scale=64, train_wall=67, gb_free=29.2, wall=31310 (progress_bar.py:260, log())
[2021-11-15 01:37:53]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 01:38:51]    INFO >> epoch 013 | loss 3.272 | nll_loss 1.439 | ppl 2.71 | wps 3075 | ups 4.54 | wpb 677.8 | bsz 4 | num_updates 131361 | lr 4.3e-05 | gnorm 2.267 | clip 0 | loss_scale 32 | train_wall 1787 | gb_free 28.9 | wall 31387 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 01:44:04]    INFO >> epoch 013 | valid on 'valid' subset | loss 3.683 | nll_loss 1.941 | ppl 3.84 | bleu 3.32757 | wps 553 | wpb 3153 | bsz 16 | num_updates 131361 | best_bleu 17.8128 (progress_bar.py:269, print())
[2021-11-15 01:44:11]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_last.pt (epoch 13 @ 131361 updates, score 3.327571) (writing took 7.150165 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 01:44:46]    INFO >> epoch 014:    139 / 10106 loss=3.271, nll_loss=1.439, ppl=2.71, wps=808.9, ups=1.16, wpb=699.2, bsz=4, num_updates=131500, lr=4.3e-05, gnorm=2.249, clip=0, loss_scale=32, train_wall=88, gb_free=28.7, wall=31742 (progress_bar.py:260, log())
[2021-11-15 01:46:22]    INFO >> epoch 014:    639 / 10106 loss=3.196, nll_loss=1.351, ppl=2.55, wps=3393.3, ups=5.22, wpb=650.4, bsz=4, num_updates=132000, lr=4.3e-05, gnorm=2.225, clip=0, loss_scale=32, train_wall=95, gb_free=28.9, wall=31838 (progress_bar.py:260, log())
[2021-11-15 01:48:05]    INFO >> epoch 014:   1139 / 10106 loss=3.25, nll_loss=1.412, ppl=2.66, wps=3323, ups=4.83, wpb=688, bsz=4, num_updates=132500, lr=4.3e-05, gnorm=2.203, clip=0, loss_scale=32, train_wall=102, gb_free=29, wall=31941 (progress_bar.py:260, log())
[2021-11-15 01:49:46]    INFO >> epoch 014:   1639 / 10106 loss=3.205, nll_loss=1.362, ppl=2.57, wps=3330.4, ups=4.98, wpb=669, bsz=4, num_updates=133000, lr=4.3e-05, gnorm=2.194, clip=0, loss_scale=32, train_wall=99, gb_free=28.9, wall=32042 (progress_bar.py:260, log())
[2021-11-15 01:51:22]    INFO >> epoch 014:   2139 / 10106 loss=3.166, nll_loss=1.318, ppl=2.49, wps=3310.6, ups=5.21, wpb=635.3, bsz=4, num_updates=133500, lr=4.3e-05, gnorm=2.172, clip=0, loss_scale=64, train_wall=95, gb_free=28.6, wall=32137 (progress_bar.py:260, log())
[2021-11-15 01:52:58]    INFO >> epoch 014:   2639 / 10106 loss=3.198, nll_loss=1.354, ppl=2.56, wps=3412.8, ups=5.2, wpb=656.2, bsz=4, num_updates=134000, lr=4.3e-05, gnorm=2.194, clip=0, loss_scale=64, train_wall=95, gb_free=29.3, wall=32234 (progress_bar.py:260, log())
[2021-11-15 01:54:39]    INFO >> epoch 014:   3139 / 10106 loss=3.241, nll_loss=1.404, ppl=2.65, wps=3440.9, ups=4.93, wpb=697.4, bsz=4, num_updates=134500, lr=4.3e-05, gnorm=2.187, clip=0, loss_scale=64, train_wall=100, gb_free=26.7, wall=32335 (progress_bar.py:260, log())
[2021-11-15 01:56:18]    INFO >> epoch 014:   3639 / 10106 loss=3.204, nll_loss=1.359, ppl=2.56, wps=3308.5, ups=5.05, wpb=654.8, bsz=4, num_updates=135000, lr=4.3e-05, gnorm=2.206, clip=0, loss_scale=64, train_wall=98, gb_free=29.2, wall=32434 (progress_bar.py:260, log())
[2021-11-15 01:57:54]    INFO >> epoch 014:   4139 / 10106 loss=3.198, nll_loss=1.354, ppl=2.56, wps=3553.1, ups=5.2, wpb=683.1, bsz=4, num_updates=135500, lr=4.3e-05, gnorm=2.151, clip=0, loss_scale=128, train_wall=95, gb_free=28, wall=32530 (progress_bar.py:260, log())
[2021-11-15 01:58:12]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 01:58:12]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 01:58:12]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 01:59:33]    INFO >> epoch 014:   4640 / 10106 loss=3.246, nll_loss=1.407, ppl=2.65, wps=3624.6, ups=5.08, wpb=713.8, bsz=4, num_updates=136000, lr=4.3e-05, gnorm=2.187, clip=0, loss_scale=32, train_wall=97, gb_free=28.2, wall=32629 (progress_bar.py:260, log())
[2021-11-15 02:01:08]    INFO >> epoch 014:   5140 / 10106 loss=3.23, nll_loss=1.388, ppl=2.62, wps=3419.2, ups=5.25, wpb=651, bsz=4, num_updates=136500, lr=4.3e-05, gnorm=2.262, clip=0, loss_scale=32, train_wall=94, gb_free=29.4, wall=32724 (progress_bar.py:260, log())
[2021-11-15 02:02:45]    INFO >> epoch 014:   5640 / 10106 loss=3.24, nll_loss=1.403, ppl=2.64, wps=3561.8, ups=5.17, wpb=689.1, bsz=4, num_updates=137000, lr=4.3e-05, gnorm=2.178, clip=0, loss_scale=32, train_wall=96, gb_free=29.2, wall=32820 (progress_bar.py:260, log())
[2021-11-15 02:04:21]    INFO >> epoch 014:   6140 / 10106 loss=3.22, nll_loss=1.376, ppl=2.6, wps=3516, ups=5.17, wpb=679.6, bsz=4, num_updates=137500, lr=4.3e-05, gnorm=2.214, clip=0, loss_scale=32, train_wall=96, gb_free=27.9, wall=32917 (progress_bar.py:260, log())
[2021-11-15 02:05:59]    INFO >> epoch 014:   6640 / 10106 loss=3.258, nll_loss=1.423, ppl=2.68, wps=3572.1, ups=5.1, wpb=700.5, bsz=4, num_updates=138000, lr=4.3e-05, gnorm=2.165, clip=0, loss_scale=64, train_wall=97, gb_free=29.1, wall=33015 (progress_bar.py:260, log())
[2021-11-15 02:07:23]    INFO >> epoch 014:   7140 / 10106 loss=3.208, nll_loss=1.365, ppl=2.58, wps=4166.1, ups=5.98, wpb=696.9, bsz=4, num_updates=138500, lr=4.3e-05, gnorm=2.163, clip=0, loss_scale=64, train_wall=83, gb_free=29.3, wall=33099 (progress_bar.py:260, log())
[2021-11-15 02:08:31]    INFO >> epoch 014:   7640 / 10106 loss=3.235, nll_loss=1.395, ppl=2.63, wps=4934.4, ups=7.39, wpb=667.3, bsz=4, num_updates=139000, lr=4.3e-05, gnorm=2.217, clip=0, loss_scale=64, train_wall=67, gb_free=26.3, wall=33166 (progress_bar.py:260, log())
[2021-11-15 02:09:40]    INFO >> epoch 014:   8140 / 10106 loss=3.228, nll_loss=1.392, ppl=2.63, wps=4792, ups=7.17, wpb=668.2, bsz=4, num_updates=139500, lr=4.3e-05, gnorm=2.208, clip=0, loss_scale=64, train_wall=69, gb_free=29.4, wall=33236 (progress_bar.py:260, log())
[2021-11-15 02:09:56]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 02:09:56]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 02:09:57]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 02:09:57]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 02:10:49]    INFO >> epoch 014:   8641 / 10106 loss=3.238, nll_loss=1.401, ppl=2.64, wps=5001.1, ups=7.3, wpb=685.3, bsz=4, num_updates=140000, lr=4.3e-05, gnorm=2.19, clip=0, loss_scale=16, train_wall=67, gb_free=29.3, wall=33305 (progress_bar.py:260, log())
[2021-11-15 02:11:57]    INFO >> epoch 014:   9141 / 10106 loss=3.259, nll_loss=1.424, ppl=2.68, wps=5215.9, ups=7.36, wpb=709.1, bsz=4, num_updates=140500, lr=4.3e-05, gnorm=2.228, clip=0, loss_scale=16, train_wall=67, gb_free=29.4, wall=33373 (progress_bar.py:260, log())
[2021-11-15 02:13:09]    INFO >> epoch 014:   9641 / 10106 loss=3.216, nll_loss=1.376, ppl=2.6, wps=4617.3, ups=6.92, wpb=667.6, bsz=4, num_updates=141000, lr=4.3e-05, gnorm=2.217, clip=0, loss_scale=16, train_wall=71, gb_free=28.9, wall=33445 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 02:14:53]    INFO >> epoch 014 | loss 3.225 | nll_loss 1.385 | ppl 2.61 | wps 3166.7 | ups 4.67 | wpb 677.7 | bsz 4 | num_updates 141465 | lr 4.3e-05 | gnorm 2.198 | clip 0 | loss_scale 16 | train_wall 1798 | gb_free 27.3 | wall 33549 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 02:19:18]    INFO >> epoch 014 | valid on 'valid' subset | loss 3.686 | nll_loss 1.921 | ppl 3.79 | bleu 6.70395 | wps 648.4 | wpb 3153 | bsz 16 | num_updates 141465 | best_bleu 17.8128 (progress_bar.py:269, print())
[2021-11-15 02:19:25]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_last.pt (epoch 14 @ 141465 updates, score 6.703947) (writing took 6.714539 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 02:19:39]    INFO >> epoch 015:     35 / 10106 loss=3.255, nll_loss=1.42, ppl=2.68, wps=878.3, ups=1.28, wpb=684.5, bsz=4, num_updates=141500, lr=4.3e-05, gnorm=2.202, clip=0, loss_scale=16, train_wall=94, gb_free=29.3, wall=33835 (progress_bar.py:260, log())
[2021-11-15 02:21:11]    INFO >> epoch 015:    535 / 10106 loss=3.197, nll_loss=1.354, ppl=2.56, wps=3714.2, ups=5.42, wpb=685.2, bsz=4, num_updates=142000, lr=4.3e-05, gnorm=2.142, clip=0, loss_scale=32, train_wall=91, gb_free=28.7, wall=33927 (progress_bar.py:260, log())
[2021-11-15 02:22:49]    INFO >> epoch 015:   1035 / 10106 loss=3.183, nll_loss=1.337, ppl=2.53, wps=3538.2, ups=5.11, wpb=692.6, bsz=4, num_updates=142500, lr=4.3e-05, gnorm=2.148, clip=0, loss_scale=32, train_wall=97, gb_free=26.7, wall=34025 (progress_bar.py:260, log())
[2021-11-15 02:24:25]    INFO >> epoch 015:   1535 / 10106 loss=3.19, nll_loss=1.344, ppl=2.54, wps=3497.1, ups=5.18, wpb=675.4, bsz=4, num_updates=143000, lr=4.3e-05, gnorm=2.156, clip=0, loss_scale=32, train_wall=96, gb_free=27.1, wall=34121 (progress_bar.py:260, log())
[2021-11-15 02:26:00]    INFO >> epoch 015:   2035 / 10106 loss=3.18, nll_loss=1.33, ppl=2.51, wps=3410.2, ups=5.29, wpb=644.6, bsz=4, num_updates=143500, lr=4.3e-05, gnorm=2.182, clip=0, loss_scale=32, train_wall=93, gb_free=28.2, wall=34216 (progress_bar.py:260, log())
[2021-11-15 02:27:41]    INFO >> epoch 015:   2535 / 10106 loss=3.184, nll_loss=1.334, ppl=2.52, wps=3406.9, ups=4.94, wpb=689.3, bsz=4, num_updates=144000, lr=4.3e-05, gnorm=2.132, clip=0, loss_scale=64, train_wall=100, gb_free=29.2, wall=34317 (progress_bar.py:260, log())
[2021-11-15 02:29:18]    INFO >> epoch 015:   3035 / 10106 loss=3.158, nll_loss=1.307, ppl=2.47, wps=3336.3, ups=5.17, wpb=645, bsz=4, num_updates=144500, lr=4.3e-05, gnorm=2.158, clip=0, loss_scale=64, train_wall=96, gb_free=29.4, wall=34414 (progress_bar.py:260, log())
[2021-11-15 02:30:53]    INFO >> epoch 015:   3535 / 10106 loss=3.178, nll_loss=1.332, ppl=2.52, wps=3593.1, ups=5.28, wpb=680.8, bsz=4, num_updates=145000, lr=4.3e-05, gnorm=2.117, clip=0, loss_scale=64, train_wall=94, gb_free=29.3, wall=34508 (progress_bar.py:260, log())
[2021-11-15 02:32:26]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 02:32:30]    INFO >> epoch 015:   4035 / 10106 loss=3.209, nll_loss=1.367, ppl=2.58, wps=3579.5, ups=5.13, wpb=698, bsz=4, num_updates=145500, lr=4.3e-05, gnorm=2.172, clip=0, loss_scale=32, train_wall=96, gb_free=27.2, wall=34606 (progress_bar.py:260, log())
[2021-11-15 02:34:07]    INFO >> epoch 015:   4535 / 10106 loss=3.179, nll_loss=1.329, ppl=2.51, wps=3433.8, ups=5.14, wpb=667.7, bsz=4, num_updates=146000, lr=4.3e-05, gnorm=2.173, clip=0, loss_scale=32, train_wall=96, gb_free=29.2, wall=34703 (progress_bar.py:260, log())
[2021-11-15 02:35:42]    INFO >> epoch 015:   5035 / 10106 loss=3.174, nll_loss=1.328, ppl=2.51, wps=3423.9, ups=5.26, wpb=650.8, bsz=4, num_updates=146500, lr=4.3e-05, gnorm=2.174, clip=0, loss_scale=32, train_wall=94, gb_free=27.1, wall=34798 (progress_bar.py:260, log())
[2021-11-15 02:37:18]    INFO >> epoch 015:   5535 / 10106 loss=3.179, nll_loss=1.333, ppl=2.52, wps=3457.5, ups=5.22, wpb=663, bsz=4, num_updates=147000, lr=4.3e-05, gnorm=2.151, clip=0, loss_scale=32, train_wall=95, gb_free=28.3, wall=34894 (progress_bar.py:260, log())
[2021-11-15 02:38:57]    INFO >> epoch 015:   6035 / 10106 loss=3.212, nll_loss=1.37, ppl=2.58, wps=3384.8, ups=5.05, wpb=670.2, bsz=4, num_updates=147500, lr=4.3e-05, gnorm=2.172, clip=0, loss_scale=64, train_wall=98, gb_free=29.1, wall=34993 (progress_bar.py:260, log())
[2021-11-15 02:40:35]    INFO >> epoch 015:   6535 / 10106 loss=3.166, nll_loss=1.318, ppl=2.49, wps=3514.4, ups=5.12, wpb=686.5, bsz=4, num_updates=148000, lr=4.3e-05, gnorm=2.12, clip=0, loss_scale=64, train_wall=97, gb_free=29.2, wall=35091 (progress_bar.py:260, log())
[2021-11-15 02:42:12]    INFO >> epoch 015:   7035 / 10106 loss=3.164, nll_loss=1.314, ppl=2.49, wps=3540.2, ups=5.15, wpb=687.2, bsz=4, num_updates=148500, lr=4.3e-05, gnorm=2.127, clip=0, loss_scale=64, train_wall=96, gb_free=29.2, wall=35188 (progress_bar.py:260, log())
[2021-11-15 02:43:23]    INFO >> epoch 015:   7535 / 10106 loss=3.182, nll_loss=1.336, ppl=2.52, wps=4958.1, ups=7.07, wpb=701.8, bsz=4, num_updates=149000, lr=4.3e-05, gnorm=2.072, clip=0, loss_scale=64, train_wall=70, gb_free=28.4, wall=35258 (progress_bar.py:260, log())
[2021-11-15 02:44:29]    INFO >> epoch 015:   8035 / 10106 loss=3.155, nll_loss=1.307, ppl=2.47, wps=4860.8, ups=7.5, wpb=648, bsz=4, num_updates=149500, lr=4.3e-05, gnorm=2.127, clip=0, loss_scale=128, train_wall=66, gb_free=28.5, wall=35325 (progress_bar.py:260, log())
[2021-11-15 02:44:51]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 02:45:39]    INFO >> epoch 015:   8535 / 10106 loss=3.175, nll_loss=1.329, ppl=2.51, wps=4853, ups=7.16, wpb=678.2, bsz=4, num_updates=150000, lr=4.3e-05, gnorm=2.111, clip=0, loss_scale=64, train_wall=69, gb_free=28.6, wall=35395 (progress_bar.py:260, log())
[2021-11-15 02:46:48]    INFO >> epoch 015:   9035 / 10106 loss=3.211, nll_loss=1.37, ppl=2.59, wps=5197.8, ups=7.24, wpb=718.1, bsz=4, num_updates=150500, lr=4.3e-05, gnorm=2.077, clip=0, loss_scale=64, train_wall=68, gb_free=28.6, wall=35464 (progress_bar.py:260, log())
[2021-11-15 02:47:58]    INFO >> epoch 015:   9535 / 10106 loss=3.173, nll_loss=1.329, ppl=2.51, wps=4995.3, ups=7.16, wpb=697.6, bsz=4, num_updates=151000, lr=4.3e-05, gnorm=2.1, clip=0, loss_scale=64, train_wall=69, gb_free=29.2, wall=35534 (progress_bar.py:260, log())
[2021-11-15 02:49:13]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 02:49:13]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 02:49:21]    INFO >> epoch 015:  10036 / 10106 loss=3.193, nll_loss=1.348, ppl=2.55, wps=4186.4, ups=6.07, wpb=689.9, bsz=4, num_updates=151500, lr=4.2e-05, gnorm=2.119, clip=0, loss_scale=32, train_wall=81, gb_free=29.1, wall=35616 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 02:49:48]    INFO >> epoch 015 | loss 3.182 | nll_loss 1.335 | ppl 2.52 | wps 3269.3 | ups 4.82 | wpb 677.8 | bsz 4 | num_updates 151570 | lr 4.2e-05 | gnorm 2.136 | clip 0 | loss_scale 32 | train_wall 1779 | gb_free 28.9 | wall 35644 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 02:52:51]    INFO >> epoch 015 | valid on 'valid' subset | loss 3.674 | nll_loss 1.899 | ppl 3.73 | bleu 0.171255 | wps 960.1 | wpb 3153 | bsz 16 | num_updates 151570 | best_bleu 17.8128 (progress_bar.py:269, print())
[2021-11-15 02:52:58]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_last.pt (epoch 15 @ 151570 updates, score 0.171255) (writing took 6.687682 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 02:54:31]    INFO >> epoch 016:    430 / 10106 loss=3.1, nll_loss=1.241, ppl=2.36, wps=1006.3, ups=1.61, wpb=625.3, bsz=4, num_updates=152000, lr=4.2e-05, gnorm=2.129, clip=0, loss_scale=32, train_wall=97, gb_free=29.2, wall=35927 (progress_bar.py:260, log())
[2021-11-15 02:56:09]    INFO >> epoch 016:    930 / 10106 loss=3.174, nll_loss=1.326, ppl=2.51, wps=3554.2, ups=5.12, wpb=693.8, bsz=4, num_updates=152500, lr=4.2e-05, gnorm=2.127, clip=0, loss_scale=32, train_wall=97, gb_free=29.1, wall=36025 (progress_bar.py:260, log())
[2021-11-15 02:57:46]    INFO >> epoch 016:   1430 / 10106 loss=3.122, nll_loss=1.265, ppl=2.4, wps=3319.8, ups=5.16, wpb=643.2, bsz=4, num_updates=153000, lr=4.2e-05, gnorm=2.138, clip=0, loss_scale=32, train_wall=96, gb_free=29.2, wall=36122 (progress_bar.py:260, log())
[2021-11-15 02:59:23]    INFO >> epoch 016:   1930 / 10106 loss=3.157, nll_loss=1.309, ppl=2.48, wps=3551.1, ups=5.12, wpb=694, bsz=4, num_updates=153500, lr=4.2e-05, gnorm=2.052, clip=0, loss_scale=64, train_wall=97, gb_free=29, wall=36219 (progress_bar.py:260, log())
[2021-11-15 03:00:44]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:00:59]    INFO >> epoch 016:   2430 / 10106 loss=3.133, nll_loss=1.279, ppl=2.43, wps=3448.6, ups=5.22, wpb=661, bsz=4, num_updates=154000, lr=4.2e-05, gnorm=2.128, clip=0, loss_scale=32, train_wall=95, gb_free=27.5, wall=36315 (progress_bar.py:260, log())
[2021-11-15 03:02:43]    INFO >> epoch 016:   2930 / 10106 loss=3.115, nll_loss=1.26, ppl=2.39, wps=3174.4, ups=4.81, wpb=660.5, bsz=4, num_updates=154500, lr=4.2e-05, gnorm=2.133, clip=0, loss_scale=32, train_wall=103, gb_free=28.6, wall=36419 (progress_bar.py:260, log())
[2021-11-15 03:04:23]    INFO >> epoch 016:   3430 / 10106 loss=3.185, nll_loss=1.34, ppl=2.53, wps=3382.3, ups=4.99, wpb=677.4, bsz=4, num_updates=155000, lr=4.2e-05, gnorm=2.126, clip=0, loss_scale=32, train_wall=99, gb_free=28.4, wall=36519 (progress_bar.py:260, log())
[2021-11-15 03:06:01]    INFO >> epoch 016:   3930 / 10106 loss=3.179, nll_loss=1.331, ppl=2.52, wps=3528.1, ups=5.13, wpb=688, bsz=4, num_updates=155500, lr=4.2e-05, gnorm=2.126, clip=0, loss_scale=32, train_wall=96, gb_free=29.2, wall=36617 (progress_bar.py:260, log())
[2021-11-15 03:07:40]    INFO >> epoch 016:   4430 / 10106 loss=3.163, nll_loss=1.313, ppl=2.48, wps=3329.9, ups=5.07, wpb=657.3, bsz=4, num_updates=156000, lr=4.2e-05, gnorm=2.113, clip=0, loss_scale=64, train_wall=98, gb_free=29.2, wall=36715 (progress_bar.py:260, log())
[2021-11-15 03:09:18]    INFO >> epoch 016:   4930 / 10106 loss=3.126, nll_loss=1.272, ppl=2.42, wps=3494.3, ups=5.08, wpb=688, bsz=4, num_updates=156500, lr=4.2e-05, gnorm=2.036, clip=0, loss_scale=64, train_wall=97, gb_free=28.7, wall=36814 (progress_bar.py:260, log())
[2021-11-15 03:10:55]    INFO >> epoch 016:   5430 / 10106 loss=3.111, nll_loss=1.256, ppl=2.39, wps=3493.2, ups=5.18, wpb=674.3, bsz=4, num_updates=157000, lr=4.2e-05, gnorm=2.057, clip=0, loss_scale=64, train_wall=95, gb_free=28.7, wall=36910 (progress_bar.py:260, log())
[2021-11-15 03:12:33]    INFO >> epoch 016:   5930 / 10106 loss=3.171, nll_loss=1.323, ppl=2.5, wps=3590, ups=5.1, wpb=703.2, bsz=4, num_updates=157500, lr=4.2e-05, gnorm=2.053, clip=0, loss_scale=64, train_wall=97, gb_free=28.6, wall=37008 (progress_bar.py:260, log())
[2021-11-15 03:14:12]    INFO >> epoch 016:   6430 / 10106 loss=3.138, nll_loss=1.285, ppl=2.44, wps=3481.7, ups=5.05, wpb=689.4, bsz=4, num_updates=158000, lr=4.2e-05, gnorm=2.066, clip=0, loss_scale=128, train_wall=98, gb_free=29.1, wall=37107 (progress_bar.py:260, log())
[2021-11-15 03:14:58]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:14:58]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:14:58]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 03:15:49]    INFO >> epoch 016:   6931 / 10106 loss=3.134, nll_loss=1.28, ppl=2.43, wps=3582.4, ups=5.15, wpb=695.3, bsz=4, num_updates=158500, lr=4.2e-05, gnorm=2.077, clip=0, loss_scale=32, train_wall=96, gb_free=29.1, wall=37204 (progress_bar.py:260, log())
[2021-11-15 03:17:26]    INFO >> epoch 016:   7431 / 10106 loss=3.151, nll_loss=1.303, ppl=2.47, wps=3514.2, ups=5.12, wpb=686.2, bsz=4, num_updates=159000, lr=4.2e-05, gnorm=2.108, clip=0, loss_scale=32, train_wall=97, gb_free=27.5, wall=37302 (progress_bar.py:260, log())
[2021-11-15 03:18:42]    INFO >> epoch 016:   7931 / 10106 loss=3.162, nll_loss=1.312, ppl=2.48, wps=4540.1, ups=6.6, wpb=687.8, bsz=4, num_updates=159500, lr=4.2e-05, gnorm=2.129, clip=0, loss_scale=32, train_wall=75, gb_free=28.9, wall=37378 (progress_bar.py:260, log())
[2021-11-15 03:20:02]    INFO >> epoch 016:   8431 / 10106 loss=3.151, nll_loss=1.302, ppl=2.47, wps=4302.8, ups=6.26, wpb=687.6, bsz=4, num_updates=160000, lr=4.2e-05, gnorm=2.119, clip=0, loss_scale=32, train_wall=79, gb_free=28.7, wall=37458 (progress_bar.py:260, log())
[2021-11-15 03:21:12]    INFO >> epoch 016:   8931 / 10106 loss=3.138, nll_loss=1.286, ppl=2.44, wps=4734.2, ups=7.09, wpb=667.4, bsz=4, num_updates=160500, lr=4.2e-05, gnorm=2.126, clip=0, loss_scale=64, train_wall=69, gb_free=26, wall=37528 (progress_bar.py:260, log())
[2021-11-15 03:22:20]    INFO >> epoch 016:   9431 / 10106 loss=3.139, nll_loss=1.289, ppl=2.44, wps=4863.8, ups=7.35, wpb=662.2, bsz=4, num_updates=161000, lr=4.2e-05, gnorm=2.091, clip=0, loss_scale=64, train_wall=67, gb_free=29, wall=37596 (progress_bar.py:260, log())
[2021-11-15 03:23:30]    INFO >> epoch 016:   9931 / 10106 loss=3.162, nll_loss=1.315, ppl=2.49, wps=5022.6, ups=7.15, wpb=702.2, bsz=4, num_updates=161500, lr=4.2e-05, gnorm=2.056, clip=0, loss_scale=64, train_wall=69, gb_free=27.1, wall=37666 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 03:24:11]    INFO >> epoch 016 | loss 3.146 | nll_loss 1.295 | ppl 2.45 | wps 3319.7 | ups 4.9 | wpb 677.8 | bsz 4 | num_updates 161675 | lr 4.2e-05 | gnorm 2.098 | clip 0 | loss_scale 64 | train_wall 1829 | gb_free 29.2 | wall 37707 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 03:29:44]    INFO >> epoch 016 | valid on 'valid' subset | loss 3.659 | nll_loss 1.883 | ppl 3.69 | bleu 8.03247 | wps 515 | wpb 3153 | bsz 16 | num_updates 161675 | best_bleu 17.8128 (progress_bar.py:269, print())
[2021-11-15 03:29:51]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_last.pt (epoch 16 @ 161675 updates, score 8.032469) (writing took 7.116161 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 03:31:02]    INFO >> epoch 017:    325 / 10106 loss=3.117, nll_loss=1.262, ppl=2.4, wps=749.2, ups=1.11, wpb=676, bsz=4, num_updates=162000, lr=4.2e-05, gnorm=2.053, clip=0, loss_scale=64, train_wall=88, gb_free=29.3, wall=38117 (progress_bar.py:260, log())
[2021-11-15 03:32:36]    INFO >> epoch 017:    825 / 10106 loss=3.099, nll_loss=1.239, ppl=2.36, wps=3417.6, ups=5.27, wpb=648.4, bsz=4, num_updates=162500, lr=4.2e-05, gnorm=2.048, clip=0, loss_scale=128, train_wall=94, gb_free=28.7, wall=38212 (progress_bar.py:260, log())
[2021-11-15 03:34:11]    INFO >> epoch 017:   1325 / 10106 loss=3.073, nll_loss=1.211, ppl=2.32, wps=3427.3, ups=5.27, wpb=650.9, bsz=4, num_updates=163000, lr=4.2e-05, gnorm=2.003, clip=0, loss_scale=128, train_wall=94, gb_free=26.4, wall=38307 (progress_bar.py:260, log())
[2021-11-15 03:35:49]    INFO >> epoch 017:   1825 / 10106 loss=3.067, nll_loss=1.204, ppl=2.3, wps=3437.7, ups=5.12, wpb=671.8, bsz=4, num_updates=163500, lr=4.2e-05, gnorm=1.98, clip=0, loss_scale=128, train_wall=97, gb_free=28.8, wall=38405 (progress_bar.py:260, log())
[2021-11-15 03:37:28]    INFO >> epoch 017:   2325 / 10106 loss=3.072, nll_loss=1.211, ppl=2.32, wps=3350.8, ups=5.07, wpb=660.8, bsz=4, num_updates=164000, lr=4.2e-05, gnorm=2.019, clip=0, loss_scale=128, train_wall=98, gb_free=29.2, wall=38503 (progress_bar.py:260, log())
[2021-11-15 03:39:02]    INFO >> epoch 017:   2825 / 10106 loss=3.052, nll_loss=1.189, ppl=2.28, wps=3417.6, ups=5.29, wpb=646.5, bsz=4, num_updates=164500, lr=4.2e-05, gnorm=1.966, clip=0, loss_scale=256, train_wall=94, gb_free=29.1, wall=38598 (progress_bar.py:260, log())
[2021-11-15 03:40:37]    INFO >> epoch 017:   3325 / 10106 loss=3.072, nll_loss=1.21, ppl=2.31, wps=3549.5, ups=5.26, wpb=675.2, bsz=4, num_updates=165000, lr=4.2e-05, gnorm=1.948, clip=0, loss_scale=256, train_wall=94, gb_free=27.5, wall=38693 (progress_bar.py:260, log())
[2021-11-15 03:42:14]    INFO >> epoch 017:   3825 / 10106 loss=3.063, nll_loss=1.199, ppl=2.3, wps=3430.9, ups=5.18, wpb=661.7, bsz=4, num_updates=165500, lr=4.2e-05, gnorm=1.972, clip=0, loss_scale=256, train_wall=95, gb_free=28.6, wall=38790 (progress_bar.py:260, log())
[2021-11-15 03:42:41]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:43:55]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:43:55]    INFO >> epoch 017:   4325 / 10106 loss=3.14, nll_loss=1.288, ppl=2.44, wps=3659.1, ups=4.93, wpb=742.2, bsz=4, num_updates=166000, lr=4.2e-05, gnorm=1.995, clip=0, loss_scale=64, train_wall=100, gb_free=28.5, wall=38891 (progress_bar.py:260, log())
[2021-11-15 03:45:32]    INFO >> epoch 017:   4825 / 10106 loss=3.089, nll_loss=1.229, ppl=2.34, wps=3483.4, ups=5.18, wpb=672, bsz=4, num_updates=166500, lr=4.2e-05, gnorm=2.059, clip=0, loss_scale=64, train_wall=95, gb_free=28.9, wall=38987 (progress_bar.py:260, log())
[2021-11-15 03:47:11]    INFO >> epoch 017:   5325 / 10106 loss=3.149, nll_loss=1.296, ppl=2.46, wps=3676.8, ups=5.02, wpb=732.7, bsz=4, num_updates=167000, lr=4.2e-05, gnorm=2.053, clip=0, loss_scale=64, train_wall=99, gb_free=29.3, wall=39087 (progress_bar.py:260, log())
[2021-11-15 03:48:46]    INFO >> epoch 017:   5825 / 10106 loss=3.104, nll_loss=1.244, ppl=2.37, wps=3472.6, ups=5.28, wpb=657.6, bsz=4, num_updates=167500, lr=4.2e-05, gnorm=2.104, clip=0, loss_scale=64, train_wall=94, gb_free=29.2, wall=39182 (progress_bar.py:260, log())
[2021-11-15 03:50:24]    INFO >> epoch 017:   6325 / 10106 loss=3.086, nll_loss=1.225, ppl=2.34, wps=3299, ups=5.12, wpb=644.1, bsz=4, num_updates=168000, lr=4.2e-05, gnorm=2.092, clip=0, loss_scale=128, train_wall=97, gb_free=28.5, wall=39279 (progress_bar.py:260, log())
[2021-11-15 03:52:00]    INFO >> epoch 017:   6825 / 10106 loss=3.102, nll_loss=1.241, ppl=2.36, wps=3495.7, ups=5.17, wpb=675.8, bsz=4, num_updates=168500, lr=4.2e-05, gnorm=2.043, clip=0, loss_scale=128, train_wall=96, gb_free=29.3, wall=39376 (progress_bar.py:260, log())
[2021-11-15 03:53:31]    INFO >> epoch 017:   7325 / 10106 loss=3.1, nll_loss=1.244, ppl=2.37, wps=3965.3, ups=5.5, wpb=721.3, bsz=4, num_updates=169000, lr=4.2e-05, gnorm=1.978, clip=0, loss_scale=128, train_wall=90, gb_free=27, wall=39467 (progress_bar.py:260, log())
[2021-11-15 03:54:29]    INFO >> epoch 017:   7825 / 10106 loss=3.107, nll_loss=1.253, ppl=2.38, wps=5855.2, ups=8.67, wpb=675.5, bsz=4, num_updates=169500, lr=4.2e-05, gnorm=2.018, clip=0, loss_scale=128, train_wall=57, gb_free=28.5, wall=39525 (progress_bar.py:260, log())
[2021-11-15 03:55:32]    INFO >> epoch 017:   8325 / 10106 loss=3.086, nll_loss=1.226, ppl=2.34, wps=5337.1, ups=7.92, wpb=673.7, bsz=4, num_updates=170000, lr=4.2e-05, gnorm=1.998, clip=0, loss_scale=256, train_wall=62, gb_free=25.9, wall=39588 (progress_bar.py:260, log())
[2021-11-15 03:56:40]    INFO >> epoch 017:   8825 / 10106 loss=3.107, nll_loss=1.251, ppl=2.38, wps=5210.7, ups=7.38, wpb=705.7, bsz=4, num_updates=170500, lr=4.2e-05, gnorm=1.969, clip=0, loss_scale=256, train_wall=67, gb_free=29.1, wall=39656 (progress_bar.py:260, log())
[2021-11-15 03:57:09]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:57:09]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 03:57:29]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:57:48]    INFO >> epoch 017:   9326 / 10106 loss=3.093, nll_loss=1.234, ppl=2.35, wps=5077.2, ups=7.31, wpb=695, bsz=4, num_updates=171000, lr=4.2e-05, gnorm=1.979, clip=0, loss_scale=64, train_wall=67, gb_free=29.1, wall=39724 (progress_bar.py:260, log())
[2021-11-15 03:58:55]    INFO >> epoch 017:   9826 / 10106 loss=3.055, nll_loss=1.191, ppl=2.28, wps=4879.4, ups=7.45, wpb=655.2, bsz=4, num_updates=171500, lr=4.1e-05, gnorm=2.046, clip=0, loss_scale=64, train_wall=66, gb_free=28.6, wall=39791 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 03:59:53]    INFO >> epoch 017 | loss 3.093 | nll_loss 1.234 | ppl 2.35 | wps 3197.7 | ups 4.72 | wpb 677.8 | bsz 4 | num_updates 171780 | lr 4.1e-05 | gnorm 2.017 | clip 0 | loss_scale 64 | train_wall 1758 | gb_free 29.2 | wall 39849 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 04:04:50]    INFO >> epoch 017 | valid on 'valid' subset | loss 3.665 | nll_loss 1.883 | ppl 3.69 | bleu 6.0875 | wps 578.1 | wpb 3153 | bsz 16 | num_updates 171780 | best_bleu 17.8128 (progress_bar.py:269, print())
[2021-11-15 04:04:58]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/java-python/checkpoints/checkpoint_last.pt (epoch 17 @ 171780 updates, score 6.087501) (writing took 7.240838 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 04:04:58]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:180, should_stop_early())
[2021-11-15 04:04:58]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:260, single_main())
[2021-11-15 04:04:58]    INFO >> done training in 40152.9 seconds (train.py:272, single_main())
