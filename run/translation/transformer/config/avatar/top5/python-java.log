nohup: ignoring input
Using backend: pytorch
[2021-11-14 16:56:01]    INFO >> Load arguments in /home/wanyao/yang/naturalcc-dev/run/translation/transformer/config/avatar/top5/python-java.yml (train.py:291, cli_main())
[2021-11-14 16:56:01]    INFO >> {'criterion': 'label_smoothed_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 500, 'log_format': 'simple', 'tensorboard_logdir': '', 'memory_efficient_fp16': 0, 'fp16_no_flatten_grads': 1, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'empty_cache_freq': 0, 'task': 'plbart_translation', 'seed': 1234, 'cpu': 0, 'fp16': 0, 'fp16_opt_level': '01', 'bf16': 0, 'memory_efficient_bf16': 0, 'server_ip': '', 'server_port': '', 'amp': 1, 'amp_batch_retries': 2, 'amp_init_scale': '2 ** 7', 'amp_scale_window': None}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 4, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 16, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'pipeline_model_parallel': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1, 'block_momentum': 0.875, 'block_lr': 1, 'use_nbm': 0, 'average_sync': 0}, 'task': {'data': '/mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap', 'source_lang': 'python', 'target_lang': 'java', 'load_alignments': 0, 'left_pad_source': 0, 'left_pad_target': 0, 'max_source_positions': 512, 'max_target_positions': 512, 'upsample_primary': 1, 'truncate_source': 1, 'truncate_target': 1, 'append_eos_to_target': 1, 'eval_bleu': 1, 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': None, 'eval_tokenized_bleu': True, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_args': None, 'eval_bleu_print_samples': 0, 'eval_with_sacrebleu': 1}, 'model': {'arch': 'fairseq_transformer', 'offset_positions_by_padding': 1, 'pooler_dropout': 0.1, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.1, 'encoder_positional_embeddings': 0, 'encoder_learned_pos': 1, 'encoder_max_relative_len': 0, 'encoder_embed_path': 0, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_layers': 6, 'encoder_attention_heads': 12, 'encoder_normalize_before': 0, 'decoder_embed_path': '', 'decoder_positional_embeddings': 0, 'decoder_learned_pos': 1, 'decoder_max_relative_len': 0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_attention_heads': 12, 'decoder_normalize_before': 0, 'no_decoder_final_norm': 0, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.1, 'adaptive_softmax_factor': 0.0, 'share_decoder_input_output_embed': 1, 'decoder_out_embed_bias': 1, 'share_all_embeddings': 1, 'adaptive_input': 0, 'adaptive_input_factor': 0.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': 0, 'tie_adaptive_proj': 0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 1, 'no_scale_embedding': 0, 'no_token_positional_embeddings': 0, 'encoder_dropout_in': 0.1, 'encoder_dropout_out': 0.1, 'decoder_dropout_in': 0.1, 'decoder_dropout_out': 0.1, 'max_source_positions': 1024, 'max_target_positions': 1024, 'multihead_attention_version': 'ncc', 'encoder_position_encoding_version': 'ncc_learned', 'decoder_position_encoding_version': 'ncc_learned'}, 'optimization': {'max_epoch': 100, 'max_update': 0, 'clip_norm': 25.0, 'update_freq': [1], 'lrs': [5e-05], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1500, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'sentence_avg': 0, 'label_smoothing': 0.1, 'adam': {'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': 0}}, 'checkpoint': {'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': 1, 'patience': 10, 'save_dir': '/mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints', 'should_continue': 0, 'model_name_or_path': None, 'cache_dir': None, 'logging_steps': 500, 'save_steps': 2000, 'save_total_limit': 2, 'overwrite_output_dir': 0, 'overwrite_cache': 0}, 'eval': {'path': '/mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_best.pt', 'remove_bpe': 'sentencepiece', 'quiet': 1, 'results_path': None, 'model_overrides': '{}', 'topk': 5, 'max_sentences': 2, 'beam': 5, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 500, 'min_len': 1, 'match_source_len': 0, 'no_early_stop': 1, 'unnormalized': 0, 'no_beamable_mm': 0, 'lenpen': 1, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': 0, 'score_reference': 0, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': 0, 'sampling_topk': -1, 'sampling_topp': -1, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': 0, 'print_step': 0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': 0, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': 0, 'retain_iter_history': 0, 'decoding_format': None, 'nltk_bleu': 1, 'rouge': 1}} (train.py:293, cli_main())
[2021-11-14 16:56:01]    INFO >> single GPU training... (train.py:322, cli_main())
[2021-11-14 16:56:01]    INFO >> [python] dictionary: 50005 types (plbart_translation.py:135, setup_task())
[2021-11-14 16:56:01]    INFO >> [java] dictionary: 50005 types (plbart_translation.py:136, setup_task())
[2021-11-14 16:56:03]    INFO >> truncate python/valid.code_tokens to 512 (plbart_translation.py:72, load_langpair_dataset())
[2021-11-14 16:56:07]    INFO >> truncate java/valid.code_tokens to 512 (plbart_translation.py:88, load_langpair_dataset())
[2021-11-14 16:56:10]    INFO >> FairseqTransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
) (train.py:213, single_main())
[2021-11-14 16:56:10]    INFO >> model fairseq_transformer, criterion LabelSmoothedCrossEntropyCriterion (train.py:214, single_main())
[2021-11-14 16:56:10]    INFO >> num. model params: 139220736 (num. trained: 139220736) (train.py:215, single_main())
[2021-11-14 16:56:19]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:542, pretty_print_cuda_env_list())
[2021-11-14 16:56:19]    INFO >> rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                     (utils.py:544, pretty_print_cuda_env_list())
[2021-11-14 16:56:19]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:550, pretty_print_cuda_env_list())
[2021-11-14 16:56:19]    INFO >> training on 1 GPUs (train.py:222, single_main())
[2021-11-14 16:56:19]    INFO >> max tokens per GPU = None and max sentences per GPU = 4 (train.py:223, single_main())
[2021-11-14 16:56:19]    INFO >> no existing checkpoint found /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())
[2021-11-14 16:56:19]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())
[2021-11-14 16:56:19]    INFO >> truncate python/train.code_tokens to 512 (plbart_translation.py:72, load_langpair_dataset())
[2021-11-14 16:56:19]    INFO >> truncate java/train.code_tokens to 512 (plbart_translation.py:88, load_langpair_dataset())
/home/wanyao/yang/naturalcc-dev/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[2021-11-14 16:56:28]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 16:56:28]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 16:56:29]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 16:56:29]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 16:58:43]    INFO >> epoch 001:    501 / 10106 loss=78.035, nll_loss=70.434, ppl=1.59478e+21, wps=3447, ups=3.71, wpb=929.2, bsz=4, num_updates=500, lr=1.7e-05, gnorm=100.188, clip=100, loss_scale=16, train_wall=134, gb_free=28.1, wall=143 (progress_bar.py:260, log())
[2021-11-14 17:00:55]    INFO >> epoch 001:   1001 / 10106 loss=44.809, nll_loss=35.051, ppl=3.559e+10, wps=3515.3, ups=3.79, wpb=926.7, bsz=4, num_updates=1000, lr=3.3e-05, gnorm=38.064, clip=97, loss_scale=16, train_wall=131, gb_free=28.3, wall=275 (progress_bar.py:260, log())
[2021-11-14 17:03:05]    INFO >> epoch 001:   1501 / 10106 loss=37.026, nll_loss=27.434, ppl=1.8128e+08, wps=3482.5, ups=3.83, wpb=908.3, bsz=4, num_updates=1500, lr=5e-05, gnorm=27.438, clip=56.6, loss_scale=16, train_wall=129, gb_free=28, wall=406 (progress_bar.py:260, log())
[2021-11-14 17:04:50]    INFO >> epoch 001:   2001 / 10106 loss=32.436, nll_loss=23.402, ppl=1.10812e+07, wps=4421.2, ups=4.75, wpb=931.6, bsz=4, num_updates=2000, lr=5e-05, gnorm=22.773, clip=21.8, loss_scale=16, train_wall=104, gb_free=28.6, wall=511 (progress_bar.py:260, log())
[2021-11-14 17:06:19]    INFO >> epoch 001:   2501 / 10106 loss=28.897, nll_loss=20.423, ppl=1.40572e+06, wps=5226.4, ups=5.66, wpb=923.8, bsz=4, num_updates=2500, lr=5e-05, gnorm=25.05, clip=41.8, loss_scale=32, train_wall=87, gb_free=28.3, wall=599 (progress_bar.py:260, log())
[2021-11-14 17:07:48]    INFO >> epoch 001:   3001 / 10106 loss=24.49, nll_loss=16.714, ppl=107471, wps=5059.4, ups=5.59, wpb=904.6, bsz=4, num_updates=3000, lr=5e-05, gnorm=31.34, clip=92.8, loss_scale=32, train_wall=88, gb_free=27.9, wall=689 (progress_bar.py:260, log())
[2021-11-14 17:09:38]    INFO >> epoch 001:   3501 / 10106 loss=21.242, nll_loss=14.236, ppl=19298.4, wps=4390.6, ups=4.54, wpb=966.6, bsz=4, num_updates=3500, lr=5e-05, gnorm=28.31, clip=69.8, loss_scale=32, train_wall=109, gb_free=28.5, wall=799 (progress_bar.py:260, log())
[2021-11-14 17:11:49]    INFO >> epoch 001:   4001 / 10106 loss=19.019, nll_loss=12.443, ppl=5569.71, wps=3307.8, ups=3.83, wpb=864.5, bsz=4, num_updates=4000, lr=5e-05, gnorm=25.658, clip=49, loss_scale=32, train_wall=130, gb_free=26.5, wall=930 (progress_bar.py:260, log())
[2021-11-14 17:13:54]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 17:14:02]    INFO >> epoch 001:   4501 / 10106 loss=17.381, nll_loss=11.114, ppl=2216.86, wps=3423.3, ups=3.75, wpb=912.9, bsz=4, num_updates=4500, lr=5e-05, gnorm=22.924, clip=20.8, loss_scale=32, train_wall=132, gb_free=28.6, wall=1063 (progress_bar.py:260, log())
[2021-11-14 17:16:18]    INFO >> epoch 001:   5001 / 10106 loss=16.53, nll_loss=10.644, ppl=1599.69, wps=3517.8, ups=3.67, wpb=957.9, bsz=4, num_updates=5000, lr=5e-05, gnorm=22.159, clip=19.4, loss_scale=32, train_wall=135, gb_free=29, wall=1199 (progress_bar.py:260, log())
[2021-11-14 17:18:09]    INFO >> epoch 001:   5501 / 10106 loss=15.201, nll_loss=9.622, ppl=788.17, wps=4340.6, ups=4.53, wpb=958.6, bsz=4, num_updates=5500, lr=5e-05, gnorm=20.731, clip=11, loss_scale=32, train_wall=109, gb_free=28.8, wall=1309 (progress_bar.py:260, log())
[2021-11-14 17:19:40]    INFO >> epoch 001:   6001 / 10106 loss=14.17, nll_loss=8.875, ppl=469.59, wps=5128.4, ups=5.51, wpb=931.4, bsz=4, num_updates=6000, lr=5e-05, gnorm=20.096, clip=8.4, loss_scale=32, train_wall=90, gb_free=29.2, wall=1400 (progress_bar.py:260, log())
[2021-11-14 17:21:14]    INFO >> epoch 001:   6501 / 10106 loss=13.149, nll_loss=8.116, ppl=277.45, wps=5020.5, ups=5.32, wpb=944.6, bsz=4, num_updates=6500, lr=5e-05, gnorm=18.396, clip=4.8, loss_scale=64, train_wall=93, gb_free=26.1, wall=1494 (progress_bar.py:260, log())
[2021-11-14 17:23:05]    INFO >> epoch 001:   7001 / 10106 loss=12.373, nll_loss=7.547, ppl=187.03, wps=4347.2, ups=4.51, wpb=964.6, bsz=4, num_updates=7000, lr=5e-05, gnorm=17.371, clip=3.6, loss_scale=64, train_wall=110, gb_free=28, wall=1605 (progress_bar.py:260, log())
[2021-11-14 17:25:13]    INFO >> epoch 001:   7501 / 10106 loss=11.714, nll_loss=7.05, ppl=132.55, wps=3414, ups=3.91, wpb=874.2, bsz=4, num_updates=7500, lr=5e-05, gnorm=17.096, clip=5.4, loss_scale=64, train_wall=127, gb_free=26.4, wall=1733 (progress_bar.py:260, log())
[2021-11-14 17:27:24]    INFO >> epoch 001:   8001 / 10106 loss=11.185, nll_loss=6.707, ppl=104.45, wps=3462.2, ups=3.82, wpb=907, bsz=4, num_updates=8000, lr=5e-05, gnorm=16.054, clip=1.4, loss_scale=64, train_wall=130, gb_free=27.3, wall=1864 (progress_bar.py:260, log())
[2021-11-14 17:29:36]    INFO >> epoch 001:   8501 / 10106 loss=10.671, nll_loss=6.354, ppl=81.81, wps=3437.1, ups=3.77, wpb=911.4, bsz=4, num_updates=8500, lr=5e-05, gnorm=15.462, clip=2, loss_scale=128, train_wall=131, gb_free=26.9, wall=1997 (progress_bar.py:260, log())
[2021-11-14 17:30:18]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 17:31:24]    INFO >> epoch 001:   9001 / 10106 loss=10.265, nll_loss=6.094, ppl=68.3, wps=4174.7, ups=4.66, wpb=895.7, bsz=4, num_updates=9000, lr=5e-05, gnorm=14.998, clip=1.2, loss_scale=64, train_wall=106, gb_free=27.8, wall=2104 (progress_bar.py:260, log())
[2021-11-14 17:32:52]    INFO >> epoch 001:   9501 / 10106 loss=9.658, nll_loss=5.607, ppl=48.74, wps=5148.1, ups=5.66, wpb=910.3, bsz=4, num_updates=9500, lr=5e-05, gnorm=13.964, clip=1, loss_scale=64, train_wall=87, gb_free=28.8, wall=2193 (progress_bar.py:260, log())
[2021-11-14 17:34:21]    INFO >> epoch 001:  10001 / 10106 loss=9.361, nll_loss=5.459, ppl=43.99, wps=5180, ups=5.65, wpb=917.2, bsz=4, num_updates=10000, lr=5e-05, gnorm=13.45, clip=1.2, loss_scale=64, train_wall=87, gb_free=27.5, wall=2281 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 17:34:55]    INFO >> epoch 001 | loss 21.782 | nll_loss 15.589 | ppl 49286.9 | wps 4039.7 | ups 4.38 | wpb 922.2 | bsz 4 | num_updates 10105 | lr 5e-05 | gnorm 25.443 | clip 30.1 | loss_scale 64 | train_wall 2269 | gb_free 28.8 | wall 2315 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 17:44:18]    INFO >> epoch 001 | valid on 'valid' subset | loss 8.592 | nll_loss 4.847 | ppl 28.78 | bleu 10.7213 | wps 329.3 | wpb 3457.3 | bsz 16 | num_updates 10105 (progress_bar.py:269, print())
[2021-11-14 17:44:24]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_best.pt (epoch 1 @ 10105 updates, score 10.721289) (writing took 6.119389 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 17:45:30]    INFO >> epoch 002:    395 / 10106 loss=8.872, nll_loss=5.077, ppl=33.77, wps=686.8, ups=0.75, wpb=920, bsz=4, num_updates=10500, lr=5e-05, gnorm=12.852, clip=0.2, loss_scale=64, train_wall=77, gb_free=26.6, wall=2951 (progress_bar.py:260, log())
[2021-11-14 17:45:56]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 17:45:56]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 17:46:41]    INFO >> epoch 002:    896 / 10106 loss=8.623, nll_loss=4.959, ppl=31.1, wps=6361.9, ups=7.11, wpb=894.6, bsz=4, num_updates=11000, lr=5e-05, gnorm=12.729, clip=0.8, loss_scale=64, train_wall=69, gb_free=29, wall=3021 (progress_bar.py:260, log())
[2021-11-14 17:47:55]    INFO >> epoch 002:   1396 / 10106 loss=8.284, nll_loss=4.742, ppl=26.76, wps=6307.9, ups=6.7, wpb=942, bsz=4, num_updates=11500, lr=4.9e-05, gnorm=11.847, clip=0.4, loss_scale=64, train_wall=74, gb_free=29.4, wall=3096 (progress_bar.py:260, log())
[2021-11-14 17:50:06]    INFO >> epoch 002:   1896 / 10106 loss=7.971, nll_loss=4.535, ppl=23.19, wps=3563.1, ups=3.84, wpb=927.9, bsz=4, num_updates=12000, lr=4.9e-05, gnorm=11.459, clip=0.8, loss_scale=64, train_wall=129, gb_free=29.1, wall=3226 (progress_bar.py:260, log())
[2021-11-14 17:52:18]    INFO >> epoch 002:   2396 / 10106 loss=7.657, nll_loss=4.325, ppl=20.04, wps=3339.3, ups=3.78, wpb=884.5, bsz=4, num_updates=12500, lr=4.9e-05, gnorm=11.125, clip=0, loss_scale=64, train_wall=131, gb_free=28.7, wall=3359 (progress_bar.py:260, log())
[2021-11-14 17:54:33]    INFO >> epoch 002:   2896 / 10106 loss=7.403, nll_loss=4.17, ppl=18, wps=3472.4, ups=3.7, wpb=937.6, bsz=4, num_updates=13000, lr=4.9e-05, gnorm=10.349, clip=0, loss_scale=128, train_wall=134, gb_free=27.7, wall=3494 (progress_bar.py:260, log())
[2021-11-14 17:54:45]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 17:56:39]    INFO >> epoch 002:   3396 / 10106 loss=7.221, nll_loss=4.068, ppl=16.77, wps=3659.2, ups=3.98, wpb=919.8, bsz=4, num_updates=13500, lr=4.9e-05, gnorm=10.279, clip=0.2, loss_scale=64, train_wall=124, gb_free=29, wall=3619 (progress_bar.py:260, log())
[2021-11-14 17:58:09]    INFO >> epoch 002:   3896 / 10106 loss=6.979, nll_loss=3.916, ppl=15.1, wps=5109.7, ups=5.51, wpb=927.9, bsz=4, num_updates=14000, lr=4.9e-05, gnorm=9.906, clip=0, loss_scale=64, train_wall=90, gb_free=27.8, wall=3710 (progress_bar.py:260, log())
[2021-11-14 17:59:38]    INFO >> epoch 002:   4396 / 10106 loss=6.804, nll_loss=3.838, ppl=14.3, wps=5445.9, ups=5.65, wpb=963.8, bsz=4, num_updates=14500, lr=4.9e-05, gnorm=9.51, clip=0.2, loss_scale=64, train_wall=87, gb_free=27.6, wall=3799 (progress_bar.py:260, log())
[2021-11-14 18:01:12]    INFO >> epoch 002:   4896 / 10106 loss=6.54, nll_loss=3.645, ppl=12.51, wps=5160.1, ups=5.31, wpb=972.4, bsz=4, num_updates=15000, lr=4.9e-05, gnorm=9.057, clip=0.2, loss_scale=64, train_wall=93, gb_free=28.8, wall=3893 (progress_bar.py:260, log())
[2021-11-14 18:03:20]    INFO >> epoch 002:   5396 / 10106 loss=6.42, nll_loss=3.608, ppl=12.19, wps=3544.1, ups=3.91, wpb=906.4, bsz=4, num_updates=15500, lr=4.9e-05, gnorm=8.901, clip=0.2, loss_scale=128, train_wall=127, gb_free=26.8, wall=4021 (progress_bar.py:260, log())
[2021-11-14 18:05:29]    INFO >> epoch 002:   5896 / 10106 loss=6.131, nll_loss=3.354, ppl=10.23, wps=3393.9, ups=3.87, wpb=876.5, bsz=4, num_updates=16000, lr=4.9e-05, gnorm=8.538, clip=0, loss_scale=128, train_wall=128, gb_free=29.4, wall=4150 (progress_bar.py:260, log())
[2021-11-14 18:07:43]    INFO >> epoch 002:   6396 / 10106 loss=6.109, nll_loss=3.413, ppl=10.65, wps=3383, ups=3.72, wpb=908.6, bsz=4, num_updates=16500, lr=4.9e-05, gnorm=8.405, clip=0, loss_scale=128, train_wall=133, gb_free=28.3, wall=4284 (progress_bar.py:260, log())
[2021-11-14 18:09:51]    INFO >> epoch 002:   6896 / 10106 loss=5.851, nll_loss=3.199, ppl=9.18, wps=3670.9, ups=3.94, wpb=932.8, bsz=4, num_updates=17000, lr=4.9e-05, gnorm=7.883, clip=0.2, loss_scale=128, train_wall=126, gb_free=28.4, wall=4411 (progress_bar.py:260, log())
[2021-11-14 18:09:59]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:10:34]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:10:34]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 18:11:18]    INFO >> epoch 002:   7397 / 10106 loss=5.742, nll_loss=3.149, ppl=8.87, wps=5079.1, ups=5.71, wpb=890, bsz=4, num_updates=17500, lr=4.9e-05, gnorm=8.062, clip=0.2, loss_scale=64, train_wall=86, gb_free=29.1, wall=4499 (progress_bar.py:260, log())
[2021-11-14 18:12:44]    INFO >> epoch 002:   7897 / 10106 loss=5.62, nll_loss=3.092, ppl=8.52, wps=5438.2, ups=5.82, wpb=933.7, bsz=4, num_updates=18000, lr=4.9e-05, gnorm=7.552, clip=0, loss_scale=64, train_wall=85, gb_free=28.9, wall=4585 (progress_bar.py:260, log())
[2021-11-14 18:14:10]    INFO >> epoch 002:   8397 / 10106 loss=5.431, nll_loss=2.959, ppl=7.78, wps=5430.5, ups=5.83, wpb=931.8, bsz=4, num_updates=18500, lr=4.9e-05, gnorm=7.223, clip=0, loss_scale=64, train_wall=85, gb_free=27.5, wall=4670 (progress_bar.py:260, log())
[2021-11-14 18:16:10]    INFO >> epoch 002:   8897 / 10106 loss=5.438, nll_loss=3.047, ppl=8.26, wps=3841.5, ups=4.17, wpb=921, bsz=4, num_updates=19000, lr=4.9e-05, gnorm=7.149, clip=0, loss_scale=64, train_wall=119, gb_free=28, wall=4790 (progress_bar.py:260, log())
[2021-11-14 18:17:43]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:18:26]    INFO >> epoch 002:   9397 / 10106 loss=5.263, nll_loss=2.921, ppl=7.57, wps=3539.6, ups=3.67, wpb=965.5, bsz=4, num_updates=19500, lr=4.9e-05, gnorm=6.765, clip=0, loss_scale=64, train_wall=135, gb_free=29.1, wall=4927 (progress_bar.py:260, log())
[2021-11-14 18:20:37]    INFO >> epoch 002:   9897 / 10106 loss=5.069, nll_loss=2.767, ppl=6.8, wps=3407.6, ups=3.81, wpb=893.5, bsz=4, num_updates=20000, lr=4.9e-05, gnorm=6.623, clip=0, loss_scale=64, train_wall=130, gb_free=28.2, wall=5058 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 18:21:47]    INFO >> epoch 002 | loss 6.612 | nll_loss 3.704 | ppl 13.03 | wps 3312.7 | ups 3.59 | wpb 922.1 | bsz 4 | num_updates 20209 | lr 4.9e-05 | gnorm 9.214 | clip 0.2 | loss_scale 64 | train_wall 2197 | gb_free 27.6 | wall 5128 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 18:29:58]    INFO >> epoch 002 | valid on 'valid' subset | loss 5.122 | nll_loss 2.9 | ppl 7.46 | bleu 20.9941 | wps 379.2 | wpb 3457.3 | bsz 16 | num_updates 20209 | best_bleu 20.9941 (progress_bar.py:269, print())
[2021-11-14 18:30:09]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_best.pt (epoch 2 @ 20209 updates, score 20.994117) (writing took 11.382637 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 18:31:09]    INFO >> epoch 003:    291 / 10106 loss=4.994, nll_loss=2.749, ppl=6.72, wps=724.2, ups=0.79, wpb=915.7, bsz=4, num_updates=20500, lr=4.9e-05, gnorm=6.421, clip=0, loss_scale=64, train_wall=107, gb_free=27.8, wall=5690 (progress_bar.py:260, log())
[2021-11-14 18:32:37]    INFO >> epoch 003:    791 / 10106 loss=4.847, nll_loss=2.644, ppl=6.25, wps=5204.3, ups=5.71, wpb=910.9, bsz=4, num_updates=21000, lr=4.9e-05, gnorm=6.165, clip=0, loss_scale=64, train_wall=86, gb_free=29.4, wall=5778 (progress_bar.py:260, log())
[2021-11-14 18:34:07]    INFO >> epoch 003:   1291 / 10106 loss=4.768, nll_loss=2.621, ppl=6.15, wps=5212.9, ups=5.53, wpb=942.1, bsz=4, num_updates=21500, lr=4.9e-05, gnorm=5.756, clip=0, loss_scale=128, train_wall=89, gb_free=29.3, wall=5868 (progress_bar.py:260, log())
[2021-11-14 18:34:20]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:35:30]    INFO >> epoch 003:   1791 / 10106 loss=4.673, nll_loss=2.563, ppl=5.91, wps=5638.9, ups=6.03, wpb=935.6, bsz=4, num_updates=22000, lr=4.9e-05, gnorm=5.715, clip=0, loss_scale=64, train_wall=82, gb_free=29.3, wall=5951 (progress_bar.py:260, log())
[2021-11-14 18:36:40]    INFO >> epoch 003:   2291 / 10106 loss=4.532, nll_loss=2.462, ppl=5.51, wps=6625.9, ups=7.22, wpb=918.1, bsz=4, num_updates=22500, lr=4.9e-05, gnorm=5.406, clip=0, loss_scale=64, train_wall=68, gb_free=29.1, wall=6020 (progress_bar.py:260, log())
[2021-11-14 18:37:23]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:37:23]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 18:37:49]    INFO >> epoch 003:   2792 / 10106 loss=4.46, nll_loss=2.431, ppl=5.39, wps=6737.8, ups=7.23, wpb=931.6, bsz=4, num_updates=23000, lr=4.9e-05, gnorm=5.209, clip=0, loss_scale=32, train_wall=68, gb_free=29.2, wall=6089 (progress_bar.py:260, log())
[2021-11-14 18:38:58]    INFO >> epoch 003:   3292 / 10106 loss=4.412, nll_loss=2.43, ppl=5.39, wps=6743.8, ups=7.25, wpb=930.4, bsz=4, num_updates=23500, lr=4.9e-05, gnorm=4.993, clip=0, loss_scale=32, train_wall=68, gb_free=27.7, wall=6158 (progress_bar.py:260, log())
[2021-11-14 18:40:07]    INFO >> epoch 003:   3792 / 10106 loss=4.342, nll_loss=2.393, ppl=5.25, wps=6391, ups=7.22, wpb=885.6, bsz=4, num_updates=24000, lr=4.9e-05, gnorm=4.953, clip=0, loss_scale=32, train_wall=68, gb_free=28.2, wall=6228 (progress_bar.py:260, log())
[2021-11-14 18:42:01]    INFO >> epoch 003:   4292 / 10106 loss=4.263, nll_loss=2.339, ppl=5.06, wps=3928, ups=4.37, wpb=899.5, bsz=4, num_updates=24500, lr=4.9e-05, gnorm=4.673, clip=0, loss_scale=32, train_wall=113, gb_free=28.6, wall=6342 (progress_bar.py:260, log())
[2021-11-14 18:44:15]    INFO >> epoch 003:   4792 / 10106 loss=4.257, nll_loss=2.36, ppl=5.13, wps=3434.1, ups=3.74, wpb=917.6, bsz=4, num_updates=25000, lr=4.9e-05, gnorm=4.521, clip=0, loss_scale=64, train_wall=132, gb_free=27.8, wall=6476 (progress_bar.py:260, log())
[2021-11-14 18:46:31]    INFO >> epoch 003:   5292 / 10106 loss=4.181, nll_loss=2.292, ppl=4.9, wps=3431.9, ups=3.69, wpb=929.8, bsz=4, num_updates=25500, lr=4.9e-05, gnorm=4.354, clip=0, loss_scale=64, train_wall=134, gb_free=28.5, wall=6611 (progress_bar.py:260, log())
[2021-11-14 18:48:00]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 18:48:42]    INFO >> epoch 003:   5792 / 10106 loss=4.139, nll_loss=2.26, ppl=4.79, wps=3495.9, ups=3.8, wpb=920.7, bsz=4, num_updates=26000, lr=4.9e-05, gnorm=4.344, clip=0, loss_scale=32, train_wall=130, gb_free=29.1, wall=6743 (progress_bar.py:260, log())
[2021-11-14 18:50:14]    INFO >> epoch 003:   6292 / 10106 loss=4.061, nll_loss=2.187, ppl=4.55, wps=4759.3, ups=5.46, wpb=872.1, bsz=4, num_updates=26500, lr=4.9e-05, gnorm=4.295, clip=0, loss_scale=32, train_wall=91, gb_free=27.4, wall=6834 (progress_bar.py:260, log())
[2021-11-14 18:51:44]    INFO >> epoch 003:   6792 / 10106 loss=4.139, nll_loss=2.29, ppl=4.89, wps=5158.1, ups=5.54, wpb=930.8, bsz=4, num_updates=27000, lr=4.9e-05, gnorm=4.284, clip=0, loss_scale=32, train_wall=89, gb_free=26, wall=6925 (progress_bar.py:260, log())
[2021-11-14 18:53:13]    INFO >> epoch 003:   7292 / 10106 loss=4.084, nll_loss=2.238, ppl=4.72, wps=5160.5, ups=5.61, wpb=919.3, bsz=4, num_updates=27500, lr=4.9e-05, gnorm=4.202, clip=0, loss_scale=32, train_wall=88, gb_free=28.3, wall=7014 (progress_bar.py:260, log())
[2021-11-14 18:55:09]    INFO >> epoch 003:   7792 / 10106 loss=4.028, nll_loss=2.185, ppl=4.55, wps=4044.4, ups=4.31, wpb=937.6, bsz=4, num_updates=28000, lr=4.9e-05, gnorm=4.036, clip=0, loss_scale=64, train_wall=115, gb_free=29.3, wall=7130 (progress_bar.py:260, log())
[2021-11-14 18:57:21]    INFO >> epoch 003:   8292 / 10106 loss=3.995, nll_loss=2.156, ppl=4.46, wps=3599.6, ups=3.79, wpb=949.9, bsz=4, num_updates=28500, lr=4.9e-05, gnorm=4.007, clip=0, loss_scale=64, train_wall=131, gb_free=27.5, wall=7262 (progress_bar.py:260, log())
[2021-11-14 18:59:36]    INFO >> epoch 003:   8792 / 10106 loss=3.997, nll_loss=2.165, ppl=4.49, wps=3463.4, ups=3.69, wpb=937.7, bsz=4, num_updates=29000, lr=4.9e-05, gnorm=3.978, clip=0, loss_scale=64, train_wall=134, gb_free=28.2, wall=7397 (progress_bar.py:260, log())
[2021-11-14 19:00:59]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 19:01:53]    INFO >> epoch 003:   9292 / 10106 loss=3.998, nll_loss=2.169, ppl=4.5, wps=3446.4, ups=3.65, wpb=944.1, bsz=4, num_updates=29500, lr=4.9e-05, gnorm=4.06, clip=0, loss_scale=32, train_wall=135, gb_free=26.9, wall=7534 (progress_bar.py:260, log())
[2021-11-14 19:03:32]    INFO >> epoch 003:   9792 / 10106 loss=3.955, nll_loss=2.129, ppl=4.37, wps=4588.8, ups=5.05, wpb=908.2, bsz=4, num_updates=30000, lr=4.9e-05, gnorm=3.971, clip=0, loss_scale=32, train_wall=98, gb_free=29.4, wall=7633 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 19:04:44]    INFO >> epoch 003 | loss 4.278 | nll_loss 2.335 | ppl 5.04 | wps 3616.3 | ups 3.92 | wpb 922.2 | bsz 4 | num_updates 30314 | lr 4.9e-05 | gnorm 4.706 | clip 0 | loss_scale 32 | train_wall 2029 | gb_free 27.4 | wall 7705 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 19:13:51]    INFO >> epoch 003 | valid on 'valid' subset | loss 4.25 | nll_loss 2.417 | ppl 5.34 | bleu 18.0461 | wps 339.1 | wpb 3457.3 | bsz 16 | num_updates 30314 | best_bleu 20.9941 (progress_bar.py:269, print())
[2021-11-14 19:13:58]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 3 @ 30314 updates, score 18.046112) (writing took 6.879692 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 19:14:55]    INFO >> epoch 004:    186 / 10106 loss=3.884, nll_loss=2.053, ppl=4.15, wps=684.8, ups=0.73, wpb=935, bsz=4, num_updates=30500, lr=4.9e-05, gnorm=3.877, clip=0, loss_scale=32, train_wall=105, gb_free=29.4, wall=8316 (progress_bar.py:260, log())
[2021-11-14 19:16:26]    INFO >> epoch 004:    686 / 10106 loss=3.859, nll_loss=2.028, ppl=4.08, wps=5194.1, ups=5.46, wpb=950.5, bsz=4, num_updates=31000, lr=4.9e-05, gnorm=3.89, clip=0, loss_scale=32, train_wall=90, gb_free=27.9, wall=8407 (progress_bar.py:260, log())
[2021-11-14 19:17:55]    INFO >> epoch 004:   1186 / 10106 loss=3.846, nll_loss=2.02, ppl=4.06, wps=5190.3, ups=5.63, wpb=922.6, bsz=4, num_updates=31500, lr=4.8e-05, gnorm=3.741, clip=0, loss_scale=64, train_wall=88, gb_free=29, wall=8496 (progress_bar.py:260, log())
[2021-11-14 19:19:24]    INFO >> epoch 004:   1686 / 10106 loss=3.845, nll_loss=2.023, ppl=4.06, wps=5250.4, ups=5.64, wpb=930.3, bsz=4, num_updates=32000, lr=4.8e-05, gnorm=3.759, clip=0, loss_scale=64, train_wall=88, gb_free=29.4, wall=8584 (progress_bar.py:260, log())
[2021-11-14 19:21:22]    INFO >> epoch 004:   2186 / 10106 loss=3.852, nll_loss=2.034, ppl=4.1, wps=3762.7, ups=4.22, wpb=891.2, bsz=4, num_updates=32500, lr=4.8e-05, gnorm=3.797, clip=0, loss_scale=64, train_wall=117, gb_free=29.4, wall=8703 (progress_bar.py:260, log())
[2021-11-14 19:22:55]    INFO >> epoch 004:   2686 / 10106 loss=3.818, nll_loss=1.998, ppl=3.99, wps=5098.5, ups=5.39, wpb=946.8, bsz=4, num_updates=33000, lr=4.8e-05, gnorm=3.682, clip=0, loss_scale=64, train_wall=92, gb_free=27.6, wall=8796 (progress_bar.py:260, log())
[2021-11-14 19:23:55]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 19:23:55]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 19:24:00]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 19:24:26]    INFO >> epoch 004:   3187 / 10106 loss=3.789, nll_loss=1.968, ppl=3.91, wps=4897.2, ups=5.54, wpb=884.5, bsz=4, num_updates=33500, lr=4.8e-05, gnorm=3.797, clip=0, loss_scale=32, train_wall=89, gb_free=28.3, wall=8886 (progress_bar.py:260, log())
[2021-11-14 19:25:54]    INFO >> epoch 004:   3687 / 10106 loss=3.785, nll_loss=1.967, ppl=3.91, wps=5228.4, ups=5.63, wpb=928.7, bsz=4, num_updates=34000, lr=4.8e-05, gnorm=3.718, clip=0, loss_scale=32, train_wall=88, gb_free=29, wall=8975 (progress_bar.py:260, log())
[2021-11-14 19:27:18]    INFO >> epoch 004:   4187 / 10106 loss=3.767, nll_loss=1.95, ppl=3.86, wps=5534.9, ups=5.98, wpb=925.1, bsz=4, num_updates=34500, lr=4.8e-05, gnorm=3.642, clip=0, loss_scale=32, train_wall=82, gb_free=27.6, wall=9058 (progress_bar.py:260, log())
[2021-11-14 19:28:13]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 19:28:27]    INFO >> epoch 004:   4687 / 10106 loss=3.794, nll_loss=1.981, ppl=3.95, wps=6428.7, ups=7.19, wpb=894.4, bsz=4, num_updates=35000, lr=4.8e-05, gnorm=3.815, clip=0, loss_scale=16, train_wall=68, gb_free=27.7, wall=9128 (progress_bar.py:260, log())
[2021-11-14 19:29:37]    INFO >> epoch 004:   5187 / 10106 loss=3.748, nll_loss=1.933, ppl=3.82, wps=6747.8, ups=7.23, wpb=933.6, bsz=4, num_updates=35500, lr=4.8e-05, gnorm=3.607, clip=0, loss_scale=16, train_wall=68, gb_free=29.4, wall=9197 (progress_bar.py:260, log())
[2021-11-14 19:30:47]    INFO >> epoch 004:   5687 / 10106 loss=3.801, nll_loss=1.999, ppl=4, wps=6785.9, ups=7.15, wpb=949.4, bsz=4, num_updates=36000, lr=4.8e-05, gnorm=3.603, clip=0, loss_scale=16, train_wall=69, gb_free=26.8, wall=9267 (progress_bar.py:260, log())
[2021-11-14 19:31:56]    INFO >> epoch 004:   6187 / 10106 loss=3.722, nll_loss=1.91, ppl=3.76, wps=6758.8, ups=7.24, wpb=933.1, bsz=4, num_updates=36500, lr=4.8e-05, gnorm=3.578, clip=0, loss_scale=16, train_wall=68, gb_free=28.3, wall=9336 (progress_bar.py:260, log())
[2021-11-14 19:33:35]    INFO >> epoch 004:   6687 / 10106 loss=3.732, nll_loss=1.925, ppl=3.8, wps=4646.5, ups=5.02, wpb=924.8, bsz=4, num_updates=37000, lr=4.8e-05, gnorm=3.634, clip=0, loss_scale=32, train_wall=98, gb_free=29.1, wall=9436 (progress_bar.py:260, log())
[2021-11-14 19:35:47]    INFO >> epoch 004:   7187 / 10106 loss=3.688, nll_loss=1.878, ppl=3.67, wps=3456.3, ups=3.78, wpb=913.9, bsz=4, num_updates=37500, lr=4.8e-05, gnorm=3.538, clip=0, loss_scale=32, train_wall=131, gb_free=28.6, wall=9568 (progress_bar.py:260, log())
[2021-11-14 19:38:02]    INFO >> epoch 004:   7687 / 10106 loss=3.699, nll_loss=1.892, ppl=3.71, wps=3510.4, ups=3.71, wpb=947.4, bsz=4, num_updates=38000, lr=4.8e-05, gnorm=3.444, clip=0, loss_scale=32, train_wall=134, gb_free=28.9, wall=9703 (progress_bar.py:260, log())
[2021-11-14 19:40:17]    INFO >> epoch 004:   8187 / 10106 loss=3.683, nll_loss=1.875, ppl=3.67, wps=3469.3, ups=3.7, wpb=937.4, bsz=4, num_updates=38500, lr=4.8e-05, gnorm=3.467, clip=0, loss_scale=32, train_wall=134, gb_free=29, wall=9838 (progress_bar.py:260, log())
[2021-11-14 19:42:03]    INFO >> epoch 004:   8687 / 10106 loss=3.655, nll_loss=1.845, ppl=3.59, wps=4267, ups=4.73, wpb=901.2, bsz=4, num_updates=39000, lr=4.8e-05, gnorm=3.487, clip=0, loss_scale=64, train_wall=105, gb_free=28, wall=9944 (progress_bar.py:260, log())
[2021-11-14 19:42:21]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 19:42:21]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 19:43:32]    INFO >> epoch 004:   9188 / 10106 loss=3.655, nll_loss=1.847, ppl=3.6, wps=5260.3, ups=5.62, wpb=936.7, bsz=4, num_updates=39500, lr=4.8e-05, gnorm=3.421, clip=0, loss_scale=32, train_wall=88, gb_free=29.2, wall=10033 (progress_bar.py:260, log())
[2021-11-14 19:45:01]    INFO >> epoch 004:   9688 / 10106 loss=3.606, nll_loss=1.791, ppl=3.46, wps=5012, ups=5.64, wpb=889.4, bsz=4, num_updates=40000, lr=4.8e-05, gnorm=3.365, clip=0, loss_scale=32, train_wall=88, gb_free=28.5, wall=10121 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 19:46:35]    INFO >> epoch 004 | loss 3.753 | nll_loss 1.939 | ppl 3.83 | wps 3711.2 | ups 4.02 | wpb 922.1 | bsz 4 | num_updates 40418 | lr 4.8e-05 | gnorm 3.626 | clip 0 | loss_scale 32 | train_wall 1912 | gb_free 28.8 | wall 10216 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 19:55:29]    INFO >> epoch 004 | valid on 'valid' subset | loss 4.013 | nll_loss 2.221 | ppl 4.66 | bleu 18.6011 | wps 348 | wpb 3457.3 | bsz 16 | num_updates 40418 | best_bleu 20.9941 (progress_bar.py:269, print())
[2021-11-14 19:55:36]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 4 @ 40418 updates, score 18.60113) (writing took 7.089035 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 19:55:59]    INFO >> epoch 005:     82 / 10106 loss=3.644, nll_loss=1.837, ppl=3.57, wps=684.5, ups=0.76, wpb=901.6, bsz=4, num_updates=40500, lr=4.8e-05, gnorm=3.422, clip=0, loss_scale=32, train_wall=94, gb_free=28.9, wall=10780 (progress_bar.py:260, log())
[2021-11-14 19:57:32]    INFO >> epoch 005:    582 / 10106 loss=3.599, nll_loss=1.786, ppl=3.45, wps=4886.6, ups=5.42, wpb=902.1, bsz=4, num_updates=41000, lr=4.8e-05, gnorm=3.354, clip=0, loss_scale=32, train_wall=91, gb_free=28.1, wall=10872 (progress_bar.py:260, log())
[2021-11-14 19:59:05]    INFO >> epoch 005:   1082 / 10106 loss=3.59, nll_loss=1.778, ppl=3.43, wps=5003, ups=5.37, wpb=932, bsz=4, num_updates=41500, lr=4.8e-05, gnorm=3.26, clip=0, loss_scale=64, train_wall=92, gb_free=28.7, wall=10965 (progress_bar.py:260, log())
[2021-11-14 20:00:20]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:01:15]    INFO >> epoch 005:   1582 / 10106 loss=3.542, nll_loss=1.723, ppl=3.3, wps=3447.6, ups=3.85, wpb=896.2, bsz=4, num_updates=42000, lr=4.8e-05, gnorm=3.273, clip=0, loss_scale=32, train_wall=129, gb_free=26.8, wall=11095 (progress_bar.py:260, log())
[2021-11-14 20:03:30]    INFO >> epoch 005:   2082 / 10106 loss=3.573, nll_loss=1.759, ppl=3.39, wps=3481.6, ups=3.71, wpb=938.7, bsz=4, num_updates=42500, lr=4.8e-05, gnorm=3.241, clip=0, loss_scale=32, train_wall=134, gb_free=29.3, wall=11230 (progress_bar.py:260, log())
[2021-11-14 20:05:41]    INFO >> epoch 005:   2582 / 10106 loss=3.579, nll_loss=1.769, ppl=3.41, wps=3441.2, ups=3.8, wpb=905.8, bsz=4, num_updates=43000, lr=4.8e-05, gnorm=3.275, clip=0, loss_scale=32, train_wall=130, gb_free=28.9, wall=11362 (progress_bar.py:260, log())
[2021-11-14 20:07:34]    INFO >> epoch 005:   3082 / 10106 loss=3.506, nll_loss=1.688, ppl=3.22, wps=3895.2, ups=4.44, wpb=877.1, bsz=4, num_updates=43500, lr=4.8e-05, gnorm=3.169, clip=0, loss_scale=32, train_wall=111, gb_free=28.9, wall=11474 (progress_bar.py:260, log())
[2021-11-14 20:08:28]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:09:03]    INFO >> epoch 005:   3582 / 10106 loss=3.575, nll_loss=1.766, ppl=3.4, wps=5212.9, ups=5.63, wpb=925.4, bsz=4, num_updates=44000, lr=4.8e-05, gnorm=3.199, clip=0, loss_scale=32, train_wall=87, gb_free=26.5, wall=11563 (progress_bar.py:260, log())
[2021-11-14 20:10:29]    INFO >> epoch 005:   4082 / 10106 loss=3.546, nll_loss=1.735, ppl=3.33, wps=5317, ups=5.75, wpb=924.2, bsz=4, num_updates=44500, lr=4.8e-05, gnorm=3.143, clip=0, loss_scale=32, train_wall=86, gb_free=29.2, wall=11650 (progress_bar.py:260, log())
[2021-11-14 20:12:00]    INFO >> epoch 005:   4582 / 10106 loss=3.568, nll_loss=1.761, ppl=3.39, wps=5182, ups=5.54, wpb=936, bsz=4, num_updates=45000, lr=4.8e-05, gnorm=3.145, clip=0, loss_scale=32, train_wall=89, gb_free=27.5, wall=11740 (progress_bar.py:260, log())
[2021-11-14 20:13:43]    INFO >> epoch 005:   5082 / 10106 loss=3.506, nll_loss=1.691, ppl=3.23, wps=4348.8, ups=4.84, wpb=898.1, bsz=4, num_updates=45500, lr=4.8e-05, gnorm=3.101, clip=0, loss_scale=32, train_wall=102, gb_free=27.7, wall=11844 (progress_bar.py:260, log())
[2021-11-14 20:14:55]    INFO >> epoch 005:   5582 / 10106 loss=3.506, nll_loss=1.692, ppl=3.23, wps=6536.9, ups=7, wpb=934.1, bsz=4, num_updates=46000, lr=4.8e-05, gnorm=3.076, clip=0, loss_scale=64, train_wall=70, gb_free=28.9, wall=11915 (progress_bar.py:260, log())
[2021-11-14 20:16:03]    INFO >> epoch 005:   6082 / 10106 loss=3.484, nll_loss=1.668, ppl=3.18, wps=6940.8, ups=7.32, wpb=947.6, bsz=4, num_updates=46500, lr=4.8e-05, gnorm=2.979, clip=0, loss_scale=64, train_wall=67, gb_free=28.3, wall=11983 (progress_bar.py:260, log())
[2021-11-14 20:17:11]    INFO >> epoch 005:   6582 / 10106 loss=3.488, nll_loss=1.674, ppl=3.19, wps=6460.8, ups=7.36, wpb=877.6, bsz=4, num_updates=47000, lr=4.8e-05, gnorm=3.086, clip=0, loss_scale=64, train_wall=67, gb_free=29.4, wall=12051 (progress_bar.py:260, log())
[2021-11-14 20:17:30]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:17:30]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 20:18:29]    INFO >> epoch 005:   7083 / 10106 loss=3.482, nll_loss=1.665, ppl=3.17, wps=5982.2, ups=6.35, wpb=942.7, bsz=4, num_updates=47500, lr=4.8e-05, gnorm=2.999, clip=0, loss_scale=32, train_wall=78, gb_free=28.7, wall=12130 (progress_bar.py:260, log())
[2021-11-14 20:19:48]    INFO >> epoch 005:   7583 / 10106 loss=3.474, nll_loss=1.658, ppl=3.16, wps=5965.8, ups=6.35, wpb=940.2, bsz=4, num_updates=48000, lr=4.8e-05, gnorm=2.962, clip=0, loss_scale=32, train_wall=78, gb_free=29.3, wall=12209 (progress_bar.py:260, log())
[2021-11-14 20:21:20]    INFO >> epoch 005:   8083 / 10106 loss=3.535, nll_loss=1.729, ppl=3.31, wps=4866.8, ups=5.43, wpb=896.2, bsz=4, num_updates=48500, lr=4.8e-05, gnorm=3.071, clip=0, loss_scale=32, train_wall=91, gb_free=26.5, wall=12301 (progress_bar.py:260, log())
[2021-11-14 20:22:54]    INFO >> epoch 005:   8583 / 10106 loss=3.47, nll_loss=1.654, ppl=3.15, wps=4937.4, ups=5.34, wpb=924.8, bsz=4, num_updates=49000, lr=4.8e-05, gnorm=2.955, clip=0, loss_scale=32, train_wall=93, gb_free=29.2, wall=12395 (progress_bar.py:260, log())
[2021-11-14 20:24:30]    INFO >> epoch 005:   9083 / 10106 loss=3.497, nll_loss=1.687, ppl=3.22, wps=4931.4, ups=5.21, wpb=946, bsz=4, num_updates=49500, lr=4.8e-05, gnorm=2.913, clip=0, loss_scale=64, train_wall=95, gb_free=29.4, wall=12490 (progress_bar.py:260, log())
[2021-11-14 20:26:05]    INFO >> epoch 005:   9583 / 10106 loss=3.449, nll_loss=1.631, ppl=3.1, wps=4983.6, ups=5.28, wpb=944.3, bsz=4, num_updates=50000, lr=4.8e-05, gnorm=2.865, clip=0, loss_scale=64, train_wall=94, gb_free=27.2, wall=12585 (progress_bar.py:260, log())
[2021-11-14 20:27:42]    INFO >> epoch 005:  10083 / 10106 loss=3.476, nll_loss=1.663, ppl=3.17, wps=4889.9, ups=5.13, wpb=952.9, bsz=4, num_updates=50500, lr=4.8e-05, gnorm=2.858, clip=0, loss_scale=64, train_wall=96, gb_free=29.2, wall=12683 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 20:28:01]    INFO >> epoch 005 | loss 3.524 | nll_loss 1.71 | ppl 3.27 | wps 3748.4 | ups 4.06 | wpb 922.3 | bsz 4 | num_updates 50523 | lr 4.8e-05 | gnorm 3.099 | clip 0 | loss_scale 64 | train_wall 1901 | gb_free 27.6 | wall 12702 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 20:33:31]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.887 | nll_loss 2.076 | ppl 4.22 | bleu 12.4587 | wps 571.6 | wpb 3457.3 | bsz 16 | num_updates 50523 | best_bleu 20.9941 (progress_bar.py:269, print())
[2021-11-14 20:33:38]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 5 @ 50523 updates, score 12.458745) (writing took 6.602874 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 20:35:19]    INFO >> epoch 006:    477 / 10106 loss=3.415, nll_loss=1.592, ppl=3.01, wps=1018, ups=1.09, wpb=931, bsz=4, num_updates=51000, lr=4.8e-05, gnorm=2.805, clip=0, loss_scale=64, train_wall=97, gb_free=28, wall=13140 (progress_bar.py:260, log())
[2021-11-14 20:35:50]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:36:54]    INFO >> epoch 006:    977 / 10106 loss=3.414, nll_loss=1.593, ppl=3.02, wps=4826.6, ups=5.31, wpb=908.4, bsz=4, num_updates=51500, lr=4.7e-05, gnorm=2.79, clip=0, loss_scale=64, train_wall=93, gb_free=28.7, wall=13234 (progress_bar.py:260, log())
[2021-11-14 20:38:28]    INFO >> epoch 006:   1477 / 10106 loss=3.358, nll_loss=1.529, ppl=2.89, wps=4837.3, ups=5.3, wpb=912.8, bsz=4, num_updates=52000, lr=4.7e-05, gnorm=2.748, clip=0, loss_scale=64, train_wall=93, gb_free=27.7, wall=13328 (progress_bar.py:260, log())
[2021-11-14 20:40:04]    INFO >> epoch 006:   1977 / 10106 loss=3.376, nll_loss=1.551, ppl=2.93, wps=4714.1, ups=5.23, wpb=902.1, bsz=4, num_updates=52500, lr=4.7e-05, gnorm=2.793, clip=0, loss_scale=64, train_wall=95, gb_free=29.3, wall=13424 (progress_bar.py:260, log())
[2021-11-14 20:41:38]    INFO >> epoch 006:   2477 / 10106 loss=3.397, nll_loss=1.574, ppl=2.98, wps=4917.2, ups=5.28, wpb=931.8, bsz=4, num_updates=53000, lr=4.7e-05, gnorm=2.714, clip=0, loss_scale=64, train_wall=94, gb_free=27.6, wall=13519 (progress_bar.py:260, log())
[2021-11-14 20:42:13]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:42:30]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:42:30]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 20:43:12]    INFO >> epoch 006:   2978 / 10106 loss=3.391, nll_loss=1.568, ppl=2.97, wps=4778.9, ups=5.33, wpb=895.9, bsz=4, num_updates=53500, lr=4.7e-05, gnorm=2.748, clip=0, loss_scale=32, train_wall=93, gb_free=26.7, wall=13613 (progress_bar.py:260, log())
[2021-11-14 20:44:48]    INFO >> epoch 006:   3478 / 10106 loss=3.361, nll_loss=1.533, ppl=2.89, wps=4944.8, ups=5.24, wpb=944.1, bsz=4, num_updates=54000, lr=4.7e-05, gnorm=2.676, clip=0, loss_scale=32, train_wall=94, gb_free=29, wall=13708 (progress_bar.py:260, log())
[2021-11-14 20:46:21]    INFO >> epoch 006:   3978 / 10106 loss=3.405, nll_loss=1.585, ppl=3, wps=4903.5, ups=5.36, wpb=915.2, bsz=4, num_updates=54500, lr=4.7e-05, gnorm=2.735, clip=0, loss_scale=32, train_wall=92, gb_free=27.4, wall=13801 (progress_bar.py:260, log())
[2021-11-14 20:47:55]    INFO >> epoch 006:   4478 / 10106 loss=3.386, nll_loss=1.564, ppl=2.96, wps=4973.9, ups=5.32, wpb=935.8, bsz=4, num_updates=55000, lr=4.7e-05, gnorm=2.655, clip=0, loss_scale=32, train_wall=93, gb_free=26.7, wall=13895 (progress_bar.py:260, log())
[2021-11-14 20:49:05]    INFO >> epoch 006:   4978 / 10106 loss=3.372, nll_loss=1.548, ppl=2.92, wps=6657.3, ups=7.18, wpb=927.7, bsz=4, num_updates=55500, lr=4.7e-05, gnorm=2.668, clip=0, loss_scale=64, train_wall=69, gb_free=28.8, wall=13965 (progress_bar.py:260, log())
[2021-11-14 20:50:13]    INFO >> epoch 006:   5478 / 10106 loss=3.36, nll_loss=1.536, ppl=2.9, wps=6630.6, ups=7.32, wpb=906.1, bsz=4, num_updates=56000, lr=4.7e-05, gnorm=2.661, clip=0, loss_scale=64, train_wall=67, gb_free=28.8, wall=14033 (progress_bar.py:260, log())
[2021-11-14 20:51:25]    INFO >> epoch 006:   5978 / 10106 loss=3.372, nll_loss=1.549, ppl=2.93, wps=6589.1, ups=6.98, wpb=943.6, bsz=4, num_updates=56500, lr=4.7e-05, gnorm=2.627, clip=0, loss_scale=64, train_wall=71, gb_free=29.3, wall=14105 (progress_bar.py:260, log())
[2021-11-14 20:52:35]    INFO >> epoch 006:   6478 / 10106 loss=3.364, nll_loss=1.539, ppl=2.91, wps=6842, ups=7.14, wpb=958.7, bsz=4, num_updates=57000, lr=4.7e-05, gnorm=2.572, clip=0, loss_scale=64, train_wall=69, gb_free=27.4, wall=14175 (progress_bar.py:260, log())
[2021-11-14 20:53:21]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:53:44]    INFO >> epoch 006:   6978 / 10106 loss=3.371, nll_loss=1.548, ppl=2.92, wps=6946.7, ups=7.23, wpb=961.3, bsz=4, num_updates=57500, lr=4.7e-05, gnorm=2.551, clip=0, loss_scale=64, train_wall=68, gb_free=28.1, wall=14244 (progress_bar.py:260, log())
[2021-11-14 20:55:06]    INFO >> epoch 006:   7478 / 10106 loss=3.354, nll_loss=1.529, ppl=2.89, wps=5322.9, ups=6.1, wpb=872.4, bsz=4, num_updates=58000, lr=4.7e-05, gnorm=2.646, clip=0, loss_scale=64, train_wall=81, gb_free=27.7, wall=14326 (progress_bar.py:260, log())
[2021-11-14 20:55:46]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 20:56:44]    INFO >> epoch 006:   7978 / 10106 loss=3.351, nll_loss=1.524, ppl=2.88, wps=4759.5, ups=5.1, wpb=932.5, bsz=4, num_updates=58500, lr=4.7e-05, gnorm=2.634, clip=0, loss_scale=32, train_wall=97, gb_free=26.9, wall=14424 (progress_bar.py:260, log())
[2021-11-14 20:58:20]    INFO >> epoch 006:   8478 / 10106 loss=3.365, nll_loss=1.542, ppl=2.91, wps=4862.5, ups=5.2, wpb=935.2, bsz=4, num_updates=59000, lr=4.7e-05, gnorm=2.593, clip=0, loss_scale=32, train_wall=95, gb_free=27.1, wall=14520 (progress_bar.py:260, log())
[2021-11-14 20:59:55]    INFO >> epoch 006:   8978 / 10106 loss=3.334, nll_loss=1.507, ppl=2.84, wps=4630.1, ups=5.26, wpb=879.7, bsz=4, num_updates=59500, lr=4.7e-05, gnorm=2.582, clip=0, loss_scale=32, train_wall=94, gb_free=28.8, wall=14615 (progress_bar.py:260, log())
[2021-11-14 21:01:28]    INFO >> epoch 006:   9478 / 10106 loss=3.341, nll_loss=1.515, ppl=2.86, wps=4853.8, ups=5.4, wpb=899.7, bsz=4, num_updates=60000, lr=4.7e-05, gnorm=2.506, clip=0, loss_scale=32, train_wall=92, gb_free=29.4, wall=14708 (progress_bar.py:260, log())
[2021-11-14 21:03:01]    INFO >> epoch 006:   9978 / 10106 loss=3.32, nll_loss=1.492, ppl=2.81, wps=4988.2, ups=5.32, wpb=937.1, bsz=4, num_updates=60500, lr=4.7e-05, gnorm=2.458, clip=0, loss_scale=64, train_wall=93, gb_free=26.7, wall=14802 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 21:03:41]    INFO >> epoch 006 | loss 3.369 | nll_loss 1.545 | ppl 2.92 | wps 4355.4 | ups 4.72 | wpb 922.2 | bsz 4 | num_updates 60628 | lr 4.7e-05 | gnorm 2.654 | clip 0 | loss_scale 64 | train_wall 1759 | gb_free 29.2 | wall 14842 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 21:11:26]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.797 | nll_loss 1.991 | ppl 3.98 | bleu 20.2158 | wps 400.1 | wpb 3457.3 | bsz 16 | num_updates 60628 | best_bleu 20.9941 (progress_bar.py:269, print())
[2021-11-14 21:11:33]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 6 @ 60628 updates, score 20.215836) (writing took 7.124465 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 21:12:55]    INFO >> epoch 007:    372 / 10106 loss=3.287, nll_loss=1.454, ppl=2.74, wps=787.2, ups=0.84, wpb=933.8, bsz=4, num_updates=61000, lr=4.7e-05, gnorm=2.446, clip=0, loss_scale=64, train_wall=98, gb_free=25.9, wall=15395 (progress_bar.py:260, log())
[2021-11-14 21:14:07]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 21:14:07]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 21:14:31]    INFO >> epoch 007:    873 / 10106 loss=3.299, nll_loss=1.465, ppl=2.76, wps=4811, ups=5.16, wpb=932.6, bsz=4, num_updates=61500, lr=4.7e-05, gnorm=2.422, clip=0, loss_scale=32, train_wall=96, gb_free=27.7, wall=15492 (progress_bar.py:260, log())
[2021-11-14 21:16:07]    INFO >> epoch 007:   1373 / 10106 loss=3.293, nll_loss=1.46, ppl=2.75, wps=4853.9, ups=5.25, wpb=924, bsz=4, num_updates=62000, lr=4.7e-05, gnorm=2.474, clip=0, loss_scale=32, train_wall=94, gb_free=28.4, wall=15587 (progress_bar.py:260, log())
[2021-11-14 21:17:42]    INFO >> epoch 007:   1873 / 10106 loss=3.291, nll_loss=1.459, ppl=2.75, wps=4852.9, ups=5.27, wpb=920.4, bsz=4, num_updates=62500, lr=4.7e-05, gnorm=2.423, clip=0, loss_scale=32, train_wall=94, gb_free=29, wall=15682 (progress_bar.py:260, log())
[2021-11-14 21:19:19]    INFO >> epoch 007:   2373 / 10106 loss=3.327, nll_loss=1.499, ppl=2.83, wps=4756.3, ups=5.12, wpb=928.4, bsz=4, num_updates=63000, lr=4.7e-05, gnorm=2.437, clip=0, loss_scale=32, train_wall=97, gb_free=28.6, wall=15780 (progress_bar.py:260, log())
[2021-11-14 21:20:54]    INFO >> epoch 007:   2873 / 10106 loss=3.237, nll_loss=1.398, ppl=2.64, wps=4783.7, ups=5.27, wpb=908.5, bsz=4, num_updates=63500, lr=4.7e-05, gnorm=2.341, clip=0, loss_scale=64, train_wall=94, gb_free=28.8, wall=15875 (progress_bar.py:260, log())
[2021-11-14 21:22:06]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 21:22:29]    INFO >> epoch 007:   3373 / 10106 loss=3.239, nll_loss=1.402, ppl=2.64, wps=4870.2, ups=5.27, wpb=924.5, bsz=4, num_updates=64000, lr=4.7e-05, gnorm=2.341, clip=0, loss_scale=32, train_wall=94, gb_free=28.5, wall=15970 (progress_bar.py:260, log())
[2021-11-14 21:23:51]    INFO >> epoch 007:   3873 / 10106 loss=3.286, nll_loss=1.454, ppl=2.74, wps=5519.6, ups=6.06, wpb=910.4, bsz=4, num_updates=64500, lr=4.7e-05, gnorm=2.398, clip=0, loss_scale=32, train_wall=81, gb_free=28, wall=16052 (progress_bar.py:260, log())
[2021-11-14 21:25:04]    INFO >> epoch 007:   4373 / 10106 loss=3.297, nll_loss=1.466, ppl=2.76, wps=6495.2, ups=6.86, wpb=947, bsz=4, num_updates=65000, lr=4.7e-05, gnorm=2.381, clip=0, loss_scale=32, train_wall=72, gb_free=29.4, wall=16125 (progress_bar.py:260, log())
[2021-11-14 21:26:13]    INFO >> epoch 007:   4873 / 10106 loss=3.236, nll_loss=1.399, ppl=2.64, wps=6756, ups=7.28, wpb=927.5, bsz=4, num_updates=65500, lr=4.7e-05, gnorm=2.333, clip=0, loss_scale=32, train_wall=68, gb_free=29.3, wall=16194 (progress_bar.py:260, log())
[2021-11-14 21:27:15]    INFO >> epoch 007:   5373 / 10106 loss=3.248, nll_loss=1.412, ppl=2.66, wps=7470.6, ups=8.05, wpb=928.4, bsz=4, num_updates=66000, lr=4.7e-05, gnorm=2.291, clip=0, loss_scale=64, train_wall=61, gb_free=28.9, wall=16256 (progress_bar.py:260, log())
[2021-11-14 21:28:26]    INFO >> epoch 007:   5873 / 10106 loss=3.243, nll_loss=1.407, ppl=2.65, wps=6349.9, ups=7.08, wpb=896.4, bsz=4, num_updates=66500, lr=4.7e-05, gnorm=2.3, clip=0, loss_scale=64, train_wall=70, gb_free=28.4, wall=16326 (progress_bar.py:260, log())
[2021-11-14 21:29:42]    INFO >> epoch 007:   6373 / 10106 loss=3.213, nll_loss=1.373, ppl=2.59, wps=6117.2, ups=6.59, wpb=928.2, bsz=4, num_updates=67000, lr=4.7e-05, gnorm=2.221, clip=0, loss_scale=64, train_wall=75, gb_free=29.3, wall=16402 (progress_bar.py:260, log())
[2021-11-14 21:31:15]    INFO >> epoch 007:   6873 / 10106 loss=3.207, nll_loss=1.366, ppl=2.58, wps=4941, ups=5.35, wpb=924.3, bsz=4, num_updates=67500, lr=4.7e-05, gnorm=2.255, clip=0, loss_scale=64, train_wall=92, gb_free=29.4, wall=16496 (progress_bar.py:260, log())
[2021-11-14 21:31:26]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 21:31:26]    INFO >> AMP: overflow detected, setting scale to to 16.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 21:31:26]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 21:32:51]    INFO >> epoch 007:   7374 / 10106 loss=3.276, nll_loss=1.444, ppl=2.72, wps=4740.8, ups=5.21, wpb=909.9, bsz=4, num_updates=68000, lr=4.7e-05, gnorm=2.306, clip=0, loss_scale=16, train_wall=95, gb_free=29.4, wall=16592 (progress_bar.py:260, log())
[2021-11-14 21:34:25]    INFO >> epoch 007:   7874 / 10106 loss=3.229, nll_loss=1.392, ppl=2.62, wps=5010.5, ups=5.33, wpb=940.2, bsz=4, num_updates=68500, lr=4.7e-05, gnorm=2.266, clip=0, loss_scale=16, train_wall=93, gb_free=26.2, wall=16686 (progress_bar.py:260, log())
[2021-11-14 21:35:58]    INFO >> epoch 007:   8374 / 10106 loss=3.229, nll_loss=1.392, ppl=2.62, wps=4954.4, ups=5.37, wpb=922.7, bsz=4, num_updates=69000, lr=4.7e-05, gnorm=2.277, clip=0, loss_scale=16, train_wall=92, gb_free=28.1, wall=16779 (progress_bar.py:260, log())
[2021-11-14 21:37:31]    INFO >> epoch 007:   8874 / 10106 loss=3.255, nll_loss=1.422, ppl=2.68, wps=4881.2, ups=5.36, wpb=911.3, bsz=4, num_updates=69500, lr=4.7e-05, gnorm=2.285, clip=0, loss_scale=16, train_wall=92, gb_free=27.9, wall=16872 (progress_bar.py:260, log())
[2021-11-14 21:39:04]    INFO >> epoch 007:   9374 / 10106 loss=3.234, nll_loss=1.398, ppl=2.64, wps=4786.8, ups=5.38, wpb=890.3, bsz=4, num_updates=70000, lr=4.7e-05, gnorm=2.265, clip=0, loss_scale=32, train_wall=92, gb_free=28.5, wall=16965 (progress_bar.py:260, log())
[2021-11-14 21:40:40]    INFO >> epoch 007:   9874 / 10106 loss=3.237, nll_loss=1.401, ppl=2.64, wps=4819.3, ups=5.25, wpb=917.8, bsz=4, num_updates=70500, lr=4.7e-05, gnorm=2.211, clip=0, loss_scale=32, train_wall=94, gb_free=28.2, wall=17060 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 21:41:40]    INFO >> epoch 007 | loss 3.257 | nll_loss 1.422 | ppl 2.68 | wps 4089.2 | ups 4.43 | wpb 922.1 | bsz 4 | num_updates 70732 | lr 4.7e-05 | gnorm 2.33 | clip 0 | loss_scale 32 | train_wall 1763 | gb_free 27.2 | wall 17120 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 21:48:45]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.733 | nll_loss 1.91 | ppl 3.76 | bleu 19.7969 | wps 438.9 | wpb 3457.3 | bsz 16 | num_updates 70732 | best_bleu 20.9941 (progress_bar.py:269, print())
[2021-11-14 21:48:52]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 7 @ 70732 updates, score 19.796902) (writing took 7.055511 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 21:49:55]    INFO >> epoch 008:    268 / 10106 loss=3.179, nll_loss=1.335, ppl=2.52, wps=849.3, ups=0.9, wpb=942.9, bsz=4, num_updates=71000, lr=4.7e-05, gnorm=2.146, clip=0, loss_scale=32, train_wall=99, gb_free=29.3, wall=17615 (progress_bar.py:260, log())
[2021-11-14 21:51:31]    INFO >> epoch 008:    768 / 10106 loss=3.2, nll_loss=1.358, ppl=2.56, wps=4784.9, ups=5.18, wpb=923.2, bsz=4, num_updates=71500, lr=4.6e-05, gnorm=2.153, clip=0, loss_scale=32, train_wall=95, gb_free=27.7, wall=17712 (progress_bar.py:260, log())
[2021-11-14 21:51:46]    INFO >> AMP: overflow detected, setting scale to to 32.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 21:53:04]    INFO >> epoch 008:   1268 / 10106 loss=3.169, nll_loss=1.325, ppl=2.51, wps=4894.3, ups=5.41, wpb=904.7, bsz=4, num_updates=72000, lr=4.6e-05, gnorm=2.173, clip=0, loss_scale=32, train_wall=91, gb_free=27.3, wall=17804 (progress_bar.py:260, log())
[2021-11-14 21:54:39]    INFO >> epoch 008:   1768 / 10106 loss=3.186, nll_loss=1.342, ppl=2.54, wps=4849.1, ups=5.26, wpb=921.6, bsz=4, num_updates=72500, lr=4.6e-05, gnorm=2.155, clip=0, loss_scale=32, train_wall=94, gb_free=28.6, wall=17899 (progress_bar.py:260, log())
[2021-11-14 21:56:13]    INFO >> epoch 008:   2268 / 10106 loss=3.186, nll_loss=1.341, ppl=2.53, wps=4995.8, ups=5.28, wpb=946.9, bsz=4, num_updates=73000, lr=4.6e-05, gnorm=2.147, clip=0, loss_scale=32, train_wall=94, gb_free=26.7, wall=17994 (progress_bar.py:260, log())
[2021-11-14 21:57:52]    INFO >> epoch 008:   2768 / 10106 loss=3.204, nll_loss=1.362, ppl=2.57, wps=4860.2, ups=5.08, wpb=956.3, bsz=4, num_updates=73500, lr=4.6e-05, gnorm=2.139, clip=0, loss_scale=32, train_wall=97, gb_free=29.4, wall=18092 (progress_bar.py:260, log())
[2021-11-14 21:59:21]    INFO >> epoch 008:   3268 / 10106 loss=3.194, nll_loss=1.352, ppl=2.55, wps=5120.7, ups=5.61, wpb=912, bsz=4, num_updates=74000, lr=4.6e-05, gnorm=2.115, clip=0, loss_scale=64, train_wall=88, gb_free=29.2, wall=18181 (progress_bar.py:260, log())
[2021-11-14 22:00:36]    INFO >> epoch 008:   3768 / 10106 loss=3.146, nll_loss=1.298, ppl=2.46, wps=6081.1, ups=6.65, wpb=915, bsz=4, num_updates=74500, lr=4.6e-05, gnorm=2.097, clip=0, loss_scale=64, train_wall=74, gb_free=29.1, wall=18257 (progress_bar.py:260, log())
[2021-11-14 22:01:47]    INFO >> epoch 008:   4268 / 10106 loss=3.124, nll_loss=1.276, ppl=2.42, wps=6330.9, ups=7, wpb=904, bsz=4, num_updates=75000, lr=4.6e-05, gnorm=2.027, clip=0, loss_scale=64, train_wall=70, gb_free=28.5, wall=18328 (progress_bar.py:260, log())
[2021-11-14 22:02:56]    INFO >> epoch 008:   4768 / 10106 loss=3.169, nll_loss=1.325, ppl=2.51, wps=6691.9, ups=7.26, wpb=921.5, bsz=4, num_updates=75500, lr=4.6e-05, gnorm=2.076, clip=0, loss_scale=64, train_wall=68, gb_free=27.1, wall=18397 (progress_bar.py:260, log())
[2021-11-14 22:04:05]    INFO >> epoch 008:   5268 / 10106 loss=3.111, nll_loss=1.26, ppl=2.39, wps=6981.4, ups=7.32, wpb=953.9, bsz=4, num_updates=76000, lr=4.6e-05, gnorm=2, clip=0, loss_scale=128, train_wall=67, gb_free=29.3, wall=18465 (progress_bar.py:260, log())
[2021-11-14 22:05:18]    INFO >> epoch 008:   5768 / 10106 loss=3.11, nll_loss=1.26, ppl=2.39, wps=5909.1, ups=6.83, wpb=865.2, bsz=4, num_updates=76500, lr=4.6e-05, gnorm=2.012, clip=0, loss_scale=128, train_wall=72, gb_free=29.3, wall=18538 (progress_bar.py:260, log())
[2021-11-14 22:06:57]    INFO >> epoch 008:   6268 / 10106 loss=3.116, nll_loss=1.266, ppl=2.4, wps=4630.5, ups=5.07, wpb=914, bsz=4, num_updates=77000, lr=4.6e-05, gnorm=2.016, clip=0, loss_scale=128, train_wall=98, gb_free=28.2, wall=18637 (progress_bar.py:260, log())
[2021-11-14 22:07:03]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:08:31]    INFO >> epoch 008:   6768 / 10106 loss=3.156, nll_loss=1.311, ppl=2.48, wps=4835.4, ups=5.3, wpb=912.2, bsz=4, num_updates=77500, lr=4.6e-05, gnorm=2.056, clip=0, loss_scale=64, train_wall=93, gb_free=28.2, wall=18731 (progress_bar.py:260, log())
[2021-11-14 22:10:06]    INFO >> epoch 008:   7268 / 10106 loss=3.138, nll_loss=1.292, ppl=2.45, wps=4849.7, ups=5.27, wpb=919.9, bsz=4, num_updates=78000, lr=4.6e-05, gnorm=2.043, clip=0, loss_scale=64, train_wall=94, gb_free=28.6, wall=18826 (progress_bar.py:260, log())
[2021-11-14 22:11:40]    INFO >> epoch 008:   7768 / 10106 loss=3.157, nll_loss=1.311, ppl=2.48, wps=5002.3, ups=5.29, wpb=945.7, bsz=4, num_updates=78500, lr=4.6e-05, gnorm=1.962, clip=0, loss_scale=64, train_wall=93, gb_free=27.4, wall=18921 (progress_bar.py:260, log())
[2021-11-14 22:13:15]    INFO >> epoch 008:   8268 / 10106 loss=3.152, nll_loss=1.306, ppl=2.47, wps=4972.8, ups=5.27, wpb=943.9, bsz=4, num_updates=79000, lr=4.6e-05, gnorm=2.019, clip=0, loss_scale=64, train_wall=94, gb_free=29.3, wall=19016 (progress_bar.py:260, log())
[2021-11-14 22:14:33]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:14:33]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 22:14:48]    INFO >> epoch 008:   8769 / 10106 loss=3.113, nll_loss=1.264, ppl=2.4, wps=4887.2, ups=5.4, wpb=904.4, bsz=4, num_updates=79500, lr=4.6e-05, gnorm=2.004, clip=0, loss_scale=64, train_wall=91, gb_free=28.3, wall=19108 (progress_bar.py:260, log())
[2021-11-14 22:16:20]    INFO >> epoch 008:   9269 / 10106 loss=3.113, nll_loss=1.263, ppl=2.4, wps=4859.7, ups=5.42, wpb=895.9, bsz=4, num_updates=80000, lr=4.6e-05, gnorm=2.008, clip=0, loss_scale=64, train_wall=91, gb_free=26.1, wall=19200 (progress_bar.py:260, log())
[2021-11-14 22:17:58]    INFO >> epoch 008:   9769 / 10106 loss=3.17, nll_loss=1.326, ppl=2.51, wps=4874.3, ups=5.08, wpb=959.4, bsz=4, num_updates=80500, lr=4.6e-05, gnorm=2.012, clip=0, loss_scale=64, train_wall=97, gb_free=28.8, wall=19299 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 22:19:19]    INFO >> epoch 008 | loss 3.151 | nll_loss 1.305 | ppl 2.47 | wps 4124.3 | ups 4.47 | wpb 922.3 | bsz 4 | num_updates 80837 | lr 4.6e-05 | gnorm 2.06 | clip 0 | loss_scale 64 | train_wall 1783 | gb_free 27.7 | wall 19380 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 22:27:24]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.656 | nll_loss 1.857 | ppl 3.62 | bleu 23.5696 | wps 385.4 | wpb 3457.3 | bsz 16 | num_updates 80837 | best_bleu 23.5696 (progress_bar.py:269, print())
[2021-11-14 22:27:35]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_best.pt (epoch 8 @ 80837 updates, score 23.569616) (writing took 10.996692 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 22:28:17]    INFO >> epoch 009:    163 / 10106 loss=3.101, nll_loss=1.248, ppl=2.38, wps=766.9, ups=0.81, wpb=948.5, bsz=4, num_updates=81000, lr=4.6e-05, gnorm=1.893, clip=0, loss_scale=64, train_wall=99, gb_free=29.1, wall=19917 (progress_bar.py:260, log())
[2021-11-14 22:29:49]    INFO >> epoch 009:    663 / 10106 loss=3.085, nll_loss=1.228, ppl=2.34, wps=4835.8, ups=5.44, wpb=889.7, bsz=4, num_updates=81500, lr=4.6e-05, gnorm=1.961, clip=0, loss_scale=128, train_wall=91, gb_free=28.9, wall=20009 (progress_bar.py:260, log())
[2021-11-14 22:31:27]    INFO >> epoch 009:   1163 / 10106 loss=3.081, nll_loss=1.224, ppl=2.34, wps=4753.8, ups=5.08, wpb=935.2, bsz=4, num_updates=82000, lr=4.6e-05, gnorm=1.889, clip=0, loss_scale=128, train_wall=97, gb_free=28.5, wall=20108 (progress_bar.py:260, log())
[2021-11-14 22:33:01]    INFO >> epoch 009:   1663 / 10106 loss=3.014, nll_loss=1.151, ppl=2.22, wps=4989.3, ups=5.34, wpb=934.6, bsz=4, num_updates=82500, lr=4.6e-05, gnorm=1.822, clip=0, loss_scale=128, train_wall=93, gb_free=27.6, wall=20201 (progress_bar.py:260, log())
[2021-11-14 22:34:32]    INFO >> epoch 009:   2163 / 10106 loss=3.084, nll_loss=1.229, ppl=2.34, wps=5042.1, ups=5.5, wpb=917.5, bsz=4, num_updates=83000, lr=4.6e-05, gnorm=1.892, clip=0, loss_scale=128, train_wall=90, gb_free=26.8, wall=20292 (progress_bar.py:260, log())
[2021-11-14 22:35:31]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:35:41]    INFO >> epoch 009:   2663 / 10106 loss=3.094, nll_loss=1.24, ppl=2.36, wps=6690.9, ups=7.2, wpb=929.9, bsz=4, num_updates=83500, lr=4.6e-05, gnorm=1.882, clip=0, loss_scale=128, train_wall=68, gb_free=28.9, wall=20362 (progress_bar.py:260, log())
[2021-11-14 22:36:50]    INFO >> epoch 009:   3163 / 10106 loss=3.059, nll_loss=1.201, ppl=2.3, wps=6733.2, ups=7.29, wpb=924.1, bsz=4, num_updates=84000, lr=4.6e-05, gnorm=1.884, clip=0, loss_scale=128, train_wall=68, gb_free=29.4, wall=20430 (progress_bar.py:260, log())
[2021-11-14 22:37:59]    INFO >> epoch 009:   3663 / 10106 loss=3.054, nll_loss=1.195, ppl=2.29, wps=6601, ups=7.18, wpb=919.8, bsz=4, num_updates=84500, lr=4.6e-05, gnorm=1.876, clip=0, loss_scale=128, train_wall=69, gb_free=27.5, wall=20500 (progress_bar.py:260, log())
[2021-11-14 22:38:47]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:39:10]    INFO >> epoch 009:   4163 / 10106 loss=3.08, nll_loss=1.224, ppl=2.34, wps=6402.4, ups=7.07, wpb=906.2, bsz=4, num_updates=85000, lr=4.6e-05, gnorm=1.917, clip=0, loss_scale=64, train_wall=70, gb_free=27.3, wall=20571 (progress_bar.py:260, log())
[2021-11-14 22:40:24]    INFO >> epoch 009:   4663 / 10106 loss=3.108, nll_loss=1.255, ppl=2.39, wps=6326.5, ups=6.78, wpb=933.6, bsz=4, num_updates=85500, lr=4.6e-05, gnorm=1.922, clip=0, loss_scale=64, train_wall=73, gb_free=28.5, wall=20645 (progress_bar.py:260, log())
[2021-11-14 22:42:01]    INFO >> epoch 009:   5163 / 10106 loss=3.08, nll_loss=1.225, ppl=2.34, wps=4711.5, ups=5.16, wpb=912.2, bsz=4, num_updates=86000, lr=4.6e-05, gnorm=1.917, clip=0, loss_scale=64, train_wall=96, gb_free=29, wall=20741 (progress_bar.py:260, log())
[2021-11-14 22:43:34]    INFO >> epoch 009:   5663 / 10106 loss=3.085, nll_loss=1.231, ppl=2.35, wps=4857.6, ups=5.35, wpb=908.8, bsz=4, num_updates=86500, lr=4.6e-05, gnorm=1.902, clip=0, loss_scale=64, train_wall=93, gb_free=29.1, wall=20835 (progress_bar.py:260, log())
[2021-11-14 22:45:07]    INFO >> epoch 009:   6163 / 10106 loss=3.055, nll_loss=1.198, ppl=2.29, wps=4997.1, ups=5.42, wpb=921.4, bsz=4, num_updates=87000, lr=4.6e-05, gnorm=1.836, clip=0, loss_scale=128, train_wall=91, gb_free=27.6, wall=20927 (progress_bar.py:260, log())
[2021-11-14 22:46:40]    INFO >> epoch 009:   6663 / 10106 loss=3.07, nll_loss=1.215, ppl=2.32, wps=4930.3, ups=5.34, wpb=922.9, bsz=4, num_updates=87500, lr=4.6e-05, gnorm=1.839, clip=0, loss_scale=128, train_wall=93, gb_free=29.3, wall=21021 (progress_bar.py:260, log())
[2021-11-14 22:48:07]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:48:07]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 22:48:14]    INFO >> epoch 009:   7164 / 10106 loss=3.048, nll_loss=1.189, ppl=2.28, wps=4932.5, ups=5.33, wpb=924.6, bsz=4, num_updates=88000, lr=4.6e-05, gnorm=1.827, clip=0, loss_scale=64, train_wall=93, gb_free=29.1, wall=21114 (progress_bar.py:260, log())
[2021-11-14 22:49:46]    INFO >> epoch 009:   7664 / 10106 loss=3.084, nll_loss=1.23, ppl=2.35, wps=4947.2, ups=5.44, wpb=908.8, bsz=4, num_updates=88500, lr=4.6e-05, gnorm=1.87, clip=0, loss_scale=64, train_wall=91, gb_free=29.2, wall=21206 (progress_bar.py:260, log())
[2021-11-14 22:51:20]    INFO >> epoch 009:   8164 / 10106 loss=3.103, nll_loss=1.253, ppl=2.38, wps=4997.6, ups=5.3, wpb=942.9, bsz=4, num_updates=89000, lr=4.6e-05, gnorm=1.856, clip=0, loss_scale=64, train_wall=93, gb_free=26, wall=21301 (progress_bar.py:260, log())
[2021-11-14 22:52:54]    INFO >> epoch 009:   8664 / 10106 loss=3.087, nll_loss=1.232, ppl=2.35, wps=4955.6, ups=5.33, wpb=929.5, bsz=4, num_updates=89500, lr=4.6e-05, gnorm=1.857, clip=0, loss_scale=64, train_wall=93, gb_free=27.3, wall=21394 (progress_bar.py:260, log())
[2021-11-14 22:54:28]    INFO >> epoch 009:   9164 / 10106 loss=3.046, nll_loss=1.187, ppl=2.28, wps=4980.2, ups=5.3, wpb=939.9, bsz=4, num_updates=90000, lr=4.6e-05, gnorm=1.813, clip=0, loss_scale=128, train_wall=93, gb_free=28, wall=21489 (progress_bar.py:260, log())
[2021-11-14 22:55:21]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 22:56:03]    INFO >> epoch 009:   9664 / 10106 loss=3.047, nll_loss=1.19, ppl=2.28, wps=4815.9, ups=5.26, wpb=915.4, bsz=4, num_updates=90500, lr=4.6e-05, gnorm=1.816, clip=0, loss_scale=64, train_wall=94, gb_free=28.4, wall=21584 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 22:57:40]    INFO >> epoch 009 | loss 3.073 | nll_loss 1.217 | ppl 2.32 | wps 4049.9 | ups 4.39 | wpb 922.2 | bsz 4 | num_updates 90942 | lr 4.6e-05 | gnorm 1.871 | clip 0 | loss_scale 64 | train_wall 1761 | gb_free 28.4 | wall 21681 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 23:03:51]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.62 | nll_loss 1.817 | ppl 3.52 | bleu 14.5928 | wps 506.5 | wpb 3457.3 | bsz 16 | num_updates 90942 | best_bleu 23.5696 (progress_bar.py:269, print())
[2021-11-14 23:03:58]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 9 @ 90942 updates, score 14.592803) (writing took 6.722428 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 23:04:17]    INFO >> epoch 010:     58 / 10106 loss=3.084, nll_loss=1.23, ppl=2.35, wps=922.7, ups=1.01, wpb=910.3, bsz=4, num_updates=91000, lr=4.6e-05, gnorm=1.845, clip=0, loss_scale=64, train_wall=92, gb_free=26.9, wall=22077 (progress_bar.py:260, log())
[2021-11-14 23:05:50]    INFO >> epoch 010:    558 / 10106 loss=3.025, nll_loss=1.162, ppl=2.24, wps=4951.9, ups=5.36, wpb=924.2, bsz=4, num_updates=91500, lr=4.5e-05, gnorm=1.8, clip=0, loss_scale=64, train_wall=92, gb_free=28.2, wall=22170 (progress_bar.py:260, log())
[2021-11-14 23:07:23]    INFO >> epoch 010:   1058 / 10106 loss=3.03, nll_loss=1.168, ppl=2.25, wps=5065.9, ups=5.39, wpb=939, bsz=4, num_updates=92000, lr=4.5e-05, gnorm=1.794, clip=0, loss_scale=64, train_wall=92, gb_free=26.9, wall=22263 (progress_bar.py:260, log())
[2021-11-14 23:08:55]    INFO >> epoch 010:   1558 / 10106 loss=3.031, nll_loss=1.169, ppl=2.25, wps=4860, ups=5.41, wpb=898.3, bsz=4, num_updates=92500, lr=4.5e-05, gnorm=1.801, clip=0, loss_scale=128, train_wall=91, gb_free=28.3, wall=22356 (progress_bar.py:260, log())
[2021-11-14 23:09:50]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 23:10:26]    INFO >> epoch 010:   2058 / 10106 loss=3.008, nll_loss=1.145, ppl=2.21, wps=5068.7, ups=5.49, wpb=922.7, bsz=4, num_updates=93000, lr=4.5e-05, gnorm=1.776, clip=0, loss_scale=64, train_wall=90, gb_free=28.6, wall=22447 (progress_bar.py:260, log())
[2021-11-14 23:11:36]    INFO >> epoch 010:   2558 / 10106 loss=3.056, nll_loss=1.198, ppl=2.29, wps=7119.6, ups=7.13, wpb=998.1, bsz=4, num_updates=93500, lr=4.5e-05, gnorm=1.773, clip=0, loss_scale=64, train_wall=69, gb_free=28, wall=22517 (progress_bar.py:260, log())
[2021-11-14 23:12:45]    INFO >> epoch 010:   3058 / 10106 loss=3.031, nll_loss=1.169, ppl=2.25, wps=6881.4, ups=7.25, wpb=949.4, bsz=4, num_updates=94000, lr=4.5e-05, gnorm=1.775, clip=0, loss_scale=64, train_wall=68, gb_free=29.3, wall=22586 (progress_bar.py:260, log())
[2021-11-14 23:13:57]    INFO >> epoch 010:   3558 / 10106 loss=3.036, nll_loss=1.175, ppl=2.26, wps=6285.6, ups=6.95, wpb=904.1, bsz=4, num_updates=94500, lr=4.5e-05, gnorm=1.803, clip=0, loss_scale=64, train_wall=71, gb_free=29.3, wall=22658 (progress_bar.py:260, log())
[2021-11-14 23:15:14]    INFO >> epoch 010:   4058 / 10106 loss=3.021, nll_loss=1.159, ppl=2.23, wps=5875.4, ups=6.47, wpb=908.7, bsz=4, num_updates=95000, lr=4.5e-05, gnorm=1.798, clip=0, loss_scale=128, train_wall=76, gb_free=28, wall=22735 (progress_bar.py:260, log())
[2021-11-14 23:15:26]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 23:15:26]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 23:16:24]    INFO >> epoch 010:   4559 / 10106 loss=3.031, nll_loss=1.169, ppl=2.25, wps=6876.5, ups=7.19, wpb=956.3, bsz=4, num_updates=95500, lr=4.5e-05, gnorm=1.762, clip=0, loss_scale=64, train_wall=69, gb_free=29.2, wall=22804 (progress_bar.py:260, log())
[2021-11-14 23:17:56]    INFO >> epoch 010:   5059 / 10106 loss=2.999, nll_loss=1.133, ppl=2.19, wps=4948.7, ups=5.42, wpb=913, bsz=4, num_updates=96000, lr=4.5e-05, gnorm=1.779, clip=0, loss_scale=64, train_wall=91, gb_free=29.1, wall=22897 (progress_bar.py:260, log())
[2021-11-14 23:19:29]    INFO >> epoch 010:   5559 / 10106 loss=3.052, nll_loss=1.194, ppl=2.29, wps=5005.8, ups=5.38, wpb=930.9, bsz=4, num_updates=96500, lr=4.5e-05, gnorm=1.821, clip=0, loss_scale=64, train_wall=92, gb_free=29, wall=22990 (progress_bar.py:260, log())
[2021-11-14 23:21:02]    INFO >> epoch 010:   6059 / 10106 loss=3.023, nll_loss=1.161, ppl=2.24, wps=4883.8, ups=5.36, wpb=910.6, bsz=4, num_updates=97000, lr=4.5e-05, gnorm=1.754, clip=0, loss_scale=64, train_wall=92, gb_free=26.9, wall=23083 (progress_bar.py:260, log())
[2021-11-14 23:22:36]    INFO >> epoch 010:   6559 / 10106 loss=2.993, nll_loss=1.128, ppl=2.19, wps=4836.5, ups=5.35, wpb=903.7, bsz=4, num_updates=97500, lr=4.5e-05, gnorm=1.719, clip=0, loss_scale=128, train_wall=92, gb_free=27.5, wall=23176 (progress_bar.py:260, log())
[2021-11-14 23:23:31]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 23:24:11]    INFO >> epoch 010:   7059 / 10106 loss=3.008, nll_loss=1.145, ppl=2.21, wps=4936, ups=5.27, wpb=937.2, bsz=4, num_updates=98000, lr=4.5e-05, gnorm=1.723, clip=0, loss_scale=64, train_wall=94, gb_free=28.7, wall=23271 (progress_bar.py:260, log())
[2021-11-14 23:25:44]    INFO >> epoch 010:   7559 / 10106 loss=3.018, nll_loss=1.156, ppl=2.23, wps=4941.4, ups=5.36, wpb=921.5, bsz=4, num_updates=98500, lr=4.5e-05, gnorm=1.745, clip=0, loss_scale=64, train_wall=92, gb_free=27.1, wall=23365 (progress_bar.py:260, log())
[2021-11-14 23:27:18]    INFO >> epoch 010:   8059 / 10106 loss=3.044, nll_loss=1.184, ppl=2.27, wps=4875.5, ups=5.31, wpb=917.4, bsz=4, num_updates=99000, lr=4.5e-05, gnorm=1.785, clip=0, loss_scale=64, train_wall=93, gb_free=29.4, wall=23459 (progress_bar.py:260, log())
[2021-11-14 23:28:52]    INFO >> epoch 010:   8559 / 10106 loss=3.002, nll_loss=1.138, ppl=2.2, wps=4992.2, ups=5.33, wpb=936, bsz=4, num_updates=99500, lr=4.5e-05, gnorm=1.725, clip=0, loss_scale=64, train_wall=93, gb_free=29, wall=23552 (progress_bar.py:260, log())
[2021-11-14 23:30:23]    INFO >> epoch 010:   9059 / 10106 loss=2.989, nll_loss=1.125, ppl=2.18, wps=4669.7, ups=5.47, wpb=853.2, bsz=4, num_updates=100000, lr=4.5e-05, gnorm=1.752, clip=0, loss_scale=128, train_wall=90, gb_free=29.3, wall=23644 (progress_bar.py:260, log())
[2021-11-14 23:31:56]    INFO >> epoch 010:   9559 / 10106 loss=2.988, nll_loss=1.123, ppl=2.18, wps=4794.9, ups=5.39, wpb=889, bsz=4, num_updates=100500, lr=4.5e-05, gnorm=1.727, clip=0, loss_scale=128, train_wall=92, gb_free=26.8, wall=23736 (progress_bar.py:260, log())
[2021-11-14 23:33:31]    INFO >> epoch 010:  10059 / 10106 loss=3, nll_loss=1.135, ppl=2.2, wps=4949.7, ups=5.26, wpb=940.3, bsz=4, num_updates=101000, lr=4.5e-05, gnorm=1.678, clip=0, loss_scale=128, train_wall=94, gb_free=27.5, wall=23831 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 23:33:54]    INFO >> epoch 010 | loss 3.02 | nll_loss 1.157 | ppl 2.23 | wps 4287.2 | ups 4.65 | wpb 922.2 | bsz 4 | num_updates 101047 | lr 4.5e-05 | gnorm 1.764 | clip 0 | loss_scale 128 | train_wall 1752 | gb_free 29.3 | wall 23855 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-14 23:39:57]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.566 | nll_loss 1.772 | ppl 3.42 | bleu 20.8916 | wps 511.8 | wpb 3457.3 | bsz 16 | num_updates 101047 | best_bleu 23.5696 (progress_bar.py:269, print())
[2021-11-14 23:40:04]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 10 @ 101047 updates, score 20.891639) (writing took 6.997255 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-14 23:41:37]    INFO >> epoch 011:    453 / 10106 loss=2.958, nll_loss=1.087, ppl=2.12, wps=936.9, ups=1.03, wpb=910.4, bsz=4, num_updates=101500, lr=4.5e-05, gnorm=1.641, clip=0, loss_scale=128, train_wall=92, gb_free=28, wall=24317 (progress_bar.py:260, log())
[2021-11-14 23:42:32]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 23:43:11]    INFO >> epoch 011:    953 / 10106 loss=2.979, nll_loss=1.111, ppl=2.16, wps=4965.4, ups=5.28, wpb=940.6, bsz=4, num_updates=102000, lr=4.5e-05, gnorm=1.669, clip=0, loss_scale=128, train_wall=94, gb_free=26.9, wall=24412 (progress_bar.py:260, log())
[2021-11-14 23:44:46]    INFO >> epoch 011:   1453 / 10106 loss=2.955, nll_loss=1.085, ppl=2.12, wps=4886.9, ups=5.28, wpb=926, bsz=4, num_updates=102500, lr=4.5e-05, gnorm=1.632, clip=0, loss_scale=128, train_wall=94, gb_free=26.4, wall=24507 (progress_bar.py:260, log())
[2021-11-14 23:46:10]    INFO >> epoch 011:   1953 / 10106 loss=2.965, nll_loss=1.094, ppl=2.14, wps=5421.4, ups=5.96, wpb=909.5, bsz=4, num_updates=103000, lr=4.5e-05, gnorm=1.671, clip=0, loss_scale=128, train_wall=83, gb_free=28, wall=24591 (progress_bar.py:260, log())
[2021-11-14 23:46:50]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 23:46:50]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-14 23:47:19]    INFO >> epoch 011:   2454 / 10106 loss=2.985, nll_loss=1.116, ppl=2.17, wps=6769, ups=7.2, wpb=939.8, bsz=4, num_updates=103500, lr=4.5e-05, gnorm=1.703, clip=0, loss_scale=64, train_wall=68, gb_free=28, wall=24660 (progress_bar.py:260, log())
[2021-11-14 23:48:26]    INFO >> epoch 011:   2954 / 10106 loss=2.965, nll_loss=1.096, ppl=2.14, wps=6767.1, ups=7.47, wpb=906.1, bsz=4, num_updates=104000, lr=4.5e-05, gnorm=1.699, clip=0, loss_scale=64, train_wall=66, gb_free=26.4, wall=24727 (progress_bar.py:260, log())
[2021-11-14 23:49:35]    INFO >> epoch 011:   3454 / 10106 loss=2.988, nll_loss=1.12, ppl=2.17, wps=6538.7, ups=7.25, wpb=901.5, bsz=4, num_updates=104500, lr=4.5e-05, gnorm=1.718, clip=0, loss_scale=64, train_wall=68, gb_free=28.6, wall=24796 (progress_bar.py:260, log())
[2021-11-14 23:50:43]    INFO >> epoch 011:   3954 / 10106 loss=2.956, nll_loss=1.085, ppl=2.12, wps=6524.4, ups=7.36, wpb=886.4, bsz=4, num_updates=105000, lr=4.5e-05, gnorm=1.704, clip=0, loss_scale=64, train_wall=67, gb_free=25.9, wall=24864 (progress_bar.py:260, log())
[2021-11-14 23:51:40]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-14 23:51:52]    INFO >> epoch 011:   4454 / 10106 loss=2.988, nll_loss=1.122, ppl=2.18, wps=6498.4, ups=7.3, wpb=890, bsz=4, num_updates=105500, lr=4.5e-05, gnorm=1.74, clip=0, loss_scale=64, train_wall=67, gb_free=29.4, wall=24932 (progress_bar.py:260, log())
[2021-11-14 23:53:24]    INFO >> epoch 011:   4954 / 10106 loss=2.996, nll_loss=1.13, ppl=2.19, wps=5272, ups=5.43, wpb=971.4, bsz=4, num_updates=106000, lr=4.5e-05, gnorm=1.651, clip=0, loss_scale=64, train_wall=91, gb_free=28.9, wall=25024 (progress_bar.py:260, log())
[2021-11-14 23:55:02]    INFO >> epoch 011:   5454 / 10106 loss=2.963, nll_loss=1.094, ppl=2.13, wps=4731.7, ups=5.1, wpb=928.6, bsz=4, num_updates=106500, lr=4.5e-05, gnorm=1.672, clip=0, loss_scale=64, train_wall=97, gb_free=29.4, wall=25123 (progress_bar.py:260, log())
[2021-11-14 23:56:35]    INFO >> epoch 011:   5954 / 10106 loss=2.962, nll_loss=1.093, ppl=2.13, wps=4746.5, ups=5.35, wpb=886.4, bsz=4, num_updates=107000, lr=4.5e-05, gnorm=1.72, clip=0, loss_scale=64, train_wall=92, gb_free=28.8, wall=25216 (progress_bar.py:260, log())
[2021-11-14 23:58:06]    INFO >> epoch 011:   6454 / 10106 loss=2.946, nll_loss=1.076, ppl=2.11, wps=4954, ups=5.5, wpb=901.2, bsz=4, num_updates=107500, lr=4.5e-05, gnorm=1.653, clip=0, loss_scale=128, train_wall=90, gb_free=29.3, wall=25307 (progress_bar.py:260, log())
[2021-11-14 23:59:42]    INFO >> epoch 011:   6954 / 10106 loss=2.965, nll_loss=1.095, ppl=2.14, wps=5053, ups=5.21, wpb=970.4, bsz=4, num_updates=108000, lr=4.5e-05, gnorm=1.629, clip=0, loss_scale=128, train_wall=95, gb_free=27.9, wall=25403 (progress_bar.py:260, log())
[2021-11-15 00:01:16]    INFO >> epoch 011:   7454 / 10106 loss=2.983, nll_loss=1.116, ppl=2.17, wps=4957.6, ups=5.36, wpb=925.3, bsz=4, num_updates=108500, lr=4.5e-05, gnorm=1.659, clip=0, loss_scale=128, train_wall=92, gb_free=28.8, wall=25496 (progress_bar.py:260, log())
[2021-11-15 00:01:42]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:02:52]    INFO >> epoch 011:   7954 / 10106 loss=2.991, nll_loss=1.125, ppl=2.18, wps=4820.6, ups=5.19, wpb=928.9, bsz=4, num_updates=109000, lr=4.5e-05, gnorm=1.698, clip=0, loss_scale=64, train_wall=95, gb_free=28.1, wall=25593 (progress_bar.py:260, log())
[2021-11-15 00:04:28]    INFO >> epoch 011:   8454 / 10106 loss=2.956, nll_loss=1.087, ppl=2.12, wps=4772.8, ups=5.23, wpb=912.1, bsz=4, num_updates=109500, lr=4.5e-05, gnorm=1.669, clip=0, loss_scale=64, train_wall=95, gb_free=29, wall=25688 (progress_bar.py:260, log())
[2021-11-15 00:06:02]    INFO >> epoch 011:   8954 / 10106 loss=2.979, nll_loss=1.112, ppl=2.16, wps=5046.2, ups=5.27, wpb=957.1, bsz=4, num_updates=110000, lr=4.5e-05, gnorm=1.662, clip=0, loss_scale=64, train_wall=94, gb_free=26.4, wall=25783 (progress_bar.py:260, log())
[2021-11-15 00:07:36]    INFO >> epoch 011:   9454 / 10106 loss=2.961, nll_loss=1.093, ppl=2.13, wps=4931.3, ups=5.36, wpb=920, bsz=4, num_updates=110500, lr=4.5e-05, gnorm=1.62, clip=0, loss_scale=64, train_wall=92, gb_free=29.3, wall=25876 (progress_bar.py:260, log())
[2021-11-15 00:09:11]    INFO >> epoch 011:   9954 / 10106 loss=2.93, nll_loss=1.059, ppl=2.08, wps=4873.6, ups=5.22, wpb=933.2, bsz=4, num_updates=111000, lr=4.5e-05, gnorm=1.631, clip=0, loss_scale=128, train_wall=95, gb_free=29.4, wall=25972 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 00:09:55]    INFO >> epoch 011 | loss 2.968 | nll_loss 1.1 | ppl 2.14 | wps 4312 | ups 4.68 | wpb 922.2 | bsz 4 | num_updates 111152 | lr 4.5e-05 | gnorm 1.672 | clip 0 | loss_scale 128 | train_wall 1747 | gb_free 29 | wall 26016 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 00:14:57]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.559 | nll_loss 1.758 | ppl 3.38 | bleu 17.0244 | wps 624.1 | wpb 3457.3 | bsz 16 | num_updates 111152 | best_bleu 23.5696 (progress_bar.py:269, print())
[2021-11-15 00:15:04]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 11 @ 111152 updates, score 17.024417) (writing took 7.059739 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 00:16:20]    INFO >> epoch 012:    348 / 10106 loss=2.927, nll_loss=1.052, ppl=2.07, wps=1091.2, ups=1.17, wpb=935.5, bsz=4, num_updates=111500, lr=4.4e-05, gnorm=1.606, clip=0, loss_scale=128, train_wall=96, gb_free=29.4, wall=26401 (progress_bar.py:260, log())
[2021-11-15 00:17:53]    INFO >> epoch 012:    848 / 10106 loss=2.939, nll_loss=1.065, ppl=2.09, wps=4971.1, ups=5.36, wpb=927.8, bsz=4, num_updates=112000, lr=4.4e-05, gnorm=1.597, clip=0, loss_scale=128, train_wall=92, gb_free=29.1, wall=26494 (progress_bar.py:260, log())
[2021-11-15 00:19:25]    INFO >> epoch 012:   1348 / 10106 loss=2.911, nll_loss=1.034, ppl=2.05, wps=4807.8, ups=5.47, wpb=878.7, bsz=4, num_updates=112500, lr=4.4e-05, gnorm=1.616, clip=0, loss_scale=128, train_wall=90, gb_free=28.7, wall=26585 (progress_bar.py:260, log())
[2021-11-15 00:20:58]    INFO >> epoch 012:   1848 / 10106 loss=2.908, nll_loss=1.031, ppl=2.04, wps=4910.7, ups=5.38, wpb=913, bsz=4, num_updates=113000, lr=4.4e-05, gnorm=1.576, clip=0, loss_scale=256, train_wall=92, gb_free=25.7, wall=26678 (progress_bar.py:260, log())
[2021-11-15 00:22:19]    INFO >> epoch 012:   2348 / 10106 loss=2.885, nll_loss=1.005, ppl=2.01, wps=5612.8, ups=6.19, wpb=906.5, bsz=4, num_updates=113500, lr=4.4e-05, gnorm=1.564, clip=0, loss_scale=256, train_wall=80, gb_free=29, wall=26759 (progress_bar.py:260, log())
[2021-11-15 00:22:35]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:22:35]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 00:23:27]    INFO >> epoch 012:   2849 / 10106 loss=2.917, nll_loss=1.042, ppl=2.06, wps=6693.8, ups=7.26, wpb=921.9, bsz=4, num_updates=114000, lr=4.4e-05, gnorm=1.591, clip=0, loss_scale=128, train_wall=68, gb_free=25.8, wall=26828 (progress_bar.py:260, log())
[2021-11-15 00:24:40]    INFO >> epoch 012:   3349 / 10106 loss=2.906, nll_loss=1.03, ppl=2.04, wps=6415.8, ups=6.87, wpb=934.2, bsz=4, num_updates=114500, lr=4.4e-05, gnorm=1.582, clip=0, loss_scale=128, train_wall=72, gb_free=29.4, wall=26901 (progress_bar.py:260, log())
[2021-11-15 00:25:51]    INFO >> epoch 012:   3849 / 10106 loss=2.921, nll_loss=1.045, ppl=2.06, wps=6450.4, ups=7.05, wpb=914.3, bsz=4, num_updates=115000, lr=4.4e-05, gnorm=1.611, clip=0, loss_scale=128, train_wall=70, gb_free=29.4, wall=26972 (progress_bar.py:260, log())
[2021-11-15 00:26:58]    INFO >> epoch 012:   4349 / 10106 loss=2.918, nll_loss=1.041, ppl=2.06, wps=6800.3, ups=7.53, wpb=903, bsz=4, num_updates=115500, lr=4.4e-05, gnorm=1.611, clip=0, loss_scale=128, train_wall=65, gb_free=28.5, wall=27038 (progress_bar.py:260, log())
[2021-11-15 00:28:12]    INFO >> epoch 012:   4849 / 10106 loss=2.9, nll_loss=1.023, ppl=2.03, wps=6179, ups=6.72, wpb=919, bsz=4, num_updates=116000, lr=4.4e-05, gnorm=1.552, clip=0, loss_scale=256, train_wall=73, gb_free=28.8, wall=27112 (progress_bar.py:260, log())
[2021-11-15 00:29:45]    INFO >> epoch 012:   5349 / 10106 loss=2.928, nll_loss=1.053, ppl=2.07, wps=4990.4, ups=5.34, wpb=933.8, bsz=4, num_updates=116500, lr=4.4e-05, gnorm=1.532, clip=0, loss_scale=256, train_wall=93, gb_free=28, wall=27206 (progress_bar.py:260, log())
[2021-11-15 00:31:19]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:31:24]    INFO >> epoch 012:   5849 / 10106 loss=2.909, nll_loss=1.035, ppl=2.05, wps=4684.6, ups=5.08, wpb=923.1, bsz=4, num_updates=117000, lr=4.4e-05, gnorm=1.565, clip=0, loss_scale=128, train_wall=97, gb_free=26.9, wall=27305 (progress_bar.py:260, log())
[2021-11-15 00:32:55]    INFO >> epoch 012:   6349 / 10106 loss=2.922, nll_loss=1.048, ppl=2.07, wps=5321.6, ups=5.5, wpb=967.5, bsz=4, num_updates=117500, lr=4.4e-05, gnorm=1.566, clip=0, loss_scale=128, train_wall=90, gb_free=29, wall=27395 (progress_bar.py:260, log())
[2021-11-15 00:34:28]    INFO >> epoch 012:   6849 / 10106 loss=2.931, nll_loss=1.056, ppl=2.08, wps=4999.8, ups=5.37, wpb=930.7, bsz=4, num_updates=118000, lr=4.4e-05, gnorm=1.591, clip=0, loss_scale=128, train_wall=92, gb_free=27.5, wall=27489 (progress_bar.py:260, log())
[2021-11-15 00:36:03]    INFO >> epoch 012:   7349 / 10106 loss=2.926, nll_loss=1.052, ppl=2.07, wps=4873.1, ups=5.25, wpb=927.9, bsz=4, num_updates=118500, lr=4.4e-05, gnorm=1.581, clip=0, loss_scale=128, train_wall=94, gb_free=27.4, wall=27584 (progress_bar.py:260, log())
[2021-11-15 00:37:36]    INFO >> epoch 012:   7849 / 10106 loss=2.897, nll_loss=1.022, ppl=2.03, wps=4902.8, ups=5.36, wpb=914.7, bsz=4, num_updates=119000, lr=4.4e-05, gnorm=1.563, clip=0, loss_scale=256, train_wall=92, gb_free=27.5, wall=27677 (progress_bar.py:260, log())
[2021-11-15 00:38:48]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:39:14]    INFO >> epoch 012:   8349 / 10106 loss=2.939, nll_loss=1.068, ppl=2.1, wps=4766.7, ups=5.11, wpb=933.3, bsz=4, num_updates=119500, lr=4.4e-05, gnorm=1.579, clip=0, loss_scale=128, train_wall=97, gb_free=28.6, wall=27775 (progress_bar.py:260, log())
[2021-11-15 00:40:48]    INFO >> epoch 012:   8849 / 10106 loss=2.891, nll_loss=1.013, ppl=2.02, wps=4822, ups=5.35, wpb=900.9, bsz=4, num_updates=120000, lr=4.4e-05, gnorm=1.568, clip=0, loss_scale=128, train_wall=92, gb_free=27.8, wall=27868 (progress_bar.py:260, log())
[2021-11-15 00:42:24]    INFO >> epoch 012:   9349 / 10106 loss=2.912, nll_loss=1.036, ppl=2.05, wps=4881.9, ups=5.2, wpb=938.4, bsz=4, num_updates=120500, lr=4.4e-05, gnorm=1.581, clip=0, loss_scale=128, train_wall=95, gb_free=28.1, wall=27964 (progress_bar.py:260, log())
[2021-11-15 00:43:57]    INFO >> epoch 012:   9849 / 10106 loss=2.933, nll_loss=1.061, ppl=2.09, wps=4811.6, ups=5.35, wpb=900, bsz=4, num_updates=121000, lr=4.4e-05, gnorm=1.614, clip=0, loss_scale=128, train_wall=92, gb_free=28.5, wall=28058 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 00:45:03]    INFO >> epoch 012 | loss 2.916 | nll_loss 1.041 | ppl 2.06 | wps 4420.6 | ups 4.79 | wpb 922.2 | bsz 4 | num_updates 121257 | lr 4.4e-05 | gnorm 1.581 | clip 0 | loss_scale 128 | train_wall 1755 | gb_free 28.8 | wall 28124 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 00:52:25]    INFO >> epoch 012 | valid on 'valid' subset | loss 3.541 | nll_loss 1.716 | ppl 3.28 | bleu 22.8269 | wps 421.5 | wpb 3457.3 | bsz 16 | num_updates 121257 | best_bleu 23.5696 (progress_bar.py:269, print())
[2021-11-15 00:52:32]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 12 @ 121257 updates, score 22.826885) (writing took 6.951202 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 00:53:26]    INFO >> epoch 013:    243 / 10106 loss=2.915, nll_loss=1.039, ppl=2.06, wps=819.8, ups=0.88, wpb=932.2, bsz=4, num_updates=121500, lr=4.4e-05, gnorm=1.581, clip=0, loss_scale=256, train_wall=96, gb_free=29, wall=28627 (progress_bar.py:260, log())
[2021-11-15 00:55:03]    INFO >> epoch 013:    743 / 10106 loss=2.874, nll_loss=0.993, ppl=1.99, wps=4794.2, ups=5.17, wpb=926.9, bsz=4, num_updates=122000, lr=4.4e-05, gnorm=1.506, clip=0, loss_scale=256, train_wall=96, gb_free=29.4, wall=28723 (progress_bar.py:260, log())
[2021-11-15 00:56:22]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 00:56:22]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 00:56:37]    INFO >> epoch 013:   1244 / 10106 loss=2.876, nll_loss=0.996, ppl=1.99, wps=4902.1, ups=5.27, wpb=929.4, bsz=4, num_updates=122500, lr=4.4e-05, gnorm=1.522, clip=0, loss_scale=128, train_wall=94, gb_free=29.3, wall=28818 (progress_bar.py:260, log())
[2021-11-15 00:57:48]    INFO >> epoch 013:   1744 / 10106 loss=2.874, nll_loss=0.991, ppl=1.99, wps=6721.2, ups=7.1, wpb=946.4, bsz=4, num_updates=123000, lr=4.4e-05, gnorm=1.519, clip=0, loss_scale=128, train_wall=69, gb_free=26.5, wall=28888 (progress_bar.py:260, log())
[2021-11-15 00:58:56]    INFO >> epoch 013:   2244 / 10106 loss=2.895, nll_loss=1.015, ppl=2.02, wps=6896.8, ups=7.29, wpb=945.6, bsz=4, num_updates=123500, lr=4.4e-05, gnorm=1.568, clip=0, loss_scale=128, train_wall=68, gb_free=28.1, wall=28957 (progress_bar.py:260, log())
[2021-11-15 01:00:07]    INFO >> epoch 013:   2744 / 10106 loss=2.873, nll_loss=0.992, ppl=1.99, wps=6376.4, ups=7.05, wpb=905.1, bsz=4, num_updates=124000, lr=4.4e-05, gnorm=1.547, clip=0, loss_scale=128, train_wall=70, gb_free=26.7, wall=29028 (progress_bar.py:260, log())
[2021-11-15 01:01:16]    INFO >> epoch 013:   3244 / 10106 loss=2.907, nll_loss=1.029, ppl=2.04, wps=6692.2, ups=7.28, wpb=919.8, bsz=4, num_updates=124500, lr=4.4e-05, gnorm=1.589, clip=0, loss_scale=256, train_wall=68, gb_free=28.1, wall=29097 (progress_bar.py:260, log())
[2021-11-15 01:02:24]    INFO >> epoch 013:   3744 / 10106 loss=2.88, nll_loss=1, ppl=2, wps=6858.5, ups=7.33, wpb=935.6, bsz=4, num_updates=125000, lr=4.4e-05, gnorm=1.521, clip=0, loss_scale=256, train_wall=67, gb_free=28.3, wall=29165 (progress_bar.py:260, log())
[2021-11-15 01:03:47]    INFO >> epoch 013:   4244 / 10106 loss=2.896, nll_loss=1.019, ppl=2.03, wps=5335.1, ups=6.05, wpb=881.9, bsz=4, num_updates=125500, lr=4.4e-05, gnorm=1.575, clip=0, loss_scale=256, train_wall=82, gb_free=25.9, wall=29248 (progress_bar.py:260, log())
[2021-11-15 01:05:20]    INFO >> epoch 013:   4744 / 10106 loss=2.845, nll_loss=0.961, ppl=1.95, wps=5045.2, ups=5.37, wpb=939.1, bsz=4, num_updates=126000, lr=4.4e-05, gnorm=1.48, clip=0, loss_scale=256, train_wall=92, gb_free=28.1, wall=29341 (progress_bar.py:260, log())
[2021-11-15 01:06:44]    INFO >> AMP: overflow detected, setting scale to to 256.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 01:06:53]    INFO >> epoch 013:   5244 / 10106 loss=2.873, nll_loss=0.992, ppl=1.99, wps=4985.3, ups=5.36, wpb=929.7, bsz=4, num_updates=126500, lr=4.4e-05, gnorm=1.513, clip=0, loss_scale=256, train_wall=92, gb_free=28.8, wall=29434 (progress_bar.py:260, log())
[2021-11-15 01:08:28]    INFO >> epoch 013:   5744 / 10106 loss=2.865, nll_loss=0.983, ppl=1.98, wps=4973.9, ups=5.29, wpb=941.1, bsz=4, num_updates=127000, lr=4.4e-05, gnorm=1.506, clip=0, loss_scale=256, train_wall=94, gb_free=27.6, wall=29528 (progress_bar.py:260, log())
[2021-11-15 01:10:03]    INFO >> epoch 013:   6244 / 10106 loss=2.894, nll_loss=1.015, ppl=2.02, wps=4897.3, ups=5.27, wpb=928.9, bsz=4, num_updates=127500, lr=4.4e-05, gnorm=1.525, clip=0, loss_scale=256, train_wall=94, gb_free=27.3, wall=29623 (progress_bar.py:260, log())
[2021-11-15 01:11:41]    INFO >> epoch 013:   6744 / 10106 loss=2.864, nll_loss=0.983, ppl=1.98, wps=4693.6, ups=5.09, wpb=922.4, bsz=4, num_updates=128000, lr=4.4e-05, gnorm=1.508, clip=0, loss_scale=256, train_wall=97, gb_free=29.4, wall=29722 (progress_bar.py:260, log())
[2021-11-15 01:13:15]    INFO >> epoch 013:   7244 / 10106 loss=2.862, nll_loss=0.979, ppl=1.97, wps=4918.8, ups=5.32, wpb=925, bsz=4, num_updates=128500, lr=4.4e-05, gnorm=1.493, clip=0, loss_scale=512, train_wall=93, gb_free=26.9, wall=29816 (progress_bar.py:260, log())
[2021-11-15 01:13:20]    INFO >> AMP: overflow detected, setting scale to to 256.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 01:13:35]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 01:13:35]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 01:14:52]    INFO >> epoch 013:   7745 / 10106 loss=2.923, nll_loss=1.047, ppl=2.07, wps=4903.5, ups=5.15, wpb=952.7, bsz=4, num_updates=129000, lr=4.4e-05, gnorm=1.548, clip=0, loss_scale=128, train_wall=96, gb_free=27.2, wall=29913 (progress_bar.py:260, log())
[2021-11-15 01:16:28]    INFO >> epoch 013:   8245 / 10106 loss=2.895, nll_loss=1.017, ppl=2.02, wps=4653.5, ups=5.24, wpb=887.9, bsz=4, num_updates=129500, lr=4.4e-05, gnorm=1.605, clip=0, loss_scale=128, train_wall=94, gb_free=28.6, wall=30008 (progress_bar.py:260, log())
[2021-11-15 01:18:02]    INFO >> epoch 013:   8745 / 10106 loss=2.883, nll_loss=1.003, ppl=2, wps=4892.2, ups=5.31, wpb=921.7, bsz=4, num_updates=130000, lr=4.4e-05, gnorm=1.566, clip=0, loss_scale=128, train_wall=93, gb_free=29.4, wall=30102 (progress_bar.py:260, log())
[2021-11-15 01:19:36]    INFO >> epoch 013:   9245 / 10106 loss=2.895, nll_loss=1.018, ppl=2.02, wps=4651.8, ups=5.31, wpb=875.2, bsz=4, num_updates=130500, lr=4.4e-05, gnorm=1.57, clip=0, loss_scale=128, train_wall=93, gb_free=29.3, wall=30196 (progress_bar.py:260, log())
[2021-11-15 01:21:09]    INFO >> epoch 013:   9745 / 10106 loss=2.887, nll_loss=1.007, ppl=2.01, wps=4823.7, ups=5.35, wpb=901.9, bsz=4, num_updates=131000, lr=4.4e-05, gnorm=1.534, clip=0, loss_scale=256, train_wall=92, gb_free=28.4, wall=30290 (progress_bar.py:260, log())
[2021-11-15 01:21:25]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 01:21:26]    INFO >> AMP: overflow detected, setting scale to to 64.0 (amp_optimizer.py:66, clip_grad_norm())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 01:22:33]    INFO >> epoch 013 | loss 2.884 | nll_loss 1.004 | ppl 2.01 | wps 4141.5 | ups 4.49 | wpb 922.1 | bsz 4 | num_updates 131361 | lr 4.3e-05 | gnorm 1.541 | clip 0 | loss_scale 64 | train_wall 1757 | gb_free 28 | wall 30374 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 01:28:44]    INFO >> epoch 013 | valid on 'valid' subset | loss 3.537 | nll_loss 1.714 | ppl 3.28 | bleu 19.5402 | wps 507.6 | wpb 3457.3 | bsz 16 | num_updates 131361 | best_bleu 23.5696 (progress_bar.py:269, print())
[2021-11-15 01:28:51]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 13 @ 131361 updates, score 19.540178) (writing took 7.095026 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 01:29:26]    INFO >> epoch 014:    139 / 10106 loss=2.915, nll_loss=1.038, ppl=2.05, wps=919.6, ups=1.01, wpb=913.2, bsz=4, num_updates=131500, lr=4.3e-05, gnorm=1.629, clip=0, loss_scale=64, train_wall=94, gb_free=27.9, wall=30786 (progress_bar.py:260, log())
[2021-11-15 01:31:00]    INFO >> epoch 014:    639 / 10106 loss=2.88, nll_loss=0.999, ppl=2, wps=4901, ups=5.3, wpb=924.6, bsz=4, num_updates=132000, lr=4.3e-05, gnorm=1.562, clip=0, loss_scale=64, train_wall=93, gb_free=27.1, wall=30881 (progress_bar.py:260, log())
[2021-11-15 01:32:25]    INFO >> epoch 014:   1139 / 10106 loss=2.89, nll_loss=1.01, ppl=2.01, wps=5549.5, ups=5.89, wpb=941.8, bsz=4, num_updates=132500, lr=4.3e-05, gnorm=1.605, clip=0, loss_scale=64, train_wall=84, gb_free=27.6, wall=30966 (progress_bar.py:260, log())
[2021-11-15 01:33:35]    INFO >> epoch 014:   1639 / 10106 loss=2.897, nll_loss=1.017, ppl=2.02, wps=6922, ups=7.18, wpb=964.1, bsz=4, num_updates=133000, lr=4.3e-05, gnorm=1.568, clip=0, loss_scale=64, train_wall=69, gb_free=28.9, wall=31035 (progress_bar.py:260, log())
[2021-11-15 01:34:42]    INFO >> epoch 014:   2139 / 10106 loss=2.854, nll_loss=0.971, ppl=1.96, wps=6697.9, ups=7.37, wpb=908.3, bsz=4, num_updates=133500, lr=4.3e-05, gnorm=1.542, clip=0, loss_scale=128, train_wall=67, gb_free=29, wall=31103 (progress_bar.py:260, log())
[2021-11-15 01:35:50]    INFO >> epoch 014:   2639 / 10106 loss=2.873, nll_loss=0.992, ppl=1.99, wps=6819.7, ups=7.38, wpb=924.2, bsz=4, num_updates=134000, lr=4.3e-05, gnorm=1.526, clip=0, loss_scale=128, train_wall=67, gb_free=29.1, wall=31171 (progress_bar.py:260, log())
[2021-11-15 01:37:00]    INFO >> epoch 014:   3139 / 10106 loss=2.891, nll_loss=1.012, ppl=2.02, wps=6785.8, ups=7.18, wpb=944.6, bsz=4, num_updates=134500, lr=4.3e-05, gnorm=1.538, clip=0, loss_scale=128, train_wall=69, gb_free=28.4, wall=31240 (progress_bar.py:260, log())
[2021-11-15 01:38:20]    INFO >> epoch 014:   3639 / 10106 loss=2.866, nll_loss=0.983, ppl=1.98, wps=5939.9, ups=6.27, wpb=947.1, bsz=4, num_updates=135000, lr=4.3e-05, gnorm=1.544, clip=0, loss_scale=128, train_wall=79, gb_free=28.9, wall=31320 (progress_bar.py:260, log())
[2021-11-15 01:39:51]    INFO >> epoch 014:   4139 / 10106 loss=2.855, nll_loss=0.973, ppl=1.96, wps=5040, ups=5.48, wpb=919.8, bsz=4, num_updates=135500, lr=4.3e-05, gnorm=1.489, clip=0, loss_scale=256, train_wall=90, gb_free=26.8, wall=31411 (progress_bar.py:260, log())
[2021-11-15 01:41:25]    INFO >> epoch 014:   4639 / 10106 loss=2.853, nll_loss=0.968, ppl=1.96, wps=4889.7, ups=5.33, wpb=918, bsz=4, num_updates=136000, lr=4.3e-05, gnorm=1.492, clip=0, loss_scale=256, train_wall=93, gb_free=29.3, wall=31505 (progress_bar.py:260, log())
[2021-11-15 01:42:57]    INFO >> epoch 014:   5139 / 10106 loss=2.849, nll_loss=0.964, ppl=1.95, wps=4836, ups=5.39, wpb=897.5, bsz=4, num_updates=136500, lr=4.3e-05, gnorm=1.51, clip=0, loss_scale=256, train_wall=92, gb_free=26.7, wall=31598 (progress_bar.py:260, log())
[2021-11-15 01:44:14]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 01:44:14]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 01:44:32]    INFO >> epoch 014:   5640 / 10106 loss=2.88, nll_loss=1, ppl=2, wps=4926.7, ups=5.29, wpb=932.1, bsz=4, num_updates=137000, lr=4.3e-05, gnorm=1.523, clip=0, loss_scale=128, train_wall=94, gb_free=29.1, wall=31693 (progress_bar.py:260, log())
[2021-11-15 01:46:06]    INFO >> epoch 014:   6140 / 10106 loss=2.878, nll_loss=0.996, ppl=1.99, wps=4787.9, ups=5.33, wpb=898.3, bsz=4, num_updates=137500, lr=4.3e-05, gnorm=1.562, clip=0, loss_scale=128, train_wall=93, gb_free=26, wall=31786 (progress_bar.py:260, log())
[2021-11-15 01:47:42]    INFO >> epoch 014:   6640 / 10106 loss=2.879, nll_loss=1, ppl=2, wps=4940.9, ups=5.22, wpb=947.4, bsz=4, num_updates=138000, lr=4.3e-05, gnorm=1.528, clip=0, loss_scale=128, train_wall=95, gb_free=29.3, wall=31882 (progress_bar.py:260, log())
[2021-11-15 01:49:13]    INFO >> epoch 014:   7140 / 10106 loss=2.85, nll_loss=0.969, ppl=1.96, wps=4741.3, ups=5.5, wpb=861.4, bsz=4, num_updates=138500, lr=4.3e-05, gnorm=1.545, clip=0, loss_scale=128, train_wall=90, gb_free=29.2, wall=31973 (progress_bar.py:260, log())
[2021-11-15 01:50:46]    INFO >> epoch 014:   7640 / 10106 loss=2.83, nll_loss=0.945, ppl=1.93, wps=4839.4, ups=5.37, wpb=901.7, bsz=4, num_updates=139000, lr=4.3e-05, gnorm=1.516, clip=0, loss_scale=256, train_wall=92, gb_free=28.6, wall=32066 (progress_bar.py:260, log())
[2021-11-15 01:52:21]    INFO >> epoch 014:   8140 / 10106 loss=2.871, nll_loss=0.991, ppl=1.99, wps=4899.5, ups=5.26, wpb=931.3, bsz=4, num_updates=139500, lr=4.3e-05, gnorm=1.537, clip=0, loss_scale=256, train_wall=94, gb_free=29.4, wall=32161 (progress_bar.py:260, log())
[2021-11-15 01:53:47]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 01:53:55]    INFO >> epoch 014:   8640 / 10106 loss=2.847, nll_loss=0.963, ppl=1.95, wps=4924.2, ups=5.3, wpb=929.1, bsz=4, num_updates=140000, lr=4.3e-05, gnorm=1.487, clip=0, loss_scale=128, train_wall=93, gb_free=28.6, wall=32256 (progress_bar.py:260, log())
[2021-11-15 01:55:30]    INFO >> epoch 014:   9140 / 10106 loss=2.889, nll_loss=1.008, ppl=2.01, wps=4830.5, ups=5.29, wpb=913.8, bsz=4, num_updates=140500, lr=4.3e-05, gnorm=1.55, clip=0, loss_scale=128, train_wall=94, gb_free=26.2, wall=32350 (progress_bar.py:260, log())
[2021-11-15 01:57:04]    INFO >> epoch 014:   9640 / 10106 loss=2.886, nll_loss=1.008, ppl=2.01, wps=4872.9, ups=5.3, wpb=920, bsz=4, num_updates=141000, lr=4.3e-05, gnorm=1.558, clip=0, loss_scale=128, train_wall=93, gb_free=29.4, wall=32445 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 01:58:48]    INFO >> epoch 014 | loss 2.869 | nll_loss 0.988 | ppl 1.98 | wps 4284 | ups 4.65 | wpb 922.2 | bsz 4 | num_updates 141466 | lr 4.3e-05 | gnorm 1.538 | clip 0 | loss_scale 128 | train_wall 1753 | gb_free 25.8 | wall 32549 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 02:05:26]    INFO >> epoch 014 | valid on 'valid' subset | loss 3.522 | nll_loss 1.697 | ppl 3.24 | bleu 18.7254 | wps 470.6 | wpb 3457.3 | bsz 16 | num_updates 141466 | best_bleu 23.5696 (progress_bar.py:269, print())
[2021-11-15 02:05:33]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 14 @ 141466 updates, score 18.725439) (writing took 7.054640 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 02:05:48]    INFO >> epoch 015:     34 / 10106 loss=2.857, nll_loss=0.974, ppl=1.96, wps=889.3, ups=0.96, wpb=930.9, bsz=4, num_updates=141500, lr=4.3e-05, gnorm=1.542, clip=0, loss_scale=128, train_wall=95, gb_free=29.2, wall=32968 (progress_bar.py:260, log())
[2021-11-15 02:07:12]    INFO >> epoch 015:    534 / 10106 loss=2.846, nll_loss=0.959, ppl=1.94, wps=5461.3, ups=5.9, wpb=925.3, bsz=4, num_updates=142000, lr=4.3e-05, gnorm=1.498, clip=0, loss_scale=256, train_wall=84, gb_free=28.7, wall=33053 (progress_bar.py:260, log())
[2021-11-15 02:07:21]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 02:08:22]    INFO >> epoch 015:   1034 / 10106 loss=2.856, nll_loss=0.971, ppl=1.96, wps=6983.9, ups=7.17, wpb=974.3, bsz=4, num_updates=142500, lr=4.3e-05, gnorm=1.491, clip=0, loss_scale=128, train_wall=69, gb_free=29.3, wall=33123 (progress_bar.py:260, log())
[2021-11-15 02:09:30]    INFO >> epoch 015:   1534 / 10106 loss=2.853, nll_loss=0.969, ppl=1.96, wps=6865.6, ups=7.34, wpb=935.4, bsz=4, num_updates=143000, lr=4.3e-05, gnorm=1.537, clip=0, loss_scale=128, train_wall=67, gb_free=27.5, wall=33191 (progress_bar.py:260, log())
[2021-11-15 02:10:38]    INFO >> epoch 015:   2034 / 10106 loss=2.841, nll_loss=0.956, ppl=1.94, wps=6521.4, ups=7.37, wpb=884.3, bsz=4, num_updates=143500, lr=4.3e-05, gnorm=1.521, clip=0, loss_scale=128, train_wall=67, gb_free=29.3, wall=33259 (progress_bar.py:260, log())
[2021-11-15 02:11:46]    INFO >> epoch 015:   2534 / 10106 loss=2.836, nll_loss=0.951, ppl=1.93, wps=6826.3, ups=7.32, wpb=932.8, bsz=4, num_updates=144000, lr=4.3e-05, gnorm=1.487, clip=0, loss_scale=128, train_wall=67, gb_free=29.2, wall=33327 (progress_bar.py:260, log())
[2021-11-15 02:12:56]    INFO >> epoch 015:   3034 / 10106 loss=2.836, nll_loss=0.95, ppl=1.93, wps=6520.9, ups=7.21, wpb=904.1, bsz=4, num_updates=144500, lr=4.3e-05, gnorm=1.516, clip=0, loss_scale=256, train_wall=68, gb_free=27.5, wall=33396 (progress_bar.py:260, log())
[2021-11-15 02:14:26]    INFO >> epoch 015:   3534 / 10106 loss=2.832, nll_loss=0.946, ppl=1.93, wps=4983.5, ups=5.5, wpb=905.6, bsz=4, num_updates=145000, lr=4.3e-05, gnorm=1.49, clip=0, loss_scale=256, train_wall=90, gb_free=29.1, wall=33487 (progress_bar.py:260, log())
[2021-11-15 02:15:17]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 02:15:17]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 02:16:00]    INFO >> epoch 015:   4035 / 10106 loss=2.86, nll_loss=0.977, ppl=1.97, wps=5028.5, ups=5.35, wpb=940.6, bsz=4, num_updates=145500, lr=4.3e-05, gnorm=1.502, clip=0, loss_scale=128, train_wall=92, gb_free=29.4, wall=33581 (progress_bar.py:260, log())
[2021-11-15 02:17:34]    INFO >> epoch 015:   4535 / 10106 loss=2.83, nll_loss=0.944, ppl=1.92, wps=5087.7, ups=5.29, wpb=960.9, bsz=4, num_updates=146000, lr=4.3e-05, gnorm=1.485, clip=0, loss_scale=128, train_wall=93, gb_free=29.4, wall=33675 (progress_bar.py:260, log())
[2021-11-15 02:19:11]    INFO >> epoch 015:   5035 / 10106 loss=2.829, nll_loss=0.943, ppl=1.92, wps=4644.2, ups=5.19, wpb=895.3, bsz=4, num_updates=146500, lr=4.3e-05, gnorm=1.523, clip=0, loss_scale=128, train_wall=95, gb_free=28.2, wall=33771 (progress_bar.py:260, log())
[2021-11-15 02:20:42]    INFO >> epoch 015:   5535 / 10106 loss=2.856, nll_loss=0.973, ppl=1.96, wps=5048, ups=5.47, wpb=922.2, bsz=4, num_updates=147000, lr=4.3e-05, gnorm=1.517, clip=0, loss_scale=128, train_wall=90, gb_free=27.7, wall=33863 (progress_bar.py:260, log())
[2021-11-15 02:22:19]    INFO >> epoch 015:   6035 / 10106 loss=2.843, nll_loss=0.959, ppl=1.94, wps=4744.7, ups=5.19, wpb=914.5, bsz=4, num_updates=147500, lr=4.3e-05, gnorm=1.497, clip=0, loss_scale=256, train_wall=95, gb_free=29.2, wall=33959 (progress_bar.py:260, log())
[2021-11-15 02:23:53]    INFO >> epoch 015:   6535 / 10106 loss=2.849, nll_loss=0.964, ppl=1.95, wps=4896.2, ups=5.31, wpb=921.3, bsz=4, num_updates=148000, lr=4.3e-05, gnorm=1.524, clip=0, loss_scale=256, train_wall=93, gb_free=28.8, wall=34053 (progress_bar.py:260, log())
[2021-11-15 02:24:36]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 02:25:26]    INFO >> epoch 015:   7035 / 10106 loss=2.826, nll_loss=0.939, ppl=1.92, wps=4931.4, ups=5.33, wpb=925.5, bsz=4, num_updates=148500, lr=4.3e-05, gnorm=1.495, clip=0, loss_scale=128, train_wall=93, gb_free=27.6, wall=34147 (progress_bar.py:260, log())
[2021-11-15 02:27:00]    INFO >> epoch 015:   7535 / 10106 loss=2.849, nll_loss=0.965, ppl=1.95, wps=4988, ups=5.37, wpb=929.4, bsz=4, num_updates=149000, lr=4.3e-05, gnorm=1.509, clip=0, loss_scale=128, train_wall=92, gb_free=27, wall=34240 (progress_bar.py:260, log())
[2021-11-15 02:28:32]    INFO >> epoch 015:   8035 / 10106 loss=2.83, nll_loss=0.944, ppl=1.92, wps=4918.2, ups=5.44, wpb=904.9, bsz=4, num_updates=149500, lr=4.3e-05, gnorm=1.552, clip=0, loss_scale=128, train_wall=91, gb_free=27.7, wall=34332 (progress_bar.py:260, log())
[2021-11-15 02:30:06]    INFO >> epoch 015:   8535 / 10106 loss=2.863, nll_loss=0.982, ppl=1.98, wps=4775.5, ups=5.27, wpb=905.4, bsz=4, num_updates=150000, lr=4.3e-05, gnorm=1.558, clip=0, loss_scale=128, train_wall=94, gb_free=28.4, wall=34427 (progress_bar.py:260, log())
[2021-11-15 02:31:39]    INFO >> epoch 015:   9035 / 10106 loss=2.847, nll_loss=0.964, ppl=1.95, wps=4958.5, ups=5.42, wpb=914.6, bsz=4, num_updates=150500, lr=4.3e-05, gnorm=1.524, clip=0, loss_scale=256, train_wall=91, gb_free=28.7, wall=34519 (progress_bar.py:260, log())
[2021-11-15 02:33:13]    INFO >> epoch 015:   9535 / 10106 loss=2.839, nll_loss=0.954, ppl=1.94, wps=4917.1, ups=5.3, wpb=928.3, bsz=4, num_updates=151000, lr=4.3e-05, gnorm=1.488, clip=0, loss_scale=256, train_wall=93, gb_free=29.4, wall=34614 (progress_bar.py:260, log())
[2021-11-15 02:34:45]    INFO >> epoch 015:  10035 / 10106 loss=2.832, nll_loss=0.947, ppl=1.93, wps=5065.4, ups=5.45, wpb=928.6, bsz=4, num_updates=151500, lr=4.2e-05, gnorm=1.483, clip=0, loss_scale=256, train_wall=91, gb_free=28.3, wall=34705 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 02:35:13]    INFO >> epoch 015 | loss 2.842 | nll_loss 0.958 | ppl 1.94 | wps 4266 | ups 4.63 | wpb 922.2 | bsz 4 | num_updates 151571 | lr 4.2e-05 | gnorm 1.509 | clip 0 | loss_scale 256 | train_wall 1736 | gb_free 28.4 | wall 34733 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 02:40:52]    INFO >> epoch 015 | valid on 'valid' subset | loss 3.507 | nll_loss 1.685 | ppl 3.22 | bleu 19.9846 | wps 558.8 | wpb 3457.3 | bsz 16 | num_updates 151571 | best_bleu 23.5696 (progress_bar.py:269, print())
[2021-11-15 02:40:59]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 15 @ 151571 updates, score 19.984607) (writing took 6.968585 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 02:42:28]    INFO >> epoch 016:    429 / 10106 loss=2.82, nll_loss=0.93, ppl=1.91, wps=995.7, ups=1.08, wpb=922.8, bsz=4, num_updates=152000, lr=4.2e-05, gnorm=1.479, clip=0, loss_scale=256, train_wall=94, gb_free=28.2, wall=35169 (progress_bar.py:260, log())
[2021-11-15 02:43:48]    INFO >> epoch 016:    929 / 10106 loss=2.792, nll_loss=0.901, ppl=1.87, wps=5826.1, ups=6.24, wpb=933.6, bsz=4, num_updates=152500, lr=4.2e-05, gnorm=1.413, clip=0, loss_scale=512, train_wall=79, gb_free=28.3, wall=35249 (progress_bar.py:260, log())
[2021-11-15 02:44:04]    INFO >> AMP: overflow detected, setting scale to to 256.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 02:44:05]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 02:44:05]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 02:44:56]    INFO >> epoch 016:   1430 / 10106 loss=2.82, nll_loss=0.932, ppl=1.91, wps=6771.2, ups=7.32, wpb=924.5, bsz=4, num_updates=153000, lr=4.2e-05, gnorm=1.503, clip=0, loss_scale=128, train_wall=67, gb_free=28.2, wall=35317 (progress_bar.py:260, log())
[2021-11-15 02:46:06]    INFO >> epoch 016:   1930 / 10106 loss=2.838, nll_loss=0.952, ppl=1.93, wps=6775.5, ups=7.24, wpb=935.9, bsz=4, num_updates=153500, lr=4.2e-05, gnorm=1.498, clip=0, loss_scale=128, train_wall=68, gb_free=29.1, wall=35386 (progress_bar.py:260, log())
[2021-11-15 02:47:15]    INFO >> epoch 016:   2430 / 10106 loss=2.803, nll_loss=0.913, ppl=1.88, wps=6737.1, ups=7.23, wpb=931.5, bsz=4, num_updates=154000, lr=4.2e-05, gnorm=1.489, clip=0, loss_scale=128, train_wall=68, gb_free=26.6, wall=35455 (progress_bar.py:260, log())
[2021-11-15 02:48:24]    INFO >> epoch 016:   2930 / 10106 loss=2.821, nll_loss=0.933, ppl=1.91, wps=6643.5, ups=7.19, wpb=923.9, bsz=4, num_updates=154500, lr=4.2e-05, gnorm=1.502, clip=0, loss_scale=128, train_wall=69, gb_free=28.9, wall=35525 (progress_bar.py:260, log())
[2021-11-15 02:50:00]    INFO >> epoch 016:   3430 / 10106 loss=2.811, nll_loss=0.922, ppl=1.89, wps=4718.8, ups=5.24, wpb=900.1, bsz=4, num_updates=155000, lr=4.2e-05, gnorm=1.484, clip=0, loss_scale=256, train_wall=94, gb_free=27.1, wall=35620 (progress_bar.py:260, log())
[2021-11-15 02:51:34]    INFO >> epoch 016:   3930 / 10106 loss=2.829, nll_loss=0.941, ppl=1.92, wps=4876.4, ups=5.29, wpb=921.4, bsz=4, num_updates=155500, lr=4.2e-05, gnorm=1.492, clip=0, loss_scale=256, train_wall=93, gb_free=29.3, wall=35715 (progress_bar.py:260, log())
[2021-11-15 02:51:50]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 02:53:09]    INFO >> epoch 016:   4430 / 10106 loss=2.821, nll_loss=0.933, ppl=1.91, wps=4909, ups=5.26, wpb=932.9, bsz=4, num_updates=156000, lr=4.2e-05, gnorm=1.476, clip=0, loss_scale=128, train_wall=94, gb_free=28.5, wall=35810 (progress_bar.py:260, log())
[2021-11-15 02:54:46]    INFO >> epoch 016:   4930 / 10106 loss=2.849, nll_loss=0.965, ppl=1.95, wps=4762.8, ups=5.18, wpb=919.2, bsz=4, num_updates=156500, lr=4.2e-05, gnorm=1.553, clip=0, loss_scale=128, train_wall=95, gb_free=27, wall=35906 (progress_bar.py:260, log())
[2021-11-15 02:56:20]    INFO >> epoch 016:   5430 / 10106 loss=2.828, nll_loss=0.942, ppl=1.92, wps=4842.7, ups=5.28, wpb=917.5, bsz=4, num_updates=157000, lr=4.2e-05, gnorm=1.508, clip=0, loss_scale=128, train_wall=94, gb_free=28.5, wall=36001 (progress_bar.py:260, log())
[2021-11-15 02:57:53]    INFO >> epoch 016:   5930 / 10106 loss=2.848, nll_loss=0.966, ppl=1.95, wps=4924.3, ups=5.39, wpb=914.3, bsz=4, num_updates=157500, lr=4.2e-05, gnorm=1.505, clip=0, loss_scale=128, train_wall=92, gb_free=27.8, wall=36094 (progress_bar.py:260, log())
[2021-11-15 02:59:28]    INFO >> epoch 016:   6430 / 10106 loss=2.832, nll_loss=0.945, ppl=1.93, wps=4989, ups=5.29, wpb=943.5, bsz=4, num_updates=158000, lr=4.2e-05, gnorm=1.49, clip=0, loss_scale=256, train_wall=94, gb_free=28.8, wall=36188 (progress_bar.py:260, log())
[2021-11-15 03:01:01]    INFO >> epoch 016:   6930 / 10106 loss=2.816, nll_loss=0.929, ppl=1.9, wps=4940.9, ups=5.36, wpb=922.5, bsz=4, num_updates=158500, lr=4.2e-05, gnorm=1.488, clip=0, loss_scale=256, train_wall=92, gb_free=27.2, wall=36282 (progress_bar.py:260, log())
[2021-11-15 03:02:36]    INFO >> epoch 016:   7430 / 10106 loss=2.803, nll_loss=0.913, ppl=1.88, wps=4913.9, ups=5.26, wpb=933.4, bsz=4, num_updates=159000, lr=4.2e-05, gnorm=1.477, clip=0, loss_scale=256, train_wall=94, gb_free=29, wall=36377 (progress_bar.py:260, log())
[2021-11-15 03:04:07]    INFO >> epoch 016:   7930 / 10106 loss=2.799, nll_loss=0.909, ppl=1.88, wps=4915.3, ups=5.48, wpb=896.8, bsz=4, num_updates=159500, lr=4.2e-05, gnorm=1.495, clip=0, loss_scale=256, train_wall=90, gb_free=28.4, wall=36468 (progress_bar.py:260, log())
[2021-11-15 03:04:51]    INFO >> AMP: overflow detected, setting scale to to 256.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:05:34]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:05:34]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 03:05:40]    INFO >> epoch 016:   8431 / 10106 loss=2.817, nll_loss=0.929, ppl=1.9, wps=4822.9, ups=5.4, wpb=893.7, bsz=4, num_updates=160000, lr=4.2e-05, gnorm=1.492, clip=0, loss_scale=128, train_wall=91, gb_free=28.5, wall=36561 (progress_bar.py:260, log())
[2021-11-15 03:07:14]    INFO >> epoch 016:   8931 / 10106 loss=2.83, nll_loss=0.944, ppl=1.92, wps=4829.6, ups=5.31, wpb=909, bsz=4, num_updates=160500, lr=4.2e-05, gnorm=1.522, clip=0, loss_scale=128, train_wall=93, gb_free=26.9, wall=36655 (progress_bar.py:260, log())
[2021-11-15 03:08:47]    INFO >> epoch 016:   9431 / 10106 loss=2.812, nll_loss=0.924, ppl=1.9, wps=4791.8, ups=5.35, wpb=895.1, bsz=4, num_updates=161000, lr=4.2e-05, gnorm=1.533, clip=0, loss_scale=128, train_wall=92, gb_free=29.3, wall=36748 (progress_bar.py:260, log())
[2021-11-15 03:10:24]    INFO >> epoch 016:   9931 / 10106 loss=2.834, nll_loss=0.947, ppl=1.93, wps=4919.9, ups=5.16, wpb=953.8, bsz=4, num_updates=161500, lr=4.2e-05, gnorm=1.506, clip=0, loss_scale=128, train_wall=96, gb_free=28.6, wall=36845 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 03:11:14]    INFO >> epoch 016 | loss 2.821 | nll_loss 0.934 | ppl 1.91 | wps 4311.8 | ups 4.68 | wpb 922.2 | bsz 4 | num_updates 161675 | lr 4.2e-05 | gnorm 1.496 | clip 0 | loss_scale 128 | train_wall 1771 | gb_free 28.7 | wall 36895 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 03:17:27]    INFO >> epoch 016 | valid on 'valid' subset | loss 3.512 | nll_loss 1.675 | ppl 3.19 | bleu 18.7864 | wps 506.6 | wpb 3457.3 | bsz 16 | num_updates 161675 | best_bleu 23.5696 (progress_bar.py:269, print())
[2021-11-15 03:17:34]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 16 @ 161675 updates, score 18.786419) (writing took 7.016222 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 03:18:26]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:18:27]    INFO >> epoch 017:    325 / 10106 loss=2.83, nll_loss=0.944, ppl=1.92, wps=953, ups=1.04, wpb=919.5, bsz=4, num_updates=162000, lr=4.2e-05, gnorm=1.479, clip=0, loss_scale=128, train_wall=78, gb_free=29.2, wall=37327 (progress_bar.py:260, log())
[2021-11-15 03:19:35]    INFO >> epoch 017:    825 / 10106 loss=2.801, nll_loss=0.911, ppl=1.88, wps=6624.3, ups=7.37, wpb=899.4, bsz=4, num_updates=162500, lr=4.2e-05, gnorm=1.493, clip=0, loss_scale=128, train_wall=67, gb_free=29.1, wall=37395 (progress_bar.py:260, log())
[2021-11-15 03:20:43]    INFO >> epoch 017:   1325 / 10106 loss=2.797, nll_loss=0.906, ppl=1.87, wps=6741.3, ups=7.34, wpb=918.3, bsz=4, num_updates=163000, lr=4.2e-05, gnorm=1.478, clip=0, loss_scale=128, train_wall=67, gb_free=28, wall=37463 (progress_bar.py:260, log())
[2021-11-15 03:21:53]    INFO >> epoch 017:   1825 / 10106 loss=2.798, nll_loss=0.908, ppl=1.88, wps=6429.8, ups=7.1, wpb=905.5, bsz=4, num_updates=163500, lr=4.2e-05, gnorm=1.496, clip=0, loss_scale=128, train_wall=69, gb_free=28.9, wall=37534 (progress_bar.py:260, log())
[2021-11-15 03:23:02]    INFO >> epoch 017:   2325 / 10106 loss=2.802, nll_loss=0.912, ppl=1.88, wps=6720.5, ups=7.29, wpb=922.3, bsz=4, num_updates=164000, lr=4.2e-05, gnorm=1.502, clip=0, loss_scale=256, train_wall=68, gb_free=28.5, wall=37602 (progress_bar.py:260, log())
[2021-11-15 03:24:25]    INFO >> epoch 017:   2825 / 10106 loss=2.814, nll_loss=0.925, ppl=1.9, wps=5630.5, ups=6.03, wpb=934.2, bsz=4, num_updates=164500, lr=4.2e-05, gnorm=1.48, clip=0, loss_scale=256, train_wall=82, gb_free=29.1, wall=37685 (progress_bar.py:260, log())
[2021-11-15 03:25:41]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:25:58]    INFO >> epoch 017:   3325 / 10106 loss=2.8, nll_loss=0.91, ppl=1.88, wps=5043.9, ups=5.34, wpb=945, bsz=4, num_updates=165000, lr=4.2e-05, gnorm=1.474, clip=0, loss_scale=128, train_wall=92, gb_free=26.6, wall=37779 (progress_bar.py:260, log())
[2021-11-15 03:27:32]    INFO >> epoch 017:   3825 / 10106 loss=2.834, nll_loss=0.948, ppl=1.93, wps=4933.9, ups=5.38, wpb=917.8, bsz=4, num_updates=165500, lr=4.2e-05, gnorm=1.53, clip=0, loss_scale=128, train_wall=92, gb_free=28.2, wall=37872 (progress_bar.py:260, log())
[2021-11-15 03:29:06]    INFO >> epoch 017:   4325 / 10106 loss=2.823, nll_loss=0.933, ppl=1.91, wps=4932.7, ups=5.28, wpb=934.1, bsz=4, num_updates=166000, lr=4.2e-05, gnorm=1.518, clip=0, loss_scale=128, train_wall=94, gb_free=29.2, wall=37967 (progress_bar.py:260, log())
[2021-11-15 03:30:40]    INFO >> epoch 017:   4825 / 10106 loss=2.815, nll_loss=0.927, ppl=1.9, wps=4957.9, ups=5.3, wpb=934.9, bsz=4, num_updates=166500, lr=4.2e-05, gnorm=1.51, clip=0, loss_scale=128, train_wall=93, gb_free=28.9, wall=38061 (progress_bar.py:260, log())
[2021-11-15 03:32:16]    INFO >> epoch 017:   5325 / 10106 loss=2.832, nll_loss=0.945, ppl=1.93, wps=4870.6, ups=5.26, wpb=926.6, bsz=4, num_updates=167000, lr=4.2e-05, gnorm=1.539, clip=0, loss_scale=256, train_wall=94, gb_free=28.1, wall=38156 (progress_bar.py:260, log())
[2021-11-15 03:33:42]    INFO >> epoch 017:   5825 / 10106 loss=2.791, nll_loss=0.901, ppl=1.87, wps=5354.1, ups=5.79, wpb=924.8, bsz=4, num_updates=167500, lr=4.2e-05, gnorm=1.471, clip=0, loss_scale=256, train_wall=85, gb_free=29.3, wall=38243 (progress_bar.py:260, log())
[2021-11-15 03:35:14]    INFO >> epoch 017:   6325 / 10106 loss=2.797, nll_loss=0.907, ppl=1.88, wps=4999.7, ups=5.43, wpb=919.9, bsz=4, num_updates=168000, lr=4.2e-05, gnorm=1.475, clip=0, loss_scale=256, train_wall=91, gb_free=26.5, wall=38335 (progress_bar.py:260, log())
[2021-11-15 03:36:51]    INFO >> epoch 017:   6825 / 10106 loss=2.809, nll_loss=0.919, ppl=1.89, wps=4785.3, ups=5.17, wpb=924.9, bsz=4, num_updates=168500, lr=4.2e-05, gnorm=1.505, clip=0, loss_scale=256, train_wall=96, gb_free=28.8, wall=38431 (progress_bar.py:260, log())
[2021-11-15 03:38:23]    INFO >> AMP: overflow detected, setting scale to to 256.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:38:23]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 03:38:26]    INFO >> epoch 017:   7326 / 10106 loss=2.803, nll_loss=0.913, ppl=1.88, wps=4692.2, ups=5.22, wpb=898.6, bsz=4, num_updates=169000, lr=4.2e-05, gnorm=1.492, clip=0, loss_scale=256, train_wall=95, gb_free=28.7, wall=38527 (progress_bar.py:260, log())
[2021-11-15 03:39:59]    INFO >> epoch 017:   7826 / 10106 loss=2.786, nll_loss=0.895, ppl=1.86, wps=5034.1, ups=5.38, wpb=936.3, bsz=4, num_updates=169500, lr=4.2e-05, gnorm=1.472, clip=0, loss_scale=256, train_wall=92, gb_free=29, wall=38620 (progress_bar.py:260, log())
[2021-11-15 03:41:31]    INFO >> epoch 017:   8326 / 10106 loss=2.749, nll_loss=0.855, ppl=1.81, wps=4935.5, ups=5.43, wpb=908.8, bsz=4, num_updates=170000, lr=4.2e-05, gnorm=1.428, clip=0, loss_scale=256, train_wall=91, gb_free=29.3, wall=38712 (progress_bar.py:260, log())
[2021-11-15 03:43:05]    INFO >> epoch 017:   8826 / 10106 loss=2.805, nll_loss=0.916, ppl=1.89, wps=4867.4, ups=5.33, wpb=913.7, bsz=4, num_updates=170500, lr=4.2e-05, gnorm=1.486, clip=0, loss_scale=256, train_wall=93, gb_free=27.8, wall=38806 (progress_bar.py:260, log())
[2021-11-15 03:43:38]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 03:44:43]    INFO >> epoch 017:   9326 / 10106 loss=2.803, nll_loss=0.915, ppl=1.89, wps=4745.1, ups=5.11, wpb=928.7, bsz=4, num_updates=171000, lr=4.2e-05, gnorm=1.48, clip=0, loss_scale=128, train_wall=97, gb_free=29.3, wall=38904 (progress_bar.py:260, log())
[2021-11-15 03:46:16]    INFO >> epoch 017:   9826 / 10106 loss=2.816, nll_loss=0.928, ppl=1.9, wps=4890.7, ups=5.38, wpb=909.6, bsz=4, num_updates=171500, lr=4.1e-05, gnorm=1.527, clip=0, loss_scale=128, train_wall=92, gb_free=28.7, wall=38997 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 03:47:22]    INFO >> epoch 017 | loss 2.806 | nll_loss 0.916 | ppl 1.89 | wps 4297.7 | ups 4.66 | wpb 922.2 | bsz 4 | num_updates 171780 | lr 4.1e-05 | gnorm 1.491 | clip 0 | loss_scale 128 | train_wall 1744 | gb_free 29.3 | wall 39063 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 03:52:43]    INFO >> epoch 017 | valid on 'valid' subset | loss 3.517 | nll_loss 1.688 | ppl 3.22 | bleu 17.4006 | wps 583 | wpb 3457.3 | bsz 16 | num_updates 171780 | best_bleu 23.5696 (progress_bar.py:269, print())
[2021-11-15 03:52:50]    INFO >> saved checkpoint /mnt/wanyao/ncc_data/avatar/translation/top5/vanilla/data-mmap/transformer/python-java/checkpoints/checkpoint_last.pt (epoch 17 @ 171780 updates, score 17.40064) (writing took 6.982988 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-11-15 03:53:35]    INFO >> epoch 018:    220 / 10106 loss=2.799, nll_loss=0.909, ppl=1.88, wps=1068, ups=1.14, wpb=936.3, bsz=4, num_updates=172000, lr=4.1e-05, gnorm=1.499, clip=0, loss_scale=128, train_wall=87, gb_free=28.5, wall=39435 (progress_bar.py:260, log())
[2021-11-15 03:54:44]    INFO >> epoch 018:    720 / 10106 loss=2.794, nll_loss=0.903, ppl=1.87, wps=6747.1, ups=7.18, wpb=939.9, bsz=4, num_updates=172500, lr=4.1e-05, gnorm=1.478, clip=0, loss_scale=128, train_wall=69, gb_free=28.6, wall=39505 (progress_bar.py:260, log())
[2021-11-15 03:55:53]    INFO >> epoch 018:   1220 / 10106 loss=2.814, nll_loss=0.926, ppl=1.9, wps=6725.7, ups=7.26, wpb=926.8, bsz=4, num_updates=173000, lr=4.1e-05, gnorm=1.5, clip=0, loss_scale=256, train_wall=68, gb_free=28.9, wall=39574 (progress_bar.py:260, log())
[2021-11-15 03:57:01]    INFO >> epoch 018:   1720 / 10106 loss=2.782, nll_loss=0.89, ppl=1.85, wps=6705.5, ups=7.31, wpb=917, bsz=4, num_updates=173500, lr=4.1e-05, gnorm=1.46, clip=0, loss_scale=256, train_wall=67, gb_free=28.7, wall=39642 (progress_bar.py:260, log())
[2021-11-15 03:58:14]    INFO >> epoch 018:   2220 / 10106 loss=2.791, nll_loss=0.9, ppl=1.87, wps=6587.4, ups=6.91, wpb=953.8, bsz=4, num_updates=174000, lr=4.1e-05, gnorm=1.472, clip=0, loss_scale=256, train_wall=71, gb_free=27.8, wall=39714 (progress_bar.py:260, log())
[2021-11-15 03:59:27]    INFO >> epoch 018:   2720 / 10106 loss=2.779, nll_loss=0.886, ppl=1.85, wps=6368.8, ups=6.83, wpb=932, bsz=4, num_updates=174500, lr=4.1e-05, gnorm=1.438, clip=0, loss_scale=256, train_wall=72, gb_free=29, wall=39788 (progress_bar.py:260, log())
[2021-11-15 03:59:47]    INFO >> AMP: overflow detected, setting scale to to 128.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 04:01:09]    INFO >> epoch 018:   3220 / 10106 loss=2.778, nll_loss=0.886, ppl=1.85, wps=4341.7, ups=4.89, wpb=887.3, bsz=4, num_updates=175000, lr=4.1e-05, gnorm=1.508, clip=0, loss_scale=128, train_wall=101, gb_free=26.3, wall=39890 (progress_bar.py:260, log())
[2021-11-15 04:03:26]    INFO >> epoch 018:   3720 / 10106 loss=2.782, nll_loss=0.89, ppl=1.85, wps=3314.3, ups=3.64, wpb=909.5, bsz=4, num_updates=175500, lr=4.1e-05, gnorm=1.507, clip=0, loss_scale=128, train_wall=136, gb_free=26.4, wall=40027 (progress_bar.py:260, log())
[2021-11-15 04:05:50]    INFO >> epoch 018:   4220 / 10106 loss=2.844, nll_loss=0.957, ppl=1.94, wps=3316, ups=3.49, wpb=951, bsz=4, num_updates=176000, lr=4.1e-05, gnorm=1.541, clip=0, loss_scale=128, train_wall=142, gb_free=28.6, wall=40170 (progress_bar.py:260, log())
[2021-11-15 04:07:35]    INFO >> epoch 018:   4720 / 10106 loss=2.786, nll_loss=0.894, ppl=1.86, wps=4344.2, ups=4.75, wpb=914.9, bsz=4, num_updates=176500, lr=4.1e-05, gnorm=1.512, clip=0, loss_scale=128, train_wall=104, gb_free=29, wall=40276 (progress_bar.py:260, log())
[2021-11-15 04:09:20]    INFO >> epoch 018:   5220 / 10106 loss=2.81, nll_loss=0.919, ppl=1.89, wps=4482.1, ups=4.79, wpb=936.6, bsz=4, num_updates=177000, lr=4.1e-05, gnorm=1.516, clip=0, loss_scale=256, train_wall=103, gb_free=26.8, wall=40380 (progress_bar.py:260, log())
[2021-11-15 04:11:31]    INFO >> epoch 018:   5720 / 10106 loss=2.796, nll_loss=0.905, ppl=1.87, wps=3559.2, ups=3.82, wpb=932.3, bsz=4, num_updates=177500, lr=4.1e-05, gnorm=1.484, clip=0, loss_scale=256, train_wall=130, gb_free=29.4, wall=40511 (progress_bar.py:260, log())
[2021-11-15 04:13:40]    INFO >> epoch 018:   6220 / 10106 loss=2.765, nll_loss=0.871, ppl=1.83, wps=3410.4, ups=3.85, wpb=884.7, bsz=4, num_updates=178000, lr=4.1e-05, gnorm=1.478, clip=0, loss_scale=256, train_wall=129, gb_free=29.4, wall=40641 (progress_bar.py:260, log())
[2021-11-15 04:15:52]    INFO >> epoch 018:   6720 / 10106 loss=2.771, nll_loss=0.877, ppl=1.84, wps=3392.4, ups=3.79, wpb=895.7, bsz=4, num_updates=178500, lr=4.1e-05, gnorm=1.453, clip=0, loss_scale=256, train_wall=131, gb_free=28.3, wall=40773 (progress_bar.py:260, log())
[2021-11-15 04:16:25]    INFO >> AMP: overflow detected, setting scale to to 256.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 04:16:25]    INFO >> AMP: skipping this batch. (ncc_trainers.py:470, train_step())
[2021-11-15 04:18:06]    INFO >> epoch 018:   7221 / 10106 loss=2.756, nll_loss=0.862, ppl=1.82, wps=3401.9, ups=3.75, wpb=906.2, bsz=4, num_updates=179000, lr=4.1e-05, gnorm=1.467, clip=0, loss_scale=256, train_wall=132, gb_free=28.5, wall=40906 (progress_bar.py:260, log())
[2021-11-15 04:20:19]    INFO >> epoch 018:   7721 / 10106 loss=2.794, nll_loss=0.903, ppl=1.87, wps=3452.2, ups=3.74, wpb=924.3, bsz=4, num_updates=179500, lr=4.1e-05, gnorm=1.465, clip=0, loss_scale=256, train_wall=133, gb_free=28.9, wall=41040 (progress_bar.py:260, log())
[2021-11-15 04:22:32]    INFO >> epoch 018:   8221 / 10106 loss=2.781, nll_loss=0.887, ppl=1.85, wps=3420.3, ups=3.76, wpb=909.6, bsz=4, num_updates=180000, lr=4.1e-05, gnorm=1.477, clip=0, loss_scale=256, train_wall=132, gb_free=26.1, wall=41173 (progress_bar.py:260, log())
[2021-11-15 04:24:46]    INFO >> epoch 018:   8721 / 10106 loss=2.79, nll_loss=0.899, ppl=1.86, wps=3498.3, ups=3.74, wpb=935.9, bsz=4, num_updates=180500, lr=4.1e-05, gnorm=1.476, clip=0, loss_scale=256, train_wall=133, gb_free=26.2, wall=41307 (progress_bar.py:260, log())
[2021-11-15 04:25:34]    INFO >> AMP: overflow detected, setting scale to to 256.0 (amp_optimizer.py:66, clip_grad_norm())
[2021-11-15 04:27:02]    INFO >> epoch 018:   9221 / 10106 loss=2.797, nll_loss=0.906, ppl=1.87, wps=3482.1, ups=3.68, wpb=945.7, bsz=4, num_updates=181000, lr=4.1e-05, gnorm=1.499, clip=0, loss_scale=256, train_wall=134, gb_free=27.9, wall=41443 (progress_bar.py:260, log())
[2021-11-15 04:28:38]    INFO >> epoch 018:   9721 / 10106 loss=2.762, nll_loss=0.867, ppl=1.82, wps=4946.7, ups=5.23, wpb=946.6, bsz=4, num_updates=181500, lr=4.1e-05, gnorm=1.461, clip=0, loss_scale=256, train_wall=95, gb_free=26.1, wall=41538 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-15 04:30:03]    INFO >> epoch 018 | loss 2.787 | nll_loss 0.896 | ppl 1.86 | wps 3638.9 | ups 3.95 | wpb 922.2 | bsz 4 | num_updates 181885 | lr 4.1e-05 | gnorm 1.485 | clip 0 | loss_scale 256 | train_wall 2189 | gb_free 27.1 | wall 41624 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
/home/wanyao/anaconda3/envs/py38-1.8/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 17 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
