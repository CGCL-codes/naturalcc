nohup: ignoring input
Using backend: pytorch
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. 
The class this function is called from is 'TransformersDictionary'.
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2021-11-07 01:39:41]    INFO >> initialize model with /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/last_checkpoint.pt (model.py:176, load_checkpoint())
[2021-11-07 01:39:44]    INFO >> Start training epoch  32/100, best bleu4: 41.25 (train.py:90, <module>())
[2021-11-07 01:44:19]    INFO >> Epoch   1/100, Batch 500/5302, train loss: 10.3974 (train.py:127, train())
[2021-11-07 01:48:03]    INFO >> Epoch   1/100, Batch 1000/5302, train loss: 10.6755 (train.py:127, train())
[2021-11-07 01:51:50]    INFO >> Epoch   1/100, Batch 1500/5302, train loss: 10.7589 (train.py:127, train())
[2021-11-07 01:55:37]    INFO >> Epoch   1/100, Batch 2000/5302, train loss: 10.7759 (train.py:127, train())
[2021-11-07 01:59:24]    INFO >> Epoch   1/100, Batch 2500/5302, train loss: 10.9239 (train.py:127, train())
[2021-11-07 02:03:10]    INFO >> Epoch   1/100, Batch 3000/5302, train loss: 11.0432 (train.py:127, train())
[2021-11-07 02:06:56]    INFO >> Epoch   1/100, Batch 3500/5302, train loss: 11.0565 (train.py:127, train())
[2021-11-07 02:10:46]    INFO >> Epoch   1/100, Batch 4000/5302, train loss: 11.0980 (train.py:127, train())
[2021-11-07 02:14:34]    INFO >> Epoch   1/100, Batch 4500/5302, train loss: 11.1235 (train.py:127, train())
[2021-11-07 02:18:20]    INFO >> Epoch   1/100, Batch 5000/5302, train loss: 11.1521 (train.py:127, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 02:20:48]    INFO >> Epoch   1/100, train loss: 11.1664 (train.py:136, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 02:25:15]    INFO >> Epoch   1/100, valid loss: 0.0000, valid bleu4: 40.75, best bleu4: 41.25 (train.py:217, <module>())
[2021-11-07 02:25:33]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/last_checkpoint.pt (train.py:222, <module>())
[2021-11-07 02:28:13]    INFO >> Epoch   2/100, Batch 500/5302, train loss: 9.8820 (train.py:127, train())
[2021-11-07 02:31:18]    INFO >> Epoch   2/100, Batch 1000/5302, train loss: 9.8555 (train.py:127, train())
[2021-11-07 02:34:33]    INFO >> Epoch   2/100, Batch 1500/5302, train loss: 10.0645 (train.py:127, train())
[2021-11-07 02:37:43]    INFO >> Epoch   2/100, Batch 2000/5302, train loss: 10.2222 (train.py:127, train())
[2021-11-07 02:40:58]    INFO >> Epoch   2/100, Batch 2500/5302, train loss: 10.2541 (train.py:127, train())
[2021-11-07 02:44:13]    INFO >> Epoch   2/100, Batch 3000/5302, train loss: 10.2571 (train.py:127, train())
[2021-11-07 02:47:41]    INFO >> Epoch   2/100, Batch 3500/5302, train loss: 10.3077 (train.py:127, train())
[2021-11-07 02:51:03]    INFO >> Epoch   2/100, Batch 4000/5302, train loss: 10.4497 (train.py:127, train())
[2021-11-07 02:54:20]    INFO >> Epoch   2/100, Batch 4500/5302, train loss: 10.5607 (train.py:127, train())
[2021-11-07 02:57:33]    INFO >> Epoch   2/100, Batch 5000/5302, train loss: 10.6185 (train.py:127, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 02:59:33]    INFO >> Epoch   2/100, train loss: 10.6401 (train.py:136, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 03:03:46]    INFO >> Epoch   2/100, valid loss: 0.0000, valid bleu4: 39.71, best bleu4: 41.25 (train.py:217, <module>())
[2021-11-07 03:04:01]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/last_checkpoint.pt (train.py:222, <module>())
[2021-11-07 03:07:59]    INFO >> Epoch   3/100, Batch 500/5302, train loss: 9.2184 (train.py:127, train())
[2021-11-07 03:11:11]    INFO >> Epoch   3/100, Batch 1000/5302, train loss: 9.3601 (train.py:127, train())
[2021-11-07 03:14:26]    INFO >> Epoch   3/100, Batch 1500/5302, train loss: 9.4654 (train.py:127, train())
[2021-11-07 03:17:41]    INFO >> Epoch   3/100, Batch 2000/5302, train loss: 9.5000 (train.py:127, train())
[2021-11-07 03:21:00]    INFO >> Epoch   3/100, Batch 2500/5302, train loss: 9.5557 (train.py:127, train())
[2021-11-07 03:24:15]    INFO >> Epoch   3/100, Batch 3000/5302, train loss: 9.6344 (train.py:127, train())
[2021-11-07 03:28:05]    INFO >> Epoch   3/100, Batch 3500/5302, train loss: 9.7697 (train.py:127, train())
[2021-11-07 03:31:51]    INFO >> Epoch   3/100, Batch 4000/5302, train loss: 9.8384 (train.py:127, train())
[2021-11-07 03:35:42]    INFO >> Epoch   3/100, Batch 4500/5302, train loss: 9.9042 (train.py:127, train())
[2021-11-07 03:39:32]    INFO >> Epoch   3/100, Batch 5000/5302, train loss: 9.9674 (train.py:127, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 03:42:02]    INFO >> Epoch   3/100, train loss: 10.0030 (train.py:136, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 03:46:33]    INFO >> Epoch   3/100, valid loss: 0.0000, valid bleu4: 40.87, best bleu4: 41.25 (train.py:217, <module>())
[2021-11-07 03:46:50]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/last_checkpoint.pt (train.py:222, <module>())
[2021-11-07 03:51:45]    INFO >> Epoch   4/100, Batch 500/5302, train loss: 8.3826 (train.py:127, train())
[2021-11-07 03:55:43]    INFO >> Epoch   4/100, Batch 1000/5302, train loss: 8.8365 (train.py:127, train())
[2021-11-07 03:59:34]    INFO >> Epoch   4/100, Batch 1500/5302, train loss: 8.9467 (train.py:127, train())
[2021-11-07 04:03:22]    INFO >> Epoch   4/100, Batch 2000/5302, train loss: 9.0338 (train.py:127, train())
[2021-11-07 04:07:14]    INFO >> Epoch   4/100, Batch 2500/5302, train loss: 9.2319 (train.py:127, train())
[2021-11-07 04:11:02]    INFO >> Epoch   4/100, Batch 3000/5302, train loss: 9.3408 (train.py:127, train())
[2021-11-07 04:14:52]    INFO >> Epoch   4/100, Batch 3500/5302, train loss: 9.3675 (train.py:127, train())
[2021-11-07 04:18:47]    INFO >> Epoch   4/100, Batch 4000/5302, train loss: 9.3949 (train.py:127, train())
[2021-11-07 04:22:38]    INFO >> Epoch   4/100, Batch 4500/5302, train loss: 9.4181 (train.py:127, train())
[2021-11-07 04:26:26]    INFO >> Epoch   4/100, Batch 5000/5302, train loss: 9.5312 (train.py:127, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 04:29:00]    INFO >> Epoch   4/100, train loss: 9.5468 (train.py:136, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 04:33:34]    INFO >> Epoch   4/100, valid loss: 0.0000, valid bleu4: 40.86, best bleu4: 41.25 (train.py:217, <module>())
[2021-11-07 04:34:00]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/last_checkpoint.pt (train.py:222, <module>())
[2021-11-07 04:39:15]    INFO >> Epoch   5/100, Batch 500/5302, train loss: 8.5404 (train.py:127, train())
[2021-11-07 04:43:02]    INFO >> Epoch   5/100, Batch 1000/5302, train loss: 8.5317 (train.py:127, train())
[2021-11-07 04:46:44]    INFO >> Epoch   5/100, Batch 1500/5302, train loss: 8.7468 (train.py:127, train())
[2021-11-07 04:50:39]    INFO >> Epoch   5/100, Batch 2000/5302, train loss: 8.7707 (train.py:127, train())
[2021-11-07 04:54:32]    INFO >> Epoch   5/100, Batch 2500/5302, train loss: 8.8541 (train.py:127, train())
[2021-11-07 04:58:14]    INFO >> Epoch   5/100, Batch 3000/5302, train loss: 8.9695 (train.py:127, train())
[2021-11-07 05:01:59]    INFO >> Epoch   5/100, Batch 3500/5302, train loss: 9.0226 (train.py:127, train())
[2021-11-07 05:05:44]    INFO >> Epoch   5/100, Batch 4000/5302, train loss: 9.0601 (train.py:127, train())
[2021-11-07 05:09:23]    INFO >> Epoch   5/100, Batch 4500/5302, train loss: 9.1291 (train.py:127, train())
[2021-11-07 05:12:58]    INFO >> Epoch   5/100, Batch 5000/5302, train loss: 9.1318 (train.py:127, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 05:15:24]    INFO >> Epoch   5/100, train loss: 9.1370 (train.py:136, <module>())
[2021-11-07 05:15:39]    INFO >> save /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/5.pt (train.py:207, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 05:20:05]    INFO >> Epoch   5/100, valid loss: 0.0000, valid bleu4: 40.99, best bleu4: 41.25 (train.py:217, <module>())
[2021-11-07 05:20:22]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/last_checkpoint.pt (train.py:222, <module>())
[2021-11-07 05:25:22]    INFO >> Epoch   6/100, Batch 500/5302, train loss: 8.2481 (train.py:127, train())
[2021-11-07 05:29:11]    INFO >> Epoch   6/100, Batch 1000/5302, train loss: 8.3144 (train.py:127, train())
[2021-11-07 05:32:51]    INFO >> Epoch   6/100, Batch 1500/5302, train loss: 8.3923 (train.py:127, train())
[2021-11-07 05:36:37]    INFO >> Epoch   6/100, Batch 2000/5302, train loss: 8.3366 (train.py:127, train())
[2021-11-07 05:40:14]    INFO >> Epoch   6/100, Batch 2500/5302, train loss: 8.3911 (train.py:127, train())
[2021-11-07 05:44:01]    INFO >> Epoch   6/100, Batch 3000/5302, train loss: 8.4092 (train.py:127, train())
[2021-11-07 05:47:29]    INFO >> Epoch   6/100, Batch 3500/5302, train loss: 8.4575 (train.py:127, train())
[2021-11-07 05:51:15]    INFO >> Epoch   6/100, Batch 4000/5302, train loss: 8.4992 (train.py:127, train())
[2021-11-07 05:55:01]    INFO >> Epoch   6/100, Batch 4500/5302, train loss: 8.5561 (train.py:127, train())
[2021-11-07 05:58:50]    INFO >> Epoch   6/100, Batch 5000/5302, train loss: 8.6603 (train.py:127, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 06:01:11]    INFO >> Epoch   6/100, train loss: 8.6792 (train.py:136, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 06:05:31]    INFO >> Epoch   6/100, valid loss: 0.0000, valid bleu4: 41.15, best bleu4: 41.25 (train.py:217, <module>())
[2021-11-07 06:05:49]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/last_checkpoint.pt (train.py:222, <module>())
[2021-11-07 06:10:34]    INFO >> Epoch   7/100, Batch 500/5302, train loss: 7.5066 (train.py:127, train())
[2021-11-07 06:14:13]    INFO >> Epoch   7/100, Batch 1000/5302, train loss: 7.6985 (train.py:127, train())
[2021-11-07 06:17:54]    INFO >> Epoch   7/100, Batch 1500/5302, train loss: 7.8502 (train.py:127, train())
[2021-11-07 06:21:35]    INFO >> Epoch   7/100, Batch 2000/5302, train loss: 7.9754 (train.py:127, train())
[2021-11-07 06:25:20]    INFO >> Epoch   7/100, Batch 2500/5302, train loss: 7.9947 (train.py:127, train())
[2021-11-07 06:28:52]    INFO >> Epoch   7/100, Batch 3000/5302, train loss: 8.0730 (train.py:127, train())
[2021-11-07 06:32:35]    INFO >> Epoch   7/100, Batch 3500/5302, train loss: 8.1268 (train.py:127, train())
[2021-11-07 06:36:19]    INFO >> Epoch   7/100, Batch 4000/5302, train loss: 8.1707 (train.py:127, train())
[2021-11-07 06:39:59]    INFO >> Epoch   7/100, Batch 4500/5302, train loss: 8.1857 (train.py:127, train())
[2021-11-07 06:43:47]    INFO >> Epoch   7/100, Batch 5000/5302, train loss: 8.2161 (train.py:127, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 06:46:05]    INFO >> Epoch   7/100, train loss: 8.2184 (train.py:136, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 06:50:41]    INFO >> Epoch   7/100, valid loss: 0.0000, valid bleu4: 40.44, best bleu4: 41.25 (train.py:217, <module>())
[2021-11-07 06:51:01]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/last_checkpoint.pt (train.py:222, <module>())
[2021-11-07 06:55:50]    INFO >> Epoch   8/100, Batch 500/5302, train loss: 7.4682 (train.py:127, train())
[2021-11-07 06:59:20]    INFO >> Epoch   8/100, Batch 1000/5302, train loss: 7.6120 (train.py:127, train())
[2021-11-07 07:03:06]    INFO >> Epoch   8/100, Batch 1500/5302, train loss: 7.5527 (train.py:127, train())
[2021-11-07 07:06:32]    INFO >> Epoch   8/100, Batch 2000/5302, train loss: 7.5748 (train.py:127, train())
[2021-11-07 07:10:15]    INFO >> Epoch   8/100, Batch 2500/5302, train loss: 7.6650 (train.py:127, train())
[2021-11-07 07:13:54]    INFO >> Epoch   8/100, Batch 3000/5302, train loss: 7.7233 (train.py:127, train())
[2021-11-07 07:17:36]    INFO >> Epoch   8/100, Batch 3500/5302, train loss: 7.7155 (train.py:127, train())
[2021-11-07 07:21:19]    INFO >> Epoch   8/100, Batch 4000/5302, train loss: 7.7172 (train.py:127, train())
[2021-11-07 07:25:13]    INFO >> Epoch   8/100, Batch 4500/5302, train loss: 7.7104 (train.py:127, train())
[2021-11-07 07:28:55]    INFO >> Epoch   8/100, Batch 5000/5302, train loss: 7.7464 (train.py:127, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 07:31:20]    INFO >> Epoch   8/100, train loss: 7.7701 (train.py:136, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 07:35:40]    INFO >> Epoch   8/100, valid loss: 0.0000, valid bleu4: 40.09, best bleu4: 41.25 (train.py:217, <module>())
[2021-11-07 07:35:59]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/last_checkpoint.pt (train.py:222, <module>())
[2021-11-07 07:40:49]    INFO >> Epoch   9/100, Batch 500/5302, train loss: 7.2268 (train.py:127, train())
[2021-11-07 07:44:41]    INFO >> Epoch   9/100, Batch 1000/5302, train loss: 7.0217 (train.py:127, train())
[2021-11-07 07:48:31]    INFO >> Epoch   9/100, Batch 1500/5302, train loss: 7.1747 (train.py:127, train())
[2021-11-07 07:51:26]    INFO >> Epoch   9/100, Batch 2000/5302, train loss: 7.3188 (train.py:127, train())
[2021-11-07 07:54:13]    INFO >> Epoch   9/100, Batch 2500/5302, train loss: 7.3862 (train.py:127, train())
[2021-11-07 07:56:53]    INFO >> Epoch   9/100, Batch 3000/5302, train loss: 7.4249 (train.py:127, train())
[2021-11-07 07:59:23]    INFO >> Epoch   9/100, Batch 3500/5302, train loss: 7.4441 (train.py:127, train())
[2021-11-07 08:02:14]    INFO >> Epoch   9/100, Batch 4000/5302, train loss: 7.4072 (train.py:127, train())
[2021-11-07 08:04:40]    INFO >> Epoch   9/100, Batch 4500/5302, train loss: 7.4136 (train.py:127, train())
[2021-11-07 08:07:18]    INFO >> Epoch   9/100, Batch 5000/5302, train loss: 7.4178 (train.py:127, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 08:09:11]    INFO >> Epoch   9/100, train loss: 7.4209 (train.py:136, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 08:12:14]    INFO >> Epoch   9/100, valid loss: 0.0000, valid bleu4: 40.35, best bleu4: 41.25 (train.py:217, <module>())
[2021-11-07 08:12:31]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/last_checkpoint.pt (train.py:222, <module>())
[2021-11-07 08:18:07]    INFO >> Epoch  10/100, Batch 500/5302, train loss: 6.5143 (train.py:127, train())
[2021-11-07 08:22:21]    INFO >> Epoch  10/100, Batch 1000/5302, train loss: 6.6323 (train.py:127, train())
[2021-11-07 08:26:38]    INFO >> Epoch  10/100, Batch 1500/5302, train loss: 6.8225 (train.py:127, train())
[2021-11-07 08:30:51]    INFO >> Epoch  10/100, Batch 2000/5302, train loss: 6.8799 (train.py:127, train())
[2021-11-07 08:34:52]    INFO >> Epoch  10/100, Batch 2500/5302, train loss: 6.8933 (train.py:127, train())
[2021-11-07 08:39:23]    INFO >> Epoch  10/100, Batch 3000/5302, train loss: 6.9195 (train.py:127, train())
[2021-11-07 08:43:37]    INFO >> Epoch  10/100, Batch 3500/5302, train loss: 6.9187 (train.py:127, train())
[2021-11-07 08:47:36]    INFO >> Epoch  10/100, Batch 4000/5302, train loss: 6.9427 (train.py:127, train())
[2021-11-07 08:51:41]    INFO >> Epoch  10/100, Batch 4500/5302, train loss: 7.0053 (train.py:127, train())
[2021-11-07 08:55:38]    INFO >> Epoch  10/100, Batch 5000/5302, train loss: 7.0567 (train.py:127, train())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 08:58:20]    INFO >> Epoch  10/100, train loss: 7.0785 (train.py:136, <module>())
[2021-11-07 08:58:38]    INFO >> save /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/10.pt (train.py:207, <module>())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-11-07 09:03:39]    INFO >> Epoch  10/100, valid loss: 0.0000, valid bleu4: 40.60, best bleu4: 41.25 (train.py:217, <module>())
[2021-11-07 09:03:55]    INFO >> update /home/wanyao/yang/ncc_data/avatar/translation/top3/graphcodebert/data-mmap/java-python/last_checkpoint.pt (train.py:222, <module>())
[2021-11-07 09:03:55]    INFO >> early stop because of no improvement in 10 epochs (train.py:224, <module>())
