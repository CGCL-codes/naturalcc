{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入一些有用的模块\n",
    "import gc\n",
    "import shutil\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import configs\n",
    "import src.data as data\n",
    "import src.prepare as prepare\n",
    "import src.process as process\n",
    "import src.utils.functions.cpg as cpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入配置信息\n",
    "PATHS = configs.Paths()\n",
    "FILES = configs.Files()\n",
    "DEVICE = FILES.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始的代码数据集存放在 data/raw (Paths.raw)中\n",
    "# 每一条数据包含：\n",
    "# project名，commit_id，target（是否为漏洞代码），func（函数代码文本）\n",
    "# 总共27318条数据，为了节约时间本实验中只使用其中的200条\n",
    "# 可以尝试修改参数DATA_SELETED来提高数据量\n",
    "# 并使用joern工具将代码数据转化为CPG图\n",
    "# 感兴趣的同学可以了解一下joern： https://docs.joern.io/home\n",
    "# 代码图数据存放在data/cpg (Paths.cpg)中，其中每100条数据写入一个pkl文件中\n",
    "DATA_SELETED=200\n",
    "def select(dataset):\n",
    "    result = dataset.loc[dataset['project'] == \"FFmpeg\"]\n",
    "    len_filter = result.func.str.len() < 1200\n",
    "    result = result.loc[len_filter]\n",
    "    #print(len(result))\n",
    "    #result = result.iloc[11001:]\n",
    "    #print(len(result))\n",
    "    # 暂时只使用前DATA_SELETED条数据\n",
    "    result = result.head(DATA_SELETED)\n",
    "\n",
    "    return result\n",
    "\n",
    "shutil.rmtree(PATHS.joern, True)\n",
    "context = configs.Create()\n",
    "raw = data.read(PATHS.raw, FILES.raw)\n",
    "filtered = data.apply_filter(raw, select)\n",
    "filtered = data.clean(filtered)\n",
    "assert DATA_SELETED <= len(filtered)\n",
    "data.drop(filtered, [\"commit_id\", \"project\"])\n",
    "slices = data.slice_frame(filtered, context.slice_size)\n",
    "slices = [(s, slice.apply(lambda x: x)) for s, slice in slices]\n",
    "\n",
    "cpg_files = []\n",
    "# Create CPG binary files\n",
    "for s, slice in slices:\n",
    "    data.to_files(slice, PATHS.joern)\n",
    "    cpg_file = prepare.joern_parse(context.joern_cli_dir, PATHS.joern, PATHS.cpg, f\"{s}_{FILES.cpg}\")\n",
    "    cpg_files.append(cpg_file)\n",
    "    print(f\"Dataset {s} to cpg.\")\n",
    "    shutil.rmtree(PATHS.joern)\n",
    "# Create CPG with graphs json files\n",
    "json_files = prepare.joern_create(context.joern_cli_dir, PATHS.cpg, PATHS.cpg, cpg_files)\n",
    "for (s, slice), json_file in zip(slices, json_files):\n",
    "    graphs = prepare.json_process(PATHS.cpg, json_file)\n",
    "    if graphs is None:\n",
    "        print(f\"Dataset chunk {s} not processed.\")\n",
    "        continue\n",
    "    dataset = data.create_with_index(graphs, [\"Index\", \"cpg\"])\n",
    "    dataset = data.inner_join_by_index(slice, dataset)\n",
    "    print(f\"Writing cpg dataset chunk {s}.\")\n",
    "    data.write(dataset, PATHS.cpg, f\"{s}_{FILES.cpg}.pkl\")\n",
    "    del dataset\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练用于编码节点中文本的word2vec模型\n",
    "\n",
    "context = configs.Embed()\n",
    "# 对源代码进行分词\n",
    "dataset_files = data.get_directory_files(PATHS.cpg)\n",
    "w2vmodel = Word2Vec(**context.w2v_args)\n",
    "w2v_init = True\n",
    "for pkl_file in dataset_files:\n",
    "    file_name = pkl_file.split(\".\")[0]\n",
    "    cpg_dataset = data.load(PATHS.cpg, pkl_file)\n",
    "    tokens_dataset = data.tokenize(cpg_dataset)\n",
    "    data.write(tokens_dataset, PATHS.tokens, f\"{file_name}_{FILES.tokens}\")\n",
    "    # 使用word2vec去学习节点文本的初始编码\n",
    "    w2vmodel.build_vocab(sentences=tokens_dataset.tokens, update=not w2v_init)\n",
    "    w2vmodel.train(tokens_dataset.tokens, total_examples=w2vmodel.corpus_count, epochs=1)\n",
    "    if w2v_init:\n",
    "        w2v_init = False\n",
    "    # 对CPG图中的节点文本进行编码并进行存储\n",
    "    cpg_dataset[\"nodes\"] = cpg_dataset.apply(lambda row: cpg.parse_to_nodes(row.cpg, context.nodes_dim), axis=1)\n",
    "    # 去掉没有节点的数据\n",
    "    cpg_dataset = cpg_dataset.loc[cpg_dataset.nodes.map(len) > 0]\n",
    "    cpg_dataset[\"input\"] = cpg_dataset.apply(lambda row: prepare.nodes_to_input(row.nodes, row.target, context.nodes_dim,\n",
    "                                                                                w2vmodel.wv, context.edge_type), axis=1)\n",
    "    data.drop(cpg_dataset, [\"nodes\"])\n",
    "    print(f\"Saving input dataset {file_name} with size {len(cpg_dataset)}.\")\n",
    "    data.write(cpg_dataset[[\"input\", \"target\"]], PATHS.input, f\"{file_name}_{FILES.input}\")\n",
    "    del cpg_dataset\n",
    "    gc.collect()\n",
    "print(\"Saving w2vmodel.\")\n",
    "w2vmodel.save(f\"{PATHS.w2v}/{FILES.w2v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练和验证\n",
    "context = configs.Process()\n",
    "devign = configs.Devign()\n",
    "model_path = PATHS.model + FILES.model\n",
    "model = process.Devign(path=model_path, device=DEVICE, model=devign.model, learning_rate=devign.learning_rate,\n",
    "                        weight_decay=devign.weight_decay,\n",
    "                        loss_lambda=devign.loss_lambda)\n",
    "train = process.Train(model, context.epochs)\n",
    "input_dataset = data.loads(PATHS.input)\n",
    "# 划分数据集并且使用DataLoad来加载数据\n",
    "train_loader, val_loader, test_loader = list(\n",
    "    map(lambda x: x.get_loader(context.batch_size, shuffle=context.shuffle),\n",
    "        data.train_val_test_split(input_dataset, shuffle=context.shuffle)))\n",
    "train_loader_step = process.LoaderStep(\"Train\", train_loader, DEVICE)\n",
    "val_loader_step = process.LoaderStep(\"Validation\", val_loader, DEVICE)\n",
    "test_loader_step = process.LoaderStep(\"Test\", test_loader, DEVICE)\n",
    "\n",
    "train(train_loader_step, val_loader_step)\n",
    "model.save()\n",
    "\n",
    "process.predict(model, test_loader_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.0 ('devign')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe92802436f3b6fdcf13a5e1be3c70c9006e1b2a17e4adb5b5980b4305bb4040"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
