nohup: ignoring input
Using backend: pytorch
[2021-09-26 17:25:50]    INFO >> Load arguments in /home/wanyao/yang/naturalcc-master/run/retrieval/selfattn/config/csn/all.yml (train.py:302, cli_main())
[2021-09-26 17:25:50]    INFO >> {'criterion': 'retrieval_softmax', 'optimizer': 'torch_adam', 'lr_scheduler': 'fixed', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 1000, 'log_format': 'simple', 'tensorboard_logdir': '', 'seed': 0, 'cpu': 0, 'fp16': 0, 'memory_efficient_fp16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'hybrid_retrieval'}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 0, 'max_tokens': None, 'max_sentences': 1000, 'code_max_tokens': 200, 'query_max_tokens': 30, 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': None, 'max_tokens_valid': None, 'max_sentences_valid': 1000, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 0, 'langs': ['go', 'java', 'javascript', 'ruby', 'python', 'php']}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500, 'local_rank': -1, 'block_momentum': 0.875, 'block_lr': 1, 'use_nbm': 0, 'average_sync': 0}, 'task': {'data': '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code_tokens', 'target_lang': 'docstring_tokens', 'source_aux_lang': 'code_tokens.wo_func', 'target_aux_lang': 'func_name', 'fraction_using_func_name': 0.1, 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'upsample_primary': 1, 'truncate_source': 0, 'eval_mrr': 1}, 'model': {'arch': 'self_attn', 'code_embed_dim': 128, 'code_token_types': 1, 'code_max_tokens': 200, 'code_position_encoding': 'learned', 'code_dropout': 0.1, 'code_pooling': 'weighted_mean', 'query_embed_dim': 128, 'query_token_types': 1, 'query_max_tokens': 30, 'query_self_attn_layers': 3, 'query_pooling': 'weighted_mean', 'query_position_encoding': 'learned', 'query_dropout': 0.1, 'self_attn_layers': 3, 'attention_heads': 8, 'ffn_embed_dim': 512, 'activation_fn': 'gelu'}, 'optimization': {'max_epoch': 300, 'max_update': 0, 'clip_norm': 1, 'sentence_avg': 0, 'update_freq': [1], 'lrs': [0.0005], 'min_lr': -1, 'use_bmuf': 0, 'lr_shrink': 1.0, 'force_anneal': 0, 'warmup_updates': 0, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0, 'use_old_adam': 1}, 'margin': 1, 'clip_norm_version': 'tf_clip_by_global_norm'}, 'checkpoint': {'save_dir': '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 1, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'mrr', 'maximize_best_checkpoint_metric': 1, 'patience': 5}, 'eval': {'path': '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_best.pt', 'quiet': 1, 'max_sentences': 1000, 'model_overrides': '{}', 'eval_size': -1}} (train.py:304, cli_main())
[2021-09-26 17:25:50]    INFO >> single GPU training... (train.py:333, cli_main())
[2021-09-26 17:25:51]    INFO >> loaded 89154 examples from: ['/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/valid.go.code_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/valid.java.code_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/valid.javascript.code_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/valid.ruby.code_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/valid.python.code_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/valid.php.code_tokens'] (hybrid_retrieval.py:54, load_tokens_dataset())
[2021-09-26 17:25:51]    INFO >> loaded 89154 examples from: ['/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/valid.go.docstring_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/valid.java.docstring_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/valid.javascript.docstring_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/valid.ruby.docstring_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/valid.python.docstring_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/valid.php.docstring_tokens'] (hybrid_retrieval.py:55, load_tokens_dataset())
[2021-09-26 17:25:51]    INFO >> SelfAttn(
  (src_encoders): ModuleDict(
    (go): SelfAttnEncoder(
      (embed): Embedding(10000, 128, padding_idx=0)
      (type_embed): Embedding(1, 128)
      (embed_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
      )
      (weight_layer): Linear(in_features=128, out_features=1, bias=False)
    )
    (java): SelfAttnEncoder(
      (embed): Embedding(10000, 128, padding_idx=0)
      (type_embed): Embedding(1, 128)
      (embed_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
      )
      (weight_layer): Linear(in_features=128, out_features=1, bias=False)
    )
    (javascript): SelfAttnEncoder(
      (embed): Embedding(10000, 128, padding_idx=0)
      (type_embed): Embedding(1, 128)
      (embed_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
      )
      (weight_layer): Linear(in_features=128, out_features=1, bias=False)
    )
    (ruby): SelfAttnEncoder(
      (embed): Embedding(10000, 128, padding_idx=0)
      (type_embed): Embedding(1, 128)
      (embed_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
      )
      (weight_layer): Linear(in_features=128, out_features=1, bias=False)
    )
    (python): SelfAttnEncoder(
      (embed): Embedding(10000, 128, padding_idx=0)
      (type_embed): Embedding(1, 128)
      (embed_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
      )
      (weight_layer): Linear(in_features=128, out_features=1, bias=False)
    )
    (php): SelfAttnEncoder(
      (embed): Embedding(10000, 128, padding_idx=0)
      (type_embed): Embedding(1, 128)
      (embed_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        )
      )
      (weight_layer): Linear(in_features=128, out_features=1, bias=False)
    )
  )
  (tgt_encoders): SelfAttnEncoder(
    (embed): Embedding(10000, 128, padding_idx=0)
    (type_embed): Embedding(1, 128)
    (embed_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (ff_layer_norm): LayerNorm((128,), eps=1e-08, elementwise_affine=True)
      )
    )
    (weight_layer): Linear(in_features=128, out_features=1, bias=False)
  )
) (train.py:223, single_main())
[2021-09-26 17:25:51]    INFO >> model self_attn, criterion SearchSoftmaxCriterion (train.py:224, single_main())
[2021-09-26 17:25:51]    INFO >> num. model params: 13284736 (num. trained: 13284736) (train.py:225, single_main())
[2021-09-26 17:25:57]    INFO >> training on 1 GPUs (train.py:233, single_main())
[2021-09-26 17:25:57]    INFO >> max tokens per GPU = None and max sentences per GPU = 1000 (train.py:234, single_main())
[2021-09-26 17:25:57]    INFO >> no existing checkpoint found /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_last.pt (ncc_trainers.py:270, load_checkpoint())
[2021-09-26 17:25:57]    INFO >> loading train data for epoch 1 (ncc_trainers.py:285, get_train_iterator())
[2021-09-26 17:25:58]    INFO >> loaded 1880853 examples from: ['/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.go.code_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.java.code_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.javascript.code_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.ruby.code_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.python.code_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.php.code_tokens'] (hybrid_retrieval.py:54, load_tokens_dataset())
[2021-09-26 17:25:58]    INFO >> loaded 1880853 examples from: ['/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.go.docstring_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.java.docstring_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.javascript.docstring_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.ruby.docstring_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.python.docstring_tokens', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.php.docstring_tokens'] (hybrid_retrieval.py:55, load_tokens_dataset())
[2021-09-26 17:25:59]    INFO >> loaded 1880853 examples from: ['/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.go.code_tokens.wo_func', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.java.code_tokens.wo_func', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.javascript.code_tokens.wo_func', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.ruby.code_tokens.wo_func', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.python.code_tokens.wo_func', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.php.code_tokens.wo_func'] (hybrid_retrieval.py:67, load_tokens_dataset())
[2021-09-26 17:25:59]    INFO >> loaded 1880853 examples from: ['/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.go.func_name', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.java.func_name', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.javascript.func_name', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.ruby.func_name', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.python.func_name', '/home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/train.php.func_name'] (hybrid_retrieval.py:81, load_tokens_dataset())
[2021-09-26 17:26:01]    INFO >> NOTE: your device may support faster training with fp16 (ncc_trainers.py:155, _setup_optimizer())
[2021-09-26 17:34:19]    INFO >> epoch 001:   1000 / 1881 loss=3.102, mrr=514.001, sample_size=1000, wps=2061.7, ups=2.06, wpb=1000, bsz=1000, num_updates=1000, lr=0.0005, gnorm=8.697, clip=100, train_wall=473, wall=502 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 17:41:26]    INFO >> epoch 001 | loss 2.391 | mrr 0.617992 | sample_size 1000 | wps 2062.4 | ups 2.06 | wpb 1000 | bsz 1000 | num_updates 1880 | lr 0.0005 | gnorm 7.055 | clip 100 | train_wall 888 | wall 928 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 17:41:45]    INFO >> epoch 001 | valid on 'valid' subset | loss 1.814 | mrr 0.705152 | sample_size 1000 | wps 7509 | wpb 1000 | bsz 1000 | num_updates 1880 (progress_bar.py:269, print())
[2021-09-26 17:41:46]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_best.pt (epoch 1 @ 1880 updates, score 0.705152) (writing took 0.557090 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 17:42:56]    INFO >> epoch 002:    120 / 1881 loss=1.533, mrr=744.196, sample_size=1000, wps=1935.5, ups=1.94, wpb=1000, bsz=1000, num_updates=2000, lr=0.0005, gnorm=5.099, clip=100, train_wall=472, wall=1018 (progress_bar.py:260, log())
[2021-09-26 17:51:00]    INFO >> epoch 002:   1121 / 1881 loss=1.171, mrr=801.29, sample_size=1000, wps=2065.3, ups=2.07, wpb=1000, bsz=1000, num_updates=3000, lr=0.0005, gnorm=4.144, clip=100, train_wall=472, wall=1502 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 17:57:05]    INFO >> epoch 002 | loss 1.149 | mrr 0.804738 | sample_size 1000 | wps 2002.2 | ups 2 | wpb 1000 | bsz 1000 | num_updates 3760 | lr 0.0005 | gnorm 3.985 | clip 100 | train_wall 884 | wall 1867 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 17:57:24]    INFO >> epoch 002 | valid on 'valid' subset | loss 1.628 | mrr 0.736248 | sample_size 1000 | wps 7480 | wpb 1000 | bsz 1000 | num_updates 3760 | best_mrr 736.248 (progress_bar.py:269, print())
[2021-09-26 17:57:25]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_best.pt (epoch 2 @ 3760 updates, score 0.736248) (writing took 0.806526 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 17:59:33]    INFO >> epoch 003:    240 / 1881 loss=1.06, mrr=818.892, sample_size=1000, wps=1948.5, ups=1.95, wpb=1000, bsz=1000, num_updates=4000, lr=0.0005, gnorm=3.622, clip=100, train_wall=468, wall=2016 (progress_bar.py:260, log())
[2021-09-26 18:07:37]    INFO >> epoch 003:   1240 / 1881 loss=0.908, mrr=843.015, sample_size=1000, wps=2065.6, ups=2.07, wpb=1000, bsz=1000, num_updates=5000, lr=0.0005, gnorm=3.321, clip=100, train_wall=472, wall=2500 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 18:12:47]    INFO >> epoch 003 | loss 0.908 | mrr 0.843097 | sample_size 1000 | wps 1994.2 | ups 1.99 | wpb 1000 | bsz 1000 | num_updates 5640 | lr 0.0005 | gnorm 3.258 | clip 100 | train_wall 887 | wall 2810 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 18:13:07]    INFO >> epoch 003 | valid on 'valid' subset | loss 1.595 | mrr 0.747512 | sample_size 1000 | wps 7478.9 | wpb 1000 | bsz 1000 | num_updates 5640 | best_mrr 747.512 (progress_bar.py:269, print())
[2021-09-26 18:13:07]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_best.pt (epoch 3 @ 5640 updates, score 0.747512) (writing took 0.781461 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 18:16:14]    INFO >> epoch 004:    360 / 1881 loss=0.852, mrr=851.979, sample_size=1000, wps=1935.3, ups=1.94, wpb=1000, bsz=1000, num_updates=6000, lr=0.0005, gnorm=3.036, clip=100, train_wall=472, wall=3016 (progress_bar.py:260, log())
[2021-09-26 18:24:18]    INFO >> epoch 004:   1361 / 1881 loss=0.778, mrr=863.777, sample_size=1000, wps=2066.1, ups=2.07, wpb=1000, bsz=1000, num_updates=7000, lr=0.0005, gnorm=2.89, clip=100, train_wall=472, wall=3501 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 18:28:30]    INFO >> epoch 004 | loss 0.776 | mrr 0.863873 | sample_size 1000 | wps 1994.1 | ups 1.99 | wpb 1000 | bsz 1000 | num_updates 7520 | lr 0.0005 | gnorm 2.863 | clip 100 | train_wall 887 | wall 3753 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 18:28:49]    INFO >> epoch 004 | valid on 'valid' subset | loss 1.601 | mrr 0.750533 | sample_size 1000 | wps 7511.8 | wpb 1000 | bsz 1000 | num_updates 7520 | best_mrr 750.533 (progress_bar.py:269, print())
[2021-09-26 18:28:50]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_best.pt (epoch 4 @ 7520 updates, score 0.750533) (writing took 0.882650 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 18:32:55]    INFO >> epoch 005:    481 / 1881 loss=0.731, mrr=871.341, sample_size=1000, wps=1933.4, ups=1.93, wpb=1000, bsz=1000, num_updates=8000, lr=0.0005, gnorm=2.704, clip=100, train_wall=472, wall=4018 (progress_bar.py:260, log())
[2021-09-26 18:40:59]    INFO >> epoch 005:   1481 / 1881 loss=0.697, mrr=876.342, sample_size=1000, wps=2067.2, ups=2.07, wpb=1000, bsz=1000, num_updates=9000, lr=0.0005, gnorm=2.629, clip=100, train_wall=472, wall=4501 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 18:44:13]    INFO >> epoch 005 | loss 0.691 | mrr 0.877469 | sample_size 1000 | wps 1993.2 | ups 1.99 | wpb 1000 | bsz 1000 | num_updates 9400 | lr 0.0005 | gnorm 2.615 | clip 100 | train_wall 888 | wall 4696 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 18:44:33]    INFO >> epoch 005 | valid on 'valid' subset | loss 1.641 | mrr 0.75279 | sample_size 1000 | wps 7508.3 | wpb 1000 | bsz 1000 | num_updates 9400 | best_mrr 752.79 (progress_bar.py:269, print())
[2021-09-26 18:44:33]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_best.pt (epoch 5 @ 9400 updates, score 0.75279) (writing took 0.901738 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 18:49:36]    INFO >> epoch 006:    600 / 1881 loss=0.647, mrr=884.706, sample_size=1000, wps=1934.7, ups=1.93, wpb=1000, bsz=1000, num_updates=10000, lr=0.0005, gnorm=2.492, clip=100, train_wall=472, wall=5018 (progress_bar.py:260, log())
[2021-09-26 18:57:38]    INFO >> epoch 006:   1601 / 1881 loss=0.639, mrr=885.392, sample_size=1000, wps=2073.9, ups=2.07, wpb=1000, bsz=1000, num_updates=11000, lr=0.0005, gnorm=2.461, clip=100, train_wall=470, wall=5501 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 18:59:54]    INFO >> epoch 006 | loss 0.628 | mrr 0.887471 | sample_size 1000 | wps 1998.1 | ups 2 | wpb 1000 | bsz 1000 | num_updates 11280 | lr 0.0005 | gnorm 2.443 | clip 100 | train_wall 885 | wall 5637 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 19:00:13]    INFO >> epoch 006 | valid on 'valid' subset | loss 1.673 | mrr 0.752052 | sample_size 1000 | wps 7522.5 | wpb 1000 | bsz 1000 | num_updates 11280 | best_mrr 752.052 (progress_bar.py:269, print())
[2021-09-26 19:00:14]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_last.pt (epoch 6 @ 11280 updates, score 0.752052) (writing took 0.617424 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 19:06:15]    INFO >> epoch 007:    721 / 1881 loss=0.582, mrr=894.968, sample_size=1000, wps=1934.8, ups=1.93, wpb=1000, bsz=1000, num_updates=12000, lr=0.0005, gnorm=2.328, clip=100, train_wall=472, wall=6017 (progress_bar.py:260, log())
[2021-09-26 19:14:19]    INFO >> epoch 007:   1721 / 1881 loss=0.596, mrr=892.527, sample_size=1000, wps=2065.7, ups=2.07, wpb=1000, bsz=1000, num_updates=13000, lr=0.0005, gnorm=2.326, clip=100, train_wall=472, wall=6502 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 19:15:37]    INFO >> epoch 007 | loss 0.581 | mrr 0.894962 | sample_size 1000 | wps 1994.1 | ups 1.99 | wpb 1000 | bsz 1000 | num_updates 13160 | lr 0.0005 | gnorm 2.311 | clip 100 | train_wall 888 | wall 6580 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 19:15:56]    INFO >> epoch 007 | valid on 'valid' subset | loss 1.704 | mrr 0.752451 | sample_size 1000 | wps 7496.9 | wpb 1000 | bsz 1000 | num_updates 13160 | best_mrr 752.451 (progress_bar.py:269, print())
[2021-09-26 19:15:57]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_last.pt (epoch 7 @ 13160 updates, score 0.752451) (writing took 0.554403 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 19:22:54]    INFO >> epoch 008:    840 / 1881 loss=0.536, mrr=902.476, sample_size=1000, wps=1940.2, ups=1.94, wpb=1000, bsz=1000, num_updates=14000, lr=0.0005, gnorm=2.199, clip=100, train_wall=471, wall=7017 (progress_bar.py:260, log())
[2021-09-26 19:30:55]    INFO >> epoch 008:   1841 / 1881 loss=0.564, mrr=897.256, sample_size=1000, wps=2082.1, ups=2.08, wpb=1000, bsz=1000, num_updates=15000, lr=0.0005, gnorm=2.221, clip=100, train_wall=468, wall=7497 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 19:31:14]    INFO >> epoch 008 | loss 0.545 | mrr 0.900775 | sample_size 1000 | wps 2005.3 | ups 2.01 | wpb 1000 | bsz 1000 | num_updates 15040 | lr 0.0005 | gnorm 2.201 | clip 100 | train_wall 882 | wall 7517 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 19:31:34]    INFO >> epoch 008 | valid on 'valid' subset | loss 1.726 | mrr 0.751988 | sample_size 1000 | wps 7467.5 | wpb 1000 | bsz 1000 | num_updates 15040 | best_mrr 751.988 (progress_bar.py:269, print())
[2021-09-26 19:31:34]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_last.pt (epoch 8 @ 15040 updates, score 0.751988) (writing took 0.581950 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 19:39:31]    INFO >> epoch 009:    961 / 1881 loss=0.498, mrr=908.598, sample_size=1000, wps=1936, ups=1.94, wpb=1000, bsz=1000, num_updates=16000, lr=0.0005, gnorm=2.088, clip=100, train_wall=472, wall=8014 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 19:46:57]    INFO >> epoch 009 | loss 0.515 | mrr 0.905475 | sample_size 1000 | wps 1995.3 | ups 2 | wpb 1000 | bsz 1000 | num_updates 16920 | lr 0.0005 | gnorm 2.107 | clip 100 | train_wall 887 | wall 8459 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 19:47:16]    INFO >> epoch 009 | valid on 'valid' subset | loss 1.773 | mrr 0.752111 | sample_size 1000 | wps 7510.7 | wpb 1000 | bsz 1000 | num_updates 16920 | best_mrr 752.111 (progress_bar.py:269, print())
[2021-09-26 19:47:17]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_last.pt (epoch 9 @ 16920 updates, score 0.752111) (writing took 0.619714 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 19:48:08]    INFO >> epoch 010:     80 / 1881 loss=0.529, mrr=902.95, sample_size=1000, wps=1936, ups=1.94, wpb=1000, bsz=1000, num_updates=17000, lr=0.0005, gnorm=2.116, clip=100, train_wall=472, wall=8530 (progress_bar.py:260, log())
[2021-09-26 19:56:11]    INFO >> epoch 010:   1081 / 1881 loss=0.476, mrr=912.171, sample_size=1000, wps=2067.6, ups=2.07, wpb=1000, bsz=1000, num_updates=18000, lr=0.0005, gnorm=2.021, clip=100, train_wall=471, wall=9014 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 20:02:40]    INFO >> epoch 010 | loss 0.49 | mrr 0.90957 | sample_size 1000 | wps 1992.4 | ups 1.99 | wpb 1000 | bsz 1000 | num_updates 18800 | lr 0.0005 | gnorm 2.029 | clip 100 | train_wall 888 | wall 9403 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 20:02:59]    INFO >> epoch 010 | valid on 'valid' subset | loss 1.795 | mrr 0.75323 | sample_size 1000 | wps 7466 | wpb 1000 | bsz 1000 | num_updates 18800 | best_mrr 753.23 (progress_bar.py:269, print())
[2021-09-26 20:03:00]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_best.pt (epoch 10 @ 18800 updates, score 0.75323) (writing took 0.997272 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 20:04:49]    INFO >> epoch 011:    200 / 1881 loss=0.496, mrr=908.607, sample_size=1000, wps=1930.1, ups=1.93, wpb=1000, bsz=1000, num_updates=19000, lr=0.0005, gnorm=2.011, clip=100, train_wall=473, wall=9532 (progress_bar.py:260, log())
[2021-09-26 20:12:54]    INFO >> epoch 011:   1200 / 1881 loss=0.461, mrr=914.4, sample_size=1000, wps=2063.5, ups=2.06, wpb=1000, bsz=1000, num_updates=20000, lr=0.0005, gnorm=1.947, clip=100, train_wall=472, wall=10017 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 20:18:24]    INFO >> epoch 011 | loss 0.469 | mrr 0.912936 | sample_size 1000 | wps 1992.9 | ups 1.99 | wpb 1000 | bsz 1000 | num_updates 20680 | lr 0.0005 | gnorm 1.952 | clip 100 | train_wall 887 | wall 10346 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 20:18:43]    INFO >> epoch 011 | valid on 'valid' subset | loss 1.867 | mrr 0.750698 | sample_size 1000 | wps 7492.9 | wpb 1000 | bsz 1000 | num_updates 20680 | best_mrr 750.698 (progress_bar.py:269, print())
[2021-09-26 20:18:43]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_last.pt (epoch 11 @ 20680 updates, score 0.750698) (writing took 0.597081 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 20:21:31]    INFO >> epoch 012:    320 / 1881 loss=0.47, mrr=912.597, sample_size=1000, wps=1934.2, ups=1.93, wpb=1000, bsz=1000, num_updates=21000, lr=0.0005, gnorm=1.936, clip=100, train_wall=472, wall=10534 (progress_bar.py:260, log())
[2021-09-26 20:29:36]    INFO >> epoch 012:   1321 / 1881 loss=0.45, mrr=916.027, sample_size=1000, wps=2063.5, ups=2.06, wpb=1000, bsz=1000, num_updates=22000, lr=0.0005, gnorm=1.907, clip=100, train_wall=472, wall=11018 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 20:34:08]    INFO >> epoch 012 | loss 0.452 | mrr 0.91552 | sample_size 1000 | wps 1990.7 | ups 1.99 | wpb 1000 | bsz 1000 | num_updates 22560 | lr 0.0005 | gnorm 1.902 | clip 100 | train_wall 889 | wall 11291 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 20:34:27]    INFO >> epoch 012 | valid on 'valid' subset | loss 1.885 | mrr 0.750422 | sample_size 1000 | wps 7508.5 | wpb 1000 | bsz 1000 | num_updates 22560 | best_mrr 750.422 (progress_bar.py:269, print())
[2021-09-26 20:34:28]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_last.pt (epoch 12 @ 22560 updates, score 0.750422) (writing took 0.698244 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 20:38:14]    INFO >> epoch 013:    440 / 1881 loss=0.447, mrr=916.389, sample_size=1000, wps=1930.8, ups=1.93, wpb=1000, bsz=1000, num_updates=23000, lr=0.0005, gnorm=1.869, clip=100, train_wall=473, wall=11536 (progress_bar.py:260, log())
[2021-09-26 20:46:18]    INFO >> epoch 013:   1441 / 1881 loss=0.437, mrr=917.99, sample_size=1000, wps=2063.2, ups=2.06, wpb=1000, bsz=1000, num_updates=24000, lr=0.0005, gnorm=1.858, clip=100, train_wall=472, wall=12021 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 20:49:53]    INFO >> epoch 013 | loss 0.436 | mrr 0.918151 | sample_size 1000 | wps 1989.1 | ups 1.99 | wpb 1000 | bsz 1000 | num_updates 24440 | lr 0.0005 | gnorm 1.846 | clip 100 | train_wall 889 | wall 12236 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 20:50:12]    INFO >> epoch 013 | valid on 'valid' subset | loss 1.889 | mrr 0.749926 | sample_size 1000 | wps 7474.2 | wpb 1000 | bsz 1000 | num_updates 24440 | best_mrr 749.926 (progress_bar.py:269, print())
[2021-09-26 20:50:13]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_last.pt (epoch 13 @ 24440 updates, score 0.749926) (writing took 0.612393 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 20:54:56]    INFO >> epoch 014:    561 / 1881 loss=0.426, mrr=919.94, sample_size=1000, wps=1930.8, ups=1.93, wpb=1000, bsz=1000, num_updates=25000, lr=0.0005, gnorm=1.807, clip=100, train_wall=473, wall=12539 (progress_bar.py:260, log())
[2021-09-26 21:03:01]    INFO >> epoch 014:   1561 / 1881 loss=0.426, mrr=919.77, sample_size=1000, wps=2063.3, ups=2.06, wpb=1000, bsz=1000, num_updates=26000, lr=0.0005, gnorm=1.815, clip=100, train_wall=472, wall=13023 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 21:05:36]    INFO >> epoch 014 | loss 0.422 | mrr 0.920592 | sample_size 1000 | wps 1993.4 | ups 1.99 | wpb 1000 | bsz 1000 | num_updates 26320 | lr 0.0005 | gnorm 1.796 | clip 100 | train_wall 888 | wall 13179 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 21:05:56]    INFO >> epoch 014 | valid on 'valid' subset | loss 1.949 | mrr 0.750318 | sample_size 1000 | wps 7377.2 | wpb 1000 | bsz 1000 | num_updates 26320 | best_mrr 750.318 (progress_bar.py:269, print())
[2021-09-26 21:05:56]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_last.pt (epoch 14 @ 26320 updates, score 0.750318) (writing took 0.603435 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 21:11:37]    INFO >> epoch 015:    681 / 1881 loss=0.408, mrr=922.819, sample_size=1000, wps=1937.5, ups=1.94, wpb=1000, bsz=1000, num_updates=27000, lr=0.0005, gnorm=1.751, clip=100, train_wall=471, wall=13540 (progress_bar.py:260, log())
[2021-09-26 21:19:39]    INFO >> epoch 015:   1681 / 1881 loss=0.42, mrr=920.609, sample_size=1000, wps=2075.1, ups=2.08, wpb=1000, bsz=1000, num_updates=28000, lr=0.0005, gnorm=1.782, clip=100, train_wall=470, wall=14021 (progress_bar.py:260, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 21:21:16]    INFO >> epoch 015 | loss 0.41 | mrr 0.922271 | sample_size 1000 | wps 2000.3 | ups 2 | wpb 1000 | bsz 1000 | num_updates 28200 | lr 0.0005 | gnorm 1.76 | clip 100 | train_wall 885 | wall 14119 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-09-26 21:21:35]    INFO >> epoch 015 | valid on 'valid' subset | loss 1.926 | mrr 0.750844 | sample_size 1000 | wps 7485.9 | wpb 1000 | bsz 1000 | num_updates 28200 | best_mrr 750.844 (progress_bar.py:269, print())
[2021-09-26 21:21:36]    INFO >> saved checkpoint /home/wanyao/ncc_data/codesearchnet/retrieval/data-mmap/all/self_attn/checkpoints/checkpoint_last.pt (epoch 15 @ 28200 updates, score 0.750844) (writing took 0.581698 seconds) (checkpoint_utils.py:79, save_checkpoint())
[2021-09-26 21:21:36]    INFO >> early stop since valid performance hasn't improved for last 5 runs (train.py:190, should_stop_early())
[2021-09-26 21:21:36]    INFO >> early stop since valid performance hasn't improved for last 5 runs (train.py:271, single_main())
[2021-09-26 21:21:36]    INFO >> done training in 14134.9 seconds (train.py:283, single_main())
