nohup: 忽略输入
Using backend: pytorch
[2021-03-03 00:18:27]    INFO >> Load arguments in /home/yanghe/Documents/naturalcc-dev/run/retrieval/deepcs/config/java.yml (train.py:393, cli_main())
[2021-03-03 00:18:27]    INFO >> {'criterion': 'search_triplet_v2', 'optimizer': 'adamw_simple', 'lr_scheduler': 'cosine', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 10000, 'log_format': 'simple', 'tensorboard_logdir': '', 'seed': 0, 'cpu': 0, 'fp16': 0, 'memory_efficient_fp16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'deepcs_retrieval'}, 'dataset': {'num_workers': 3, 'skip_invalid_size_inputs_valid_test': 0, 'max_tokens': None, 'max_sentences': 64, 'src_max_tokens': [6, 30, 50], 'tgt_max_tokens': [30], 'required_batch_size_multiple': 1, 'dataset_impl': 'mmap', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': None, 'max_tokens_valid': None, 'max_sentences_valid': 10000, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 0}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 5000, 'local_rank': -1, 'block_momentum': 0.875, 'block_lr': 1, 'use_nbm': 0, 'average_sync': 0}, 'task': {'data': '/data/yanghe/.ncc/deepcs/retrieval/data-mmap', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'source_langs': ['name', 'apiseq', 'tokens'], 'target_langs': ['desc'], 'load_alignments': 0, 'left_pad_source': 0, 'left_pad_target': 0, 'upsample_primary': 1, 'truncate_source': 0, 'eval_mrr': 1}, 'model': {'arch': 'deepcs', 'embed_dim': 512, 'hidden_size': 256, 'rnn_layers': 1, 'bidirectional': 1, 'dropout': 0.25, 'candidate': 9999}, 'optimization': {'max_epoch': 15, 'max_update': 0, 'clip_norm': 5, 'sentence_avg': 0, 'update_freq': [1], 'lr': [0.000208], 'min_lr': -1, 'use_bmuf': 1, 'lr_shrink': 1.0, 'warmup_init_lr': 0, 'force_anneal': 0, 'warmup_updates': 5000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.98, 'use_old_adam': 0}, 'adamw': {'adam_epsilon': 1e-08}, 'margin': 0.3986}, 'checkpoint': {'save_dir': '/data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'acc', 'maximize_best_checkpoint_metric': 1, 'patience': -1}, 'eval': {'path': '/data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint_best.pt', 'quiet': 1, 'max_sentences': 1000, 'model_overrides': '{}', 'eval_size': 1000}} (train.py:395, cli_main())
[2021-03-03 00:18:27]    INFO >> single GPU training... (train.py:424, cli_main())
[2021-03-03 00:18:27]    INFO >> [name] dictionary: 10000 types (deepcs_retrieval.py:98, setup_task())
[2021-03-03 00:18:27]    INFO >> [apiseq] dictionary: 10000 types (deepcs_retrieval.py:98, setup_task())
[2021-03-03 00:18:27]    INFO >> [tokens] dictionary: 10000 types (deepcs_retrieval.py:98, setup_task())
[2021-03-03 00:18:27]    INFO >> [desc] dictionary: 10000 types (deepcs_retrieval.py:102, setup_task())
[2021-03-03 00:18:28]    INFO >> DeepCS(
  (src_encoders): ModuleDict(
    (name): ModuleList(
      (0): SeqEncoder(
        (embed): Embedding(10000, 512, padding_idx=0)
        (rnn): LSTM(512, 256, batch_first=True, bidirectional=True)
      )
      (1): Linear(in_features=512, out_features=512, bias=True)
    )
    (apiseq): ModuleList(
      (0): SeqEncoder(
        (embed): Embedding(10000, 512, padding_idx=0)
        (rnn): LSTM(512, 256, batch_first=True, bidirectional=True)
      )
      (1): Linear(in_features=512, out_features=512, bias=True)
    )
    (tokens): ModuleList(
      (0): NBOWEncoder(
        (embed): Embedding(10000, 512, padding_idx=0)
      )
      (1): Linear(in_features=512, out_features=512, bias=True)
    )
  )
  (tgt_encoders): ModuleDict(
    (desc): ModuleList(
      (0): SeqEncoder(
        (embed): Embedding(10000, 512, padding_idx=0)
        (rnn): LSTM(512, 256, batch_first=True, bidirectional=True)
      )
      (1): Linear(in_features=512, out_features=512, bias=True)
    )
  )
  (fusion): Linear(in_features=512, out_features=512, bias=True)
) (train.py:313, single_main())
[2021-03-03 00:18:28]    INFO >> model deepcs, criterion SearchTripletV2Criterion (train.py:314, single_main())
[2021-03-03 00:18:28]    INFO >> num. model params: 26524160 (num. trained: 26524160) (train.py:317, single_main())
[2021-03-03 00:18:29]    INFO >> training on 1 GPUs (train.py:323, single_main())
[2021-03-03 00:18:29]    INFO >> max tokens per GPU = None and max sentences per GPU = 64 (train.py:326, single_main())
[2021-03-03 00:18:29]    INFO >> no existing checkpoint found /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint_last.pt (ncc_trainer.py:267, load_checkpoint())
[2021-03-03 00:18:29]    INFO >> loading train data for epoch 1 (ncc_trainer.py:281, get_train_iterator())
[2021-03-03 00:18:33]    INFO >> NOTE: your device may support faster training with --fp16 (deepcs_trainer.py:38, _build_optimizer())
[2021-03-03 00:18:33] WARNING >> lr_period_updates has not been set and, therefore, set epoch_num * batch_num as deafault. (cosine_lr_scheduler.py:55, __init__())
/home/yanghe/Documents/naturalcc-dev/ncc/utils/utils.py:575: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
/home/yanghe/Documents/naturalcc-dev/ncc/tasks/ncc_task.py:401: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  "ntokens not found in Criterion logging outputs, cannot log wpb or wps"
[2021-03-03 00:22:36]    INFO >> epoch 001:  10000 / 284748 loss=0.21885, ups=43.42, bsz=64, num_updates=10000, lr=0.000207842, gnorm=0.337, clip=0, train_wall=208, wall=247 (progress_bar.py:262, log())
[2021-03-03 00:26:27]    INFO >> epoch 001:  20000 / 284748 loss=0.062629, ups=43.37, bsz=64, num_updates=20000, lr=0.000206579, gnorm=0.199, clip=0, train_wall=208, wall=478 (progress_bar.py:262, log())
[2021-03-03 00:30:17]    INFO >> epoch 001:  30000 / 284748 loss=0.043889, ups=43.41, bsz=64, num_updates=30000, lr=0.000204069, gnorm=0.157, clip=0, train_wall=208, wall=708 (progress_bar.py:262, log())
[2021-03-03 00:34:09]    INFO >> epoch 001:  40000 / 284748 loss=0.035927, ups=43.27, bsz=64, num_updates=40000, lr=0.000200342, gnorm=0.137, clip=0, train_wall=208, wall=939 (progress_bar.py:262, log())
[2021-03-03 00:37:59]    INFO >> epoch 001:  50000 / 284748 loss=0.032, ups=43.38, bsz=64, num_updates=50000, lr=0.000195444, gnorm=0.123, clip=0, train_wall=208, wall=1170 (progress_bar.py:262, log())
[2021-03-03 00:41:50]    INFO >> epoch 001:  60000 / 284748 loss=0.028937, ups=43.33, bsz=64, num_updates=60000, lr=0.000189433, gnorm=0.113, clip=0, train_wall=208, wall=1401 (progress_bar.py:262, log())
[2021-03-03 00:45:40]    INFO >> epoch 001:  70000 / 284748 loss=0.026662, ups=43.4, bsz=64, num_updates=70000, lr=0.000182384, gnorm=0.105, clip=0, train_wall=208, wall=1631 (progress_bar.py:262, log())
[2021-03-03 00:49:32]    INFO >> epoch 001:  80000 / 284748 loss=0.025324, ups=43.23, bsz=64, num_updates=80000, lr=0.000174381, gnorm=0.099, clip=0, train_wall=209, wall=1862 (progress_bar.py:262, log())
[2021-03-03 00:53:22]    INFO >> epoch 001:  90000 / 284748 loss=0.023524, ups=43.31, bsz=64, num_updates=90000, lr=0.000165523, gnorm=0.093, clip=0, train_wall=208, wall=2093 (progress_bar.py:262, log())
[2021-03-03 00:57:13]    INFO >> epoch 001:  100000 / 284748 loss=0.022982, ups=43.32, bsz=64, num_updates=100000, lr=0.000155917, gnorm=0.089, clip=0, train_wall=208, wall=2324 (progress_bar.py:262, log())
[2021-03-03 01:01:04]    INFO >> epoch 001:  110000 / 284748 loss=0.022182, ups=43.35, bsz=64, num_updates=110000, lr=0.000145679, gnorm=0.087, clip=0, train_wall=208, wall=2555 (progress_bar.py:262, log())
[2021-03-03 01:04:55]    INFO >> epoch 001:  120000 / 284748 loss=0.021221, ups=43.34, bsz=64, num_updates=120000, lr=0.000134934, gnorm=0.084, clip=0, train_wall=208, wall=2785 (progress_bar.py:262, log())
[2021-03-03 01:08:46]    INFO >> epoch 001:  130000 / 284748 loss=0.020308, ups=43.27, bsz=64, num_updates=130000, lr=0.000123813, gnorm=0.08, clip=0, train_wall=209, wall=3017 (progress_bar.py:262, log())
[2021-03-03 01:12:37]    INFO >> epoch 001:  140000 / 284748 loss=0.019759, ups=43.34, bsz=64, num_updates=140000, lr=0.000112452, gnorm=0.078, clip=0, train_wall=208, wall=3247 (progress_bar.py:262, log())
[2021-03-03 01:16:27]    INFO >> epoch 001:  150000 / 284748 loss=0.019077, ups=43.45, bsz=64, num_updates=150000, lr=0.000100987, gnorm=0.076, clip=0, train_wall=208, wall=3477 (progress_bar.py:262, log())
[2021-03-03 01:20:17]    INFO >> epoch 001:  160000 / 284748 loss=0.018587, ups=43.33, bsz=64, num_updates=160000, lr=8.95595e-05, gnorm=0.075, clip=0, train_wall=208, wall=3708 (progress_bar.py:262, log())
[2021-03-03 01:24:09]    INFO >> epoch 001:  170000 / 284748 loss=0.017898, ups=43.28, bsz=64, num_updates=170000, lr=7.83073e-05, gnorm=0.073, clip=0, train_wall=208, wall=3939 (progress_bar.py:262, log())
[2021-03-03 01:27:59]    INFO >> epoch 001:  180000 / 284748 loss=0.017135, ups=43.3, bsz=64, num_updates=180000, lr=6.73675e-05, gnorm=0.072, clip=0, train_wall=208, wall=4170 (progress_bar.py:262, log())
[2021-03-03 01:31:50]    INFO >> epoch 001:  190000 / 284748 loss=0.016747, ups=43.43, bsz=64, num_updates=190000, lr=5.68731e-05, gnorm=0.071, clip=0, train_wall=208, wall=4400 (progress_bar.py:262, log())
[2021-03-03 01:35:41]    INFO >> epoch 001:  200000 / 284748 loss=0.016285, ups=43.27, bsz=64, num_updates=200000, lr=4.69519e-05, gnorm=0.07, clip=0, train_wall=208, wall=4632 (progress_bar.py:262, log())
[2021-03-03 01:39:32]    INFO >> epoch 001:  210000 / 284748 loss=0.016009, ups=43.31, bsz=64, num_updates=210000, lr=3.77243e-05, gnorm=0.069, clip=0, train_wall=208, wall=4862 (progress_bar.py:262, log())
[2021-03-03 01:43:22]    INFO >> epoch 001:  220000 / 284748 loss=0.015538, ups=43.37, bsz=64, num_updates=220000, lr=2.93027e-05, gnorm=0.069, clip=0, train_wall=208, wall=5093 (progress_bar.py:262, log())
[2021-03-03 01:47:13]    INFO >> epoch 001:  230000 / 284748 loss=0.015203, ups=43.3, bsz=64, num_updates=230000, lr=2.17894e-05, gnorm=0.068, clip=0, train_wall=208, wall=5324 (progress_bar.py:262, log())
[2021-03-03 01:51:04]    INFO >> epoch 001:  240000 / 284748 loss=0.015092, ups=43.33, bsz=64, num_updates=240000, lr=1.52758e-05, gnorm=0.068, clip=0, train_wall=208, wall=5555 (progress_bar.py:262, log())
[2021-03-03 01:54:55]    INFO >> epoch 001:  250000 / 284748 loss=0.014803, ups=43.23, bsz=64, num_updates=250000, lr=9.84104e-06, gnorm=0.067, clip=0, train_wall=209, wall=5786 (progress_bar.py:262, log())
[2021-03-03 01:58:46]    INFO >> epoch 001:  260000 / 284748 loss=0.014966, ups=43.35, bsz=64, num_updates=260000, lr=5.55131e-06, gnorm=0.067, clip=0, train_wall=208, wall=6017 (progress_bar.py:262, log())
[2021-03-03 02:02:37]    INFO >> epoch 001:  270000 / 284748 loss=0.014725, ups=43.39, bsz=64, num_updates=270000, lr=2.45872e-06, gnorm=0.067, clip=0, train_wall=208, wall=6247 (progress_bar.py:262, log())
[2021-03-03 02:06:27]    INFO >> epoch 001:  280000 / 284748 loss=0.014617, ups=43.34, bsz=64, num_updates=280000, lr=6.00889e-07, gnorm=0.067, clip=0, train_wall=208, wall=6478 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 02:08:17]    INFO >> epoch 001 | loss 0.029423 | nll_loss inf | ppl inf | ups 43.34 | bsz 64 | num_updates 284748 | lr 1.58202e-07 | gnorm 0.098 | clip 0 | train_wall 5928 | wall 6588 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 02:08:36]    INFO >> epoch 001 | valid on 'valid' subset | acc 0.5479 | mrr 0.5479 | map 0.5479 | ndcg 0.5479 (progress_bar.py:269, print())
[2021-03-03 02:08:38]    INFO >> saved checkpoint /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint1.pt (epoch 1 @ 284748 updates, score None) (writing took 1.762505 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-03-03 02:10:52]    INFO >> epoch 002:   5252 / 284748 loss=0.014553, ups=37.73, bsz=64, num_updates=290000, lr=0.000208, gnorm=0.067, clip=0, train_wall=208, wall=6743 (progress_bar.py:262, log())
[2021-03-03 02:14:43]    INFO >> epoch 002:  15252 / 284748 loss=0.01752, ups=43.32, bsz=64, num_updates=300000, lr=0.000207335, gnorm=0.075, clip=0, train_wall=208, wall=6974 (progress_bar.py:262, log())
[2021-03-03 02:18:35]    INFO >> epoch 002:  25252 / 284748 loss=0.01759, ups=43.19, bsz=64, num_updates=310000, lr=0.000205415, gnorm=0.072, clip=0, train_wall=209, wall=7205 (progress_bar.py:262, log())
[2021-03-03 02:22:26]    INFO >> epoch 002:  35252 / 284748 loss=0.017113, ups=43.33, bsz=64, num_updates=320000, lr=0.000202261, gnorm=0.068, clip=0, train_wall=208, wall=7436 (progress_bar.py:262, log())
[2021-03-03 02:26:17]    INFO >> epoch 002:  45252 / 284748 loss=0.016874, ups=43.13, bsz=64, num_updates=330000, lr=0.000197912, gnorm=0.066, clip=0, train_wall=209, wall=7668 (progress_bar.py:262, log())
[2021-03-03 02:30:08]    INFO >> epoch 002:  55252 / 284748 loss=0.016037, ups=43.35, bsz=64, num_updates=340000, lr=0.000192421, gnorm=0.062, clip=0, train_wall=208, wall=7899 (progress_bar.py:262, log())
[2021-03-03 02:33:59]    INFO >> epoch 002:  65252 / 284748 loss=0.015352, ups=43.34, bsz=64, num_updates=350000, lr=0.000185855, gnorm=0.059, clip=0, train_wall=208, wall=8130 (progress_bar.py:262, log())
[2021-03-03 02:37:50]    INFO >> epoch 002:  75252 / 284748 loss=0.014828, ups=43.23, bsz=64, num_updates=360000, lr=0.000178294, gnorm=0.057, clip=0, train_wall=209, wall=8361 (progress_bar.py:262, log())
[2021-03-03 02:41:41]    INFO >> epoch 002:  85252 / 284748 loss=0.014327, ups=43.29, bsz=64, num_updates=370000, lr=0.000169829, gnorm=0.055, clip=0, train_wall=208, wall=8592 (progress_bar.py:262, log())
[2021-03-03 02:45:32]    INFO >> epoch 002:  95252 / 284748 loss=0.013953, ups=43.27, bsz=64, num_updates=380000, lr=0.000160564, gnorm=0.053, clip=0, train_wall=208, wall=8823 (progress_bar.py:262, log())
[2021-03-03 02:49:23]    INFO >> epoch 002:  105252 / 284748 loss=0.013516, ups=43.35, bsz=64, num_updates=390000, lr=0.000150611, gnorm=0.051, clip=0, train_wall=208, wall=9054 (progress_bar.py:262, log())
[2021-03-03 02:53:14]    INFO >> epoch 002:  115252 / 284748 loss=0.013229, ups=43.27, bsz=64, num_updates=400000, lr=0.000140091, gnorm=0.05, clip=0, train_wall=208, wall=9285 (progress_bar.py:262, log())
[2021-03-03 02:57:05]    INFO >> epoch 002:  125252 / 284748 loss=0.012833, ups=43.25, bsz=64, num_updates=410000, lr=0.000129132, gnorm=0.048, clip=0, train_wall=209, wall=9516 (progress_bar.py:262, log())
[2021-03-03 03:00:56]    INFO >> epoch 002:  135252 / 284748 loss=0.012432, ups=43.3, bsz=64, num_updates=420000, lr=0.000117868, gnorm=0.047, clip=0, train_wall=208, wall=9747 (progress_bar.py:262, log())
[2021-03-03 03:04:47]    INFO >> epoch 002:  145252 / 284748 loss=0.012335, ups=43.3, bsz=64, num_updates=430000, lr=0.000106435, gnorm=0.047, clip=0, train_wall=208, wall=9978 (progress_bar.py:262, log())
[2021-03-03 03:08:38]    INFO >> epoch 002:  155252 / 284748 loss=0.01209, ups=43.29, bsz=64, num_updates=440000, lr=9.4972e-05, gnorm=0.046, clip=0, train_wall=208, wall=10209 (progress_bar.py:262, log())
[2021-03-03 03:12:29]    INFO >> epoch 002:  165252 / 284748 loss=0.011706, ups=43.25, bsz=64, num_updates=450000, lr=8.36192e-05, gnorm=0.045, clip=0, train_wall=209, wall=10440 (progress_bar.py:262, log())
[2021-03-03 03:16:21]    INFO >> epoch 002:  175252 / 284748 loss=0.011501, ups=43.24, bsz=64, num_updates=460000, lr=7.25142e-05, gnorm=0.044, clip=0, train_wall=209, wall=10671 (progress_bar.py:262, log())
[2021-03-03 03:20:12]    INFO >> epoch 002:  185252 / 284748 loss=0.011269, ups=43.27, bsz=64, num_updates=470000, lr=6.17921e-05, gnorm=0.044, clip=0, train_wall=208, wall=10902 (progress_bar.py:262, log())
[2021-03-03 03:24:02]    INFO >> epoch 002:  195252 / 284748 loss=0.010974, ups=43.39, bsz=64, num_updates=480000, lr=5.15832e-05, gnorm=0.043, clip=0, train_wall=208, wall=11133 (progress_bar.py:262, log())
[2021-03-03 03:27:54]    INFO >> epoch 002:  205252 / 284748 loss=0.010866, ups=43.24, bsz=64, num_updates=490000, lr=4.20117e-05, gnorm=0.043, clip=0, train_wall=209, wall=11364 (progress_bar.py:262, log())
[2021-03-03 03:31:44]    INFO >> epoch 002:  215252 / 284748 loss=0.010786, ups=43.33, bsz=64, num_updates=500000, lr=3.31941e-05, gnorm=0.043, clip=0, train_wall=208, wall=11595 (progress_bar.py:262, log())
[2021-03-03 03:35:36]    INFO >> epoch 002:  225252 / 284748 loss=0.010524, ups=43.15, bsz=64, num_updates=510000, lr=2.52374e-05, gnorm=0.042, clip=0, train_wall=209, wall=11827 (progress_bar.py:262, log())
[2021-03-03 03:39:27]    INFO >> epoch 002:  235252 / 284748 loss=0.010401, ups=43.25, bsz=64, num_updates=520000, lr=1.82385e-05, gnorm=0.042, clip=0, train_wall=209, wall=12058 (progress_bar.py:262, log())
[2021-03-03 03:43:18]    INFO >> epoch 002:  245252 / 284748 loss=0.010511, ups=43.38, bsz=64, num_updates=530000, lr=1.22824e-05, gnorm=0.042, clip=0, train_wall=208, wall=12288 (progress_bar.py:262, log())
[2021-03-03 03:47:09]    INFO >> epoch 002:  255252 / 284748 loss=0.010304, ups=43.3, bsz=64, num_updates=540000, lr=7.44167e-06, gnorm=0.042, clip=0, train_wall=208, wall=12519 (progress_bar.py:262, log())
[2021-03-03 03:50:59]    INFO >> epoch 002:  265252 / 284748 loss=0.01031, ups=43.42, bsz=64, num_updates=550000, lr=3.77509e-06, gnorm=0.042, clip=0, train_wall=208, wall=12750 (progress_bar.py:262, log())
[2021-03-03 03:54:50]    INFO >> epoch 002:  275252 / 284748 loss=0.010214, ups=43.3, bsz=64, num_updates=560000, lr=1.32725e-06, gnorm=0.042, clip=0, train_wall=208, wall=12981 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 03:58:29]    INFO >> epoch 002 | loss 0.012874 | nll_loss inf | ppl inf | ups 43.06 | bsz 64 | num_updates 569496 | lr 1.58202e-07 | gnorm 0.051 | clip 0 | train_wall 5934 | wall 13200 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 03:58:48]    INFO >> epoch 002 | valid on 'valid' subset | acc 0.6661 | mrr 0.6661 | map 0.6661 | ndcg 0.6661 (progress_bar.py:269, print())
[2021-03-03 03:58:50]    INFO >> saved checkpoint /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint2.pt (epoch 2 @ 569496 updates, score None) (writing took 1.820798 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-03-03 03:59:15]    INFO >> epoch 003:    504 / 284748 loss=0.010071, ups=37.77, bsz=64, num_updates=570000, lr=1.27922e-07, gnorm=0.041, clip=0, train_wall=208, wall=13245 (progress_bar.py:262, log())
[2021-03-03 04:03:06]    INFO >> epoch 003:  10504 / 284748 loss=0.011105, ups=43.23, bsz=64, num_updates=580000, lr=0.000207808, gnorm=0.044, clip=0, train_wall=209, wall=13477 (progress_bar.py:262, log())
[2021-03-03 04:06:57]    INFO >> epoch 003:  20504 / 284748 loss=0.012222, ups=43.36, bsz=64, num_updates=590000, lr=0.000206482, gnorm=0.046, clip=0, train_wall=208, wall=13707 (progress_bar.py:262, log())
[2021-03-03 04:10:48]    INFO >> epoch 003:  30504 / 284748 loss=0.012167, ups=43.16, bsz=64, num_updates=600000, lr=0.00020391, gnorm=0.045, clip=0, train_wall=209, wall=13939 (progress_bar.py:262, log())
[2021-03-03 04:14:39]    INFO >> epoch 003:  40504 / 284748 loss=0.012214, ups=43.31, bsz=64, num_updates=610000, lr=0.000200123, gnorm=0.043, clip=0, train_wall=208, wall=14170 (progress_bar.py:262, log())
[2021-03-03 04:18:30]    INFO >> epoch 003:  50504 / 284748 loss=0.012439, ups=43.26, bsz=64, num_updates=620000, lr=0.000195167, gnorm=0.043, clip=0, train_wall=209, wall=14401 (progress_bar.py:262, log())
[2021-03-03 04:22:21]    INFO >> epoch 003:  60504 / 284748 loss=0.011945, ups=43.28, bsz=64, num_updates=630000, lr=0.000189102, gnorm=0.041, clip=0, train_wall=208, wall=14632 (progress_bar.py:262, log())
[2021-03-03 04:26:13]    INFO >> epoch 003:  70504 / 284748 loss=0.011831, ups=43.17, bsz=64, num_updates=640000, lr=0.000182002, gnorm=0.04, clip=0, train_wall=209, wall=14864 (progress_bar.py:262, log())
[2021-03-03 04:30:04]    INFO >> epoch 003:  80504 / 284748 loss=0.011808, ups=43.24, bsz=64, num_updates=650000, lr=0.000173954, gnorm=0.039, clip=0, train_wall=209, wall=15095 (progress_bar.py:262, log())
[2021-03-03 04:33:56]    INFO >> epoch 003:  90504 / 284748 loss=0.011648, ups=43.25, bsz=64, num_updates=660000, lr=0.000165056, gnorm=0.038, clip=0, train_wall=209, wall=15326 (progress_bar.py:262, log())
[2021-03-03 04:37:47]    INFO >> epoch 003:  100504 / 284748 loss=0.011092, ups=43.29, bsz=64, num_updates=670000, lr=0.000155415, gnorm=0.037, clip=0, train_wall=208, wall=15557 (progress_bar.py:262, log())
[2021-03-03 04:41:37]    INFO >> epoch 003:  110504 / 284748 loss=0.011035, ups=43.42, bsz=64, num_updates=680000, lr=0.000145148, gnorm=0.036, clip=0, train_wall=208, wall=15788 (progress_bar.py:262, log())
[2021-03-03 04:45:28]    INFO >> epoch 003:  120504 / 284748 loss=0.011033, ups=43.35, bsz=64, num_updates=690000, lr=0.000134382, gnorm=0.036, clip=0, train_wall=208, wall=16018 (progress_bar.py:262, log())
[2021-03-03 04:49:19]    INFO >> epoch 003:  130504 / 284748 loss=0.010616, ups=43.3, bsz=64, num_updates=700000, lr=0.000123245, gnorm=0.035, clip=0, train_wall=208, wall=16249 (progress_bar.py:262, log())
[2021-03-03 04:53:09]    INFO >> epoch 003:  140504 / 284748 loss=0.010309, ups=43.31, bsz=64, num_updates=710000, lr=0.000111875, gnorm=0.034, clip=0, train_wall=208, wall=16480 (progress_bar.py:262, log())
[2021-03-03 04:57:00]    INFO >> epoch 003:  150504 / 284748 loss=0.010273, ups=43.37, bsz=64, num_updates=720000, lr=0.000100409, gnorm=0.034, clip=0, train_wall=208, wall=16711 (progress_bar.py:262, log())
[2021-03-03 05:00:51]    INFO >> epoch 003:  160504 / 284748 loss=0.010106, ups=43.32, bsz=64, num_updates=730000, lr=8.8987e-05, gnorm=0.034, clip=0, train_wall=208, wall=16942 (progress_bar.py:262, log())
[2021-03-03 05:04:42]    INFO >> epoch 003:  170504 / 284748 loss=0.009966, ups=43.35, bsz=64, num_updates=740000, lr=7.77473e-05, gnorm=0.033, clip=0, train_wall=208, wall=17172 (progress_bar.py:262, log())
[2021-03-03 05:08:32]    INFO >> epoch 003:  180504 / 284748 loss=0.009723, ups=43.33, bsz=64, num_updates=750000, lr=6.68268e-05, gnorm=0.033, clip=0, train_wall=208, wall=17403 (progress_bar.py:262, log())
[2021-03-03 05:12:23]    INFO >> epoch 003:  190504 / 284748 loss=0.009572, ups=43.3, bsz=64, num_updates=760000, lr=5.63583e-05, gnorm=0.032, clip=0, train_wall=208, wall=17634 (progress_bar.py:262, log())
[2021-03-03 05:16:14]    INFO >> epoch 003:  200504 / 284748 loss=0.009247, ups=43.41, bsz=64, num_updates=770000, lr=4.64692e-05, gnorm=0.032, clip=0, train_wall=208, wall=17864 (progress_bar.py:262, log())
[2021-03-03 05:20:05]    INFO >> epoch 003:  210504 / 284748 loss=0.009426, ups=43.31, bsz=64, num_updates=780000, lr=3.72797e-05, gnorm=0.032, clip=0, train_wall=208, wall=18095 (progress_bar.py:262, log())
[2021-03-03 05:23:57]    INFO >> epoch 003:  220504 / 284748 loss=0.009419, ups=43.06, bsz=64, num_updates=790000, lr=2.89015e-05, gnorm=0.032, clip=0, train_wall=210, wall=18328 (progress_bar.py:262, log())
[2021-03-03 05:27:49]    INFO >> epoch 003:  230504 / 284748 loss=0.009141, ups=43.11, bsz=64, num_updates=800000, lr=2.14365e-05, gnorm=0.032, clip=0, train_wall=209, wall=18560 (progress_bar.py:262, log())
[2021-03-03 05:31:39]    INFO >> epoch 003:  240504 / 284748 loss=0.009213, ups=43.4, bsz=64, num_updates=810000, lr=1.49754e-05, gnorm=0.032, clip=0, train_wall=208, wall=18790 (progress_bar.py:262, log())
[2021-03-03 05:35:30]    INFO >> epoch 003:  250504 / 284748 loss=0.008989, ups=43.29, bsz=64, num_updates=820000, lr=9.59694e-06, gnorm=0.031, clip=0, train_wall=208, wall=19021 (progress_bar.py:262, log())
[2021-03-03 05:39:22]    INFO >> epoch 003:  260504 / 284748 loss=0.009154, ups=43.2, bsz=64, num_updates=830000, lr=5.36642e-06, gnorm=0.032, clip=0, train_wall=209, wall=19252 (progress_bar.py:262, log())
[2021-03-03 05:43:12]    INFO >> epoch 003:  270504 / 284748 loss=0.008865, ups=43.32, bsz=64, num_updates=840000, lr=2.33529e-06, gnorm=0.031, clip=0, train_wall=208, wall=19483 (progress_bar.py:262, log())
[2021-03-03 05:47:03]    INFO >> epoch 003:  280504 / 284748 loss=0.008964, ups=43.4, bsz=64, num_updates=850000, lr=5.40412e-07, gnorm=0.031, clip=0, train_wall=208, wall=19714 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 05:48:41]    INFO >> epoch 003 | loss 0.01046 | nll_loss inf | ppl inf | ups 43.07 | bsz 64 | num_updates 854244 | lr 1.58202e-07 | gnorm 0.036 | clip 0 | train_wall 5933 | wall 19812 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 05:49:00]    INFO >> epoch 003 | valid on 'valid' subset | acc 0.7083 | mrr 0.7083 | map 0.7083 | ndcg 0.7083 (progress_bar.py:269, print())
[2021-03-03 05:49:01]    INFO >> saved checkpoint /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint3.pt (epoch 3 @ 854244 updates, score None) (writing took 1.579864 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-03-03 05:51:28]    INFO >> epoch 004:   5756 / 284748 loss=0.008924, ups=37.77, bsz=64, num_updates=860000, lr=0.000207996, gnorm=0.031, clip=0, train_wall=208, wall=19978 (progress_bar.py:262, log())
[2021-03-03 05:55:19]    INFO >> epoch 004:  15756 / 284748 loss=0.010272, ups=43.3, bsz=64, num_updates=870000, lr=0.000207269, gnorm=0.034, clip=0, train_wall=208, wall=20209 (progress_bar.py:262, log())
[2021-03-03 05:59:10]    INFO >> epoch 004:  25756 / 284748 loss=0.010515, ups=43.26, bsz=64, num_updates=880000, lr=0.000205285, gnorm=0.034, clip=0, train_wall=208, wall=20441 (progress_bar.py:262, log())
[2021-03-03 06:03:01]    INFO >> epoch 004:  35756 / 284748 loss=0.010576, ups=43.32, bsz=64, num_updates=890000, lr=0.00020207, gnorm=0.034, clip=0, train_wall=208, wall=20671 (progress_bar.py:262, log())
[2021-03-03 06:06:52]    INFO >> epoch 004:  45756 / 284748 loss=0.010873, ups=43.21, bsz=64, num_updates=900000, lr=0.000197662, gnorm=0.033, clip=0, train_wall=209, wall=20903 (progress_bar.py:262, log())
[2021-03-03 06:10:42]    INFO >> epoch 004:  55756 / 284748 loss=0.010583, ups=43.44, bsz=64, num_updates=910000, lr=0.000192115, gnorm=0.032, clip=0, train_wall=208, wall=21133 (progress_bar.py:262, log())
[2021-03-03 06:14:33]    INFO >> epoch 004:  65756 / 284748 loss=0.01036, ups=43.34, bsz=64, num_updates=920000, lr=0.000185497, gnorm=0.031, clip=0, train_wall=208, wall=21364 (progress_bar.py:262, log())
[2021-03-03 06:18:24]    INFO >> epoch 004:  75756 / 284748 loss=0.010307, ups=43.36, bsz=64, num_updates=930000, lr=0.000177888, gnorm=0.031, clip=0, train_wall=208, wall=21594 (progress_bar.py:262, log())
[2021-03-03 06:22:14]    INFO >> epoch 004:  85756 / 284748 loss=0.010223, ups=43.47, bsz=64, num_updates=940000, lr=0.00016938, gnorm=0.03, clip=0, train_wall=207, wall=21824 (progress_bar.py:262, log())
[2021-03-03 06:26:04]    INFO >> epoch 004:  95756 / 284748 loss=0.010115, ups=43.39, bsz=64, num_updates=950000, lr=0.000160078, gnorm=0.03, clip=0, train_wall=208, wall=22055 (progress_bar.py:262, log())
[2021-03-03 06:29:55]    INFO >> epoch 004:  105756 / 284748 loss=0.009988, ups=43.41, bsz=64, num_updates=960000, lr=0.000150093, gnorm=0.029, clip=0, train_wall=208, wall=22285 (progress_bar.py:262, log())
[2021-03-03 06:33:45]    INFO >> epoch 004:  115756 / 284748 loss=0.009767, ups=43.42, bsz=64, num_updates=970000, lr=0.000139548, gnorm=0.029, clip=0, train_wall=208, wall=22516 (progress_bar.py:262, log())
[2021-03-03 06:37:35]    INFO >> epoch 004:  125756 / 284748 loss=0.009579, ups=43.35, bsz=64, num_updates=980000, lr=0.00012857, gnorm=0.029, clip=0, train_wall=208, wall=22746 (progress_bar.py:262, log())
[2021-03-03 06:41:25]    INFO >> epoch 004:  135756 / 284748 loss=0.009332, ups=43.48, bsz=64, num_updates=990000, lr=0.000117294, gnorm=0.028, clip=0, train_wall=207, wall=22976 (progress_bar.py:262, log())
[2021-03-03 06:45:17]    INFO >> epoch 004:  145756 / 284748 loss=0.00931, ups=43.28, bsz=64, num_updates=1e+06, lr=0.000105856, gnorm=0.028, clip=0, train_wall=208, wall=23207 (progress_bar.py:262, log())
[2021-03-03 06:49:07]    INFO >> epoch 004:  155756 / 284748 loss=0.009107, ups=43.42, bsz=64, num_updates=1.01e+06, lr=9.4396e-05, gnorm=0.028, clip=0, train_wall=208, wall=23438 (progress_bar.py:262, log())
[2021-03-03 06:52:58]    INFO >> epoch 004:  165756 / 284748 loss=0.008965, ups=43.34, bsz=64, num_updates=1.02e+06, lr=8.30524e-05, gnorm=0.027, clip=0, train_wall=208, wall=23668 (progress_bar.py:262, log())
[2021-03-03 06:56:48]    INFO >> epoch 004:  175756 / 284748 loss=0.008863, ups=43.32, bsz=64, num_updates=1.03e+06, lr=7.19635e-05, gnorm=0.027, clip=0, train_wall=208, wall=23899 (progress_bar.py:262, log())
[2021-03-03 07:00:39]    INFO >> epoch 004:  185756 / 284748 loss=0.008818, ups=43.33, bsz=64, num_updates=1.04e+06, lr=6.12642e-05, gnorm=0.027, clip=0, train_wall=208, wall=24130 (progress_bar.py:262, log())
[2021-03-03 07:04:30]    INFO >> epoch 004:  195756 / 284748 loss=0.008615, ups=43.37, bsz=64, num_updates=1.05e+06, lr=5.10845e-05, gnorm=0.027, clip=0, train_wall=208, wall=24361 (progress_bar.py:262, log())
[2021-03-03 07:08:20]    INFO >> epoch 004:  205756 / 284748 loss=0.008555, ups=43.43, bsz=64, num_updates=1.06e+06, lr=4.15484e-05, gnorm=0.027, clip=0, train_wall=208, wall=24591 (progress_bar.py:262, log())
[2021-03-03 07:12:11]    INFO >> epoch 004:  215756 / 284748 loss=0.008416, ups=43.35, bsz=64, num_updates=1.07e+06, lr=3.27716e-05, gnorm=0.026, clip=0, train_wall=208, wall=24821 (progress_bar.py:262, log())
[2021-03-03 07:16:02]    INFO >> epoch 004:  225756 / 284748 loss=0.008397, ups=43.21, bsz=64, num_updates=1.08e+06, lr=2.48609e-05, gnorm=0.027, clip=0, train_wall=209, wall=25053 (progress_bar.py:262, log())
[2021-03-03 07:19:54]    INFO >> epoch 004:  235756 / 284748 loss=0.008328, ups=43.11, bsz=64, num_updates=1.09e+06, lr=1.79127e-05, gnorm=0.026, clip=0, train_wall=209, wall=25285 (progress_bar.py:262, log())
[2021-03-03 07:23:45]    INFO >> epoch 004:  245756 / 284748 loss=0.008208, ups=43.33, bsz=64, num_updates=1.1e+06, lr=1.20112e-05, gnorm=0.026, clip=0, train_wall=208, wall=25516 (progress_bar.py:262, log())
[2021-03-03 07:27:36]    INFO >> epoch 004:  255756 / 284748 loss=0.008195, ups=43.34, bsz=64, num_updates=1.11e+06, lr=7.22835e-06, gnorm=0.026, clip=0, train_wall=208, wall=25746 (progress_bar.py:262, log())
[2021-03-03 07:31:26]    INFO >> epoch 004:  265756 / 284748 loss=0.008099, ups=43.38, bsz=64, num_updates=1.12e+06, lr=3.62224e-06, gnorm=0.026, clip=0, train_wall=208, wall=25977 (progress_bar.py:262, log())
[2021-03-03 07:35:17]    INFO >> epoch 004:  275756 / 284748 loss=0.00804, ups=43.25, bsz=64, num_updates=1.13e+06, lr=1.23674e-06, gnorm=0.026, clip=0, train_wall=209, wall=26208 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 07:38:45]    INFO >> epoch 004 | loss 0.009302 | nll_loss inf | ppl inf | ups 43.12 | bsz 64 | num_updates 1.13899e+06 | lr 1.58202e-07 | gnorm 0.029 | clip 0 | train_wall 5926 | wall 26415 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 07:39:03]    INFO >> epoch 004 | valid on 'valid' subset | acc 0.7358 | mrr 0.7358 | map 0.7358 | ndcg 0.7358 (progress_bar.py:269, print())
[2021-03-03 07:39:05]    INFO >> saved checkpoint /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint4.pt (epoch 4 @ 1138992 updates, score None) (writing took 1.825365 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-03-03 07:39:41]    INFO >> epoch 005:   1008 / 284748 loss=0.008214, ups=37.86, bsz=64, num_updates=1.14e+06, lr=1.00854e-07, gnorm=0.026, clip=0, train_wall=208, wall=26472 (progress_bar.py:262, log())
[2021-03-03 07:43:33]    INFO >> epoch 005:  11008 / 284748 loss=0.008789, ups=43.27, bsz=64, num_updates=1.15e+06, lr=0.000207772, gnorm=0.028, clip=0, train_wall=208, wall=26703 (progress_bar.py:262, log())
[2021-03-03 07:47:24]    INFO >> epoch 005:  21008 / 284748 loss=0.009701, ups=43.27, bsz=64, num_updates=1.16e+06, lr=0.000206382, gnorm=0.029, clip=0, train_wall=209, wall=26934 (progress_bar.py:262, log())
[2021-03-03 07:51:15]    INFO >> epoch 005:  31008 / 284748 loss=0.009608, ups=43.31, bsz=64, num_updates=1.17e+06, lr=0.000203748, gnorm=0.028, clip=0, train_wall=208, wall=27165 (progress_bar.py:262, log())
[2021-03-03 07:55:05]    INFO >> epoch 005:  41008 / 284748 loss=0.009722, ups=43.35, bsz=64, num_updates=1.18e+06, lr=0.0001999, gnorm=0.028, clip=0, train_wall=208, wall=27396 (progress_bar.py:262, log())
[2021-03-03 07:58:56]    INFO >> epoch 005:  51008 / 284748 loss=0.009748, ups=43.31, bsz=64, num_updates=1.19e+06, lr=0.000194887, gnorm=0.028, clip=0, train_wall=208, wall=27627 (progress_bar.py:262, log())
[2021-03-03 08:02:48]    INFO >> epoch 005:  61008 / 284748 loss=0.009619, ups=43.18, bsz=64, num_updates=1.2e+06, lr=0.000188768, gnorm=0.027, clip=0, train_wall=209, wall=27858 (progress_bar.py:262, log())
[2021-03-03 08:06:39]    INFO >> epoch 005:  71008 / 284748 loss=0.009439, ups=43.29, bsz=64, num_updates=1.21e+06, lr=0.000181619, gnorm=0.027, clip=0, train_wall=208, wall=28089 (progress_bar.py:262, log())
[2021-03-03 08:10:30]    INFO >> epoch 005:  81008 / 284748 loss=0.009237, ups=43.26, bsz=64, num_updates=1.22e+06, lr=0.000173525, gnorm=0.026, clip=0, train_wall=208, wall=28321 (progress_bar.py:262, log())
[2021-03-03 08:14:21]    INFO >> epoch 005:  91008 / 284748 loss=0.009083, ups=43.19, bsz=64, num_updates=1.23e+06, lr=0.000164587, gnorm=0.026, clip=0, train_wall=209, wall=28552 (progress_bar.py:262, log())
[2021-03-03 08:18:12]    INFO >> epoch 005:  101008 / 284748 loss=0.008841, ups=43.37, bsz=64, num_updates=1.24e+06, lr=0.000154911, gnorm=0.025, clip=0, train_wall=208, wall=28783 (progress_bar.py:262, log())
[2021-03-03 08:22:03]    INFO >> epoch 005:  111008 / 284748 loss=0.008694, ups=43.33, bsz=64, num_updates=1.25e+06, lr=0.000144617, gnorm=0.025, clip=0, train_wall=208, wall=29014 (progress_bar.py:262, log())
[2021-03-03 08:25:55]    INFO >> epoch 005:  121008 / 284748 loss=0.008516, ups=43.04, bsz=64, num_updates=1.26e+06, lr=0.000133828, gnorm=0.025, clip=0, train_wall=210, wall=29246 (progress_bar.py:262, log())
[2021-03-03 08:29:46]    INFO >> epoch 005:  131008 / 284748 loss=0.008387, ups=43.27, bsz=64, num_updates=1.27e+06, lr=0.000122677, gnorm=0.025, clip=0, train_wall=208, wall=29477 (progress_bar.py:262, log())
[2021-03-03 08:33:37]    INFO >> epoch 005:  141008 / 284748 loss=0.008054, ups=43.26, bsz=64, num_updates=1.28e+06, lr=0.000111298, gnorm=0.024, clip=0, train_wall=209, wall=29708 (progress_bar.py:262, log())
[2021-03-03 08:37:28]    INFO >> epoch 005:  151008 / 284748 loss=0.007977, ups=43.29, bsz=64, num_updates=1.29e+06, lr=9.98314e-05, gnorm=0.024, clip=0, train_wall=208, wall=29939 (progress_bar.py:262, log())
[2021-03-03 08:41:20]    INFO >> epoch 005:  161008 / 284748 loss=0.007731, ups=43.23, bsz=64, num_updates=1.3e+06, lr=8.8415e-05, gnorm=0.024, clip=0, train_wall=209, wall=30171 (progress_bar.py:262, log())
[2021-03-03 08:45:10]    INFO >> epoch 005:  171008 / 284748 loss=0.007646, ups=43.35, bsz=64, num_updates=1.31e+06, lr=7.71881e-05, gnorm=0.024, clip=0, train_wall=208, wall=30401 (progress_bar.py:262, log())
[2021-03-03 08:49:01]    INFO >> epoch 005:  181008 / 284748 loss=0.007442, ups=43.35, bsz=64, num_updates=1.32e+06, lr=6.62873e-05, gnorm=0.024, clip=0, train_wall=208, wall=30632 (progress_bar.py:262, log())
[2021-03-03 08:52:53]    INFO >> epoch 005:  191008 / 284748 loss=0.0075, ups=43.11, bsz=64, num_updates=1.33e+06, lr=5.5845e-05, gnorm=0.024, clip=0, train_wall=209, wall=30864 (progress_bar.py:262, log())
[2021-03-03 08:56:44]    INFO >> epoch 005:  201008 / 284748 loss=0.007116, ups=43.33, bsz=64, num_updates=1.34e+06, lr=4.59884e-05, gnorm=0.023, clip=0, train_wall=208, wall=31095 (progress_bar.py:262, log())
[2021-03-03 09:00:35]    INFO >> epoch 005:  211008 / 284748 loss=0.00725, ups=43.25, bsz=64, num_updates=1.35e+06, lr=3.68371e-05, gnorm=0.023, clip=0, train_wall=209, wall=31326 (progress_bar.py:262, log())
[2021-03-03 09:04:26]    INFO >> epoch 005:  221008 / 284748 loss=0.007158, ups=43.26, bsz=64, num_updates=1.36e+06, lr=2.85026e-05, gnorm=0.023, clip=0, train_wall=209, wall=31557 (progress_bar.py:262, log())
[2021-03-03 09:08:17]    INFO >> epoch 005:  231008 / 284748 loss=0.007018, ups=43.37, bsz=64, num_updates=1.37e+06, lr=2.10861e-05, gnorm=0.023, clip=0, train_wall=208, wall=31788 (progress_bar.py:262, log())
[2021-03-03 09:12:08]    INFO >> epoch 005:  241008 / 284748 loss=0.006884, ups=43.26, bsz=64, num_updates=1.38e+06, lr=1.46778e-05, gnorm=0.023, clip=0, train_wall=209, wall=32019 (progress_bar.py:262, log())
[2021-03-03 09:15:59]    INFO >> epoch 005:  251008 / 284748 loss=0.006856, ups=43.21, bsz=64, num_updates=1.39e+06, lr=9.35577e-06, gnorm=0.023, clip=0, train_wall=209, wall=32250 (progress_bar.py:262, log())
[2021-03-03 09:19:50]    INFO >> epoch 005:  261008 / 284748 loss=0.006733, ups=43.43, bsz=64, num_updates=1.4e+06, lr=5.18458e-06, gnorm=0.023, clip=0, train_wall=208, wall=32481 (progress_bar.py:262, log())
[2021-03-03 09:23:41]    INFO >> epoch 005:  271008 / 284748 loss=0.006821, ups=43.26, bsz=64, num_updates=1.41e+06, lr=2.215e-06, gnorm=0.023, clip=0, train_wall=209, wall=32712 (progress_bar.py:262, log())
[2021-03-03 09:27:32]    INFO >> epoch 005:  281008 / 284748 loss=0.006749, ups=43.25, bsz=64, num_updates=1.42e+06, lr=4.83134e-07, gnorm=0.023, clip=0, train_wall=209, wall=32943 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 09:28:59]    INFO >> epoch 005 | loss 0.008141 | nll_loss inf | ppl inf | ups 43.05 | bsz 64 | num_updates 1.42374e+06 | lr 1.58202e-07 | gnorm 0.025 | clip 0 | train_wall 5937 | wall 33030 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 09:29:18]    INFO >> epoch 005 | valid on 'valid' subset | acc 0.7592 | mrr 0.7592 | map 0.7592 | ndcg 0.7592 (progress_bar.py:269, print())
[2021-03-03 09:29:19]    INFO >> saved checkpoint /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint5.pt (epoch 5 @ 1423740 updates, score None) (writing took 1.557238 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-03-03 09:31:57]    INFO >> epoch 006:   6260 / 284748 loss=0.007038, ups=37.69, bsz=64, num_updates=1.43e+06, lr=0.00020799, gnorm=0.023, clip=0, train_wall=209, wall=33208 (progress_bar.py:262, log())
[2021-03-03 09:35:48]    INFO >> epoch 006:  16260 / 284748 loss=0.007974, ups=43.38, bsz=64, num_updates=1.44e+06, lr=0.000207199, gnorm=0.026, clip=0, train_wall=208, wall=33439 (progress_bar.py:262, log())
[2021-03-03 09:39:39]    INFO >> epoch 006:  26260 / 284748 loss=0.008255, ups=43.29, bsz=64, num_updates=1.45e+06, lr=0.000205152, gnorm=0.025, clip=0, train_wall=208, wall=33670 (progress_bar.py:262, log())
[2021-03-03 09:43:30]    INFO >> epoch 006:  36260 / 284748 loss=0.00802, ups=43.34, bsz=64, num_updates=1.46e+06, lr=0.000201876, gnorm=0.025, clip=0, train_wall=208, wall=33900 (progress_bar.py:262, log())
[2021-03-03 09:47:20]    INFO >> epoch 006:  46260 / 284748 loss=0.008229, ups=43.37, bsz=64, num_updates=1.47e+06, lr=0.000197409, gnorm=0.025, clip=0, train_wall=208, wall=34131 (progress_bar.py:262, log())
[2021-03-03 09:51:11]    INFO >> epoch 006:  56260 / 284748 loss=0.007857, ups=43.32, bsz=64, num_updates=1.48e+06, lr=0.000191807, gnorm=0.024, clip=0, train_wall=208, wall=34362 (progress_bar.py:262, log())
[2021-03-03 09:55:02]    INFO >> epoch 006:  66260 / 284748 loss=0.007869, ups=43.37, bsz=64, num_updates=1.49e+06, lr=0.000185137, gnorm=0.024, clip=0, train_wall=208, wall=34592 (progress_bar.py:262, log())
[2021-03-03 09:58:53]    INFO >> epoch 006:  76260 / 284748 loss=0.0079, ups=43.32, bsz=64, num_updates=1.5e+06, lr=0.00017748, gnorm=0.024, clip=0, train_wall=208, wall=34823 (progress_bar.py:262, log())
[2021-03-03 10:02:43]    INFO >> epoch 006:  86260 / 284748 loss=0.007638, ups=43.35, bsz=64, num_updates=1.51e+06, lr=0.00016893, gnorm=0.023, clip=0, train_wall=208, wall=35054 (progress_bar.py:262, log())
[2021-03-03 10:06:34]    INFO >> epoch 006:  96260 / 284748 loss=0.007346, ups=43.39, bsz=64, num_updates=1.52e+06, lr=0.00015959, gnorm=0.023, clip=0, train_wall=208, wall=35284 (progress_bar.py:262, log())
[2021-03-03 10:10:25]    INFO >> epoch 006:  106260 / 284748 loss=0.007403, ups=43.25, bsz=64, num_updates=1.53e+06, lr=0.000149574, gnorm=0.023, clip=0, train_wall=209, wall=35516 (progress_bar.py:262, log())
[2021-03-03 10:14:16]    INFO >> epoch 006:  116260 / 284748 loss=0.007292, ups=43.2, bsz=64, num_updates=1.54e+06, lr=0.000139004, gnorm=0.023, clip=0, train_wall=209, wall=35747 (progress_bar.py:262, log())
[2021-03-03 10:18:07]    INFO >> epoch 006:  126260 / 284748 loss=0.006949, ups=43.33, bsz=64, num_updates=1.55e+06, lr=0.000128008, gnorm=0.022, clip=0, train_wall=208, wall=35978 (progress_bar.py:262, log())
[2021-03-03 10:21:58]    INFO >> epoch 006:  136260 / 284748 loss=0.007005, ups=43.32, bsz=64, num_updates=1.56e+06, lr=0.00011672, gnorm=0.022, clip=0, train_wall=208, wall=36209 (progress_bar.py:262, log())
[2021-03-03 10:25:48]    INFO >> epoch 006:  146260 / 284748 loss=0.006947, ups=43.43, bsz=64, num_updates=1.57e+06, lr=0.000105278, gnorm=0.022, clip=0, train_wall=208, wall=36439 (progress_bar.py:262, log())
[2021-03-03 10:29:39]    INFO >> epoch 006:  156260 / 284748 loss=0.006732, ups=43.31, bsz=64, num_updates=1.58e+06, lr=9.38203e-05, gnorm=0.022, clip=0, train_wall=208, wall=36670 (progress_bar.py:262, log())
[2021-03-03 10:33:30]    INFO >> epoch 006:  166260 / 284748 loss=0.006611, ups=43.28, bsz=64, num_updates=1.59e+06, lr=8.24863e-05, gnorm=0.022, clip=0, train_wall=208, wall=36901 (progress_bar.py:262, log())
[2021-03-03 10:37:21]    INFO >> epoch 006:  176260 / 284748 loss=0.006674, ups=43.3, bsz=64, num_updates=1.6e+06, lr=7.14138e-05, gnorm=0.022, clip=0, train_wall=208, wall=37132 (progress_bar.py:262, log())
[2021-03-03 10:41:12]    INFO >> epoch 006:  186260 / 284748 loss=0.006366, ups=43.27, bsz=64, num_updates=1.61e+06, lr=6.07376e-05, gnorm=0.022, clip=0, train_wall=208, wall=37363 (progress_bar.py:262, log())
[2021-03-03 10:45:03]    INFO >> epoch 006:  196260 / 284748 loss=0.006229, ups=43.31, bsz=64, num_updates=1.62e+06, lr=5.05875e-05, gnorm=0.021, clip=0, train_wall=208, wall=37594 (progress_bar.py:262, log())
[2021-03-03 10:48:54]    INFO >> epoch 006:  206260 / 284748 loss=0.006357, ups=43.31, bsz=64, num_updates=1.63e+06, lr=4.10869e-05, gnorm=0.021, clip=0, train_wall=208, wall=37825 (progress_bar.py:262, log())
[2021-03-03 10:52:45]    INFO >> epoch 006:  216260 / 284748 loss=0.0061, ups=43.36, bsz=64, num_updates=1.64e+06, lr=3.23513e-05, gnorm=0.021, clip=0, train_wall=208, wall=38055 (progress_bar.py:262, log())
[2021-03-03 10:56:35]    INFO >> epoch 006:  226260 / 284748 loss=0.006058, ups=43.33, bsz=64, num_updates=1.65e+06, lr=2.4487e-05, gnorm=0.021, clip=0, train_wall=208, wall=38286 (progress_bar.py:262, log())
[2021-03-03 11:00:26]    INFO >> epoch 006:  236260 / 284748 loss=0.005912, ups=43.36, bsz=64, num_updates=1.66e+06, lr=1.75895e-05, gnorm=0.021, clip=0, train_wall=208, wall=38517 (progress_bar.py:262, log())
[2021-03-03 11:04:17]    INFO >> epoch 006:  246260 / 284748 loss=0.005943, ups=43.3, bsz=64, num_updates=1.67e+06, lr=1.17428e-05, gnorm=0.021, clip=0, train_wall=208, wall=38748 (progress_bar.py:262, log())
[2021-03-03 11:08:08]    INFO >> epoch 006:  256260 / 284748 loss=0.006054, ups=43.24, bsz=64, num_updates=1.68e+06, lr=7.01801e-06, gnorm=0.021, clip=0, train_wall=209, wall=38979 (progress_bar.py:262, log())
[2021-03-03 11:11:59]    INFO >> epoch 006:  266260 / 284748 loss=0.005953, ups=43.4, bsz=64, num_updates=1.69e+06, lr=3.4725e-06, gnorm=0.021, clip=0, train_wall=208, wall=39209 (progress_bar.py:262, log())
[2021-03-03 11:15:49]    INFO >> epoch 006:  276260 / 284748 loss=0.005937, ups=43.45, bsz=64, num_updates=1.7e+06, lr=1.14941e-06, gnorm=0.021, clip=0, train_wall=208, wall=39440 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 11:19:05]    INFO >> epoch 006 | loss 0.006921 | nll_loss inf | ppl inf | ups 43.11 | bsz 64 | num_updates 1.70849e+06 | lr 1.58202e-07 | gnorm 0.022 | clip 0 | train_wall 5928 | wall 39635 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 11:19:23]    INFO >> epoch 006 | valid on 'valid' subset | acc 0.7932 | mrr 0.7932 | map 0.7932 | ndcg 0.7932 (progress_bar.py:269, print())
[2021-03-03 11:19:25]    INFO >> saved checkpoint /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint6.pt (epoch 6 @ 1708488 updates, score None) (writing took 1.642797 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-03-03 11:20:13]    INFO >> epoch 007:   1512 / 284748 loss=0.005973, ups=37.88, bsz=64, num_updates=1.71e+06, lr=7.69983e-08, gnorm=0.021, clip=0, train_wall=208, wall=39704 (progress_bar.py:262, log())
[2021-03-03 11:24:04]    INFO >> epoch 007:  11512 / 284748 loss=0.006383, ups=43.3, bsz=64, num_updates=1.72e+06, lr=0.000207732, gnorm=0.022, clip=0, train_wall=208, wall=39935 (progress_bar.py:262, log())
[2021-03-03 11:27:55]    INFO >> epoch 007:  21512 / 284748 loss=0.007161, ups=43.27, bsz=64, num_updates=1.73e+06, lr=0.000206279, gnorm=0.023, clip=0, train_wall=208, wall=40166 (progress_bar.py:262, log())
[2021-03-03 11:31:46]    INFO >> epoch 007:  31512 / 284748 loss=0.007284, ups=43.33, bsz=64, num_updates=1.74e+06, lr=0.000203583, gnorm=0.023, clip=0, train_wall=208, wall=40396 (progress_bar.py:262, log())
[2021-03-03 11:35:37]    INFO >> epoch 007:  41512 / 284748 loss=0.007412, ups=43.32, bsz=64, num_updates=1.75e+06, lr=0.000199675, gnorm=0.023, clip=0, train_wall=208, wall=40627 (progress_bar.py:262, log())
[2021-03-03 11:39:28]    INFO >> epoch 007:  51512 / 284748 loss=0.007274, ups=43.23, bsz=64, num_updates=1.76e+06, lr=0.000194604, gnorm=0.023, clip=0, train_wall=209, wall=40859 (progress_bar.py:262, log())
[2021-03-03 11:43:19]    INFO >> epoch 007:  61512 / 284748 loss=0.007368, ups=43.34, bsz=64, num_updates=1.77e+06, lr=0.000188432, gnorm=0.023, clip=0, train_wall=208, wall=41089 (progress_bar.py:262, log())
[2021-03-03 11:47:09]    INFO >> epoch 007:  71512 / 284748 loss=0.007153, ups=43.35, bsz=64, num_updates=1.78e+06, lr=0.000181233, gnorm=0.022, clip=0, train_wall=208, wall=41320 (progress_bar.py:262, log())
[2021-03-03 11:51:00]    INFO >> epoch 007:  81512 / 284748 loss=0.007164, ups=43.36, bsz=64, num_updates=1.79e+06, lr=0.000173094, gnorm=0.022, clip=0, train_wall=208, wall=41551 (progress_bar.py:262, log())
[2021-03-03 11:54:51]    INFO >> epoch 007:  91512 / 284748 loss=0.007016, ups=43.28, bsz=64, num_updates=1.8e+06, lr=0.000164116, gnorm=0.022, clip=0, train_wall=208, wall=41782 (progress_bar.py:262, log())
[2021-03-03 11:58:42]    INFO >> epoch 007:  101512 / 284748 loss=0.006867, ups=43.3, bsz=64, num_updates=1.81e+06, lr=0.000154406, gnorm=0.022, clip=0, train_wall=208, wall=42013 (progress_bar.py:262, log())
[2021-03-03 12:02:33]    INFO >> epoch 007:  111512 / 284748 loss=0.006776, ups=43.22, bsz=64, num_updates=1.82e+06, lr=0.000144084, gnorm=0.022, clip=0, train_wall=209, wall=42244 (progress_bar.py:262, log())
[2021-03-03 12:06:25]    INFO >> epoch 007:  121512 / 284748 loss=0.006736, ups=43.18, bsz=64, num_updates=1.83e+06, lr=0.000133274, gnorm=0.022, clip=0, train_wall=209, wall=42476 (progress_bar.py:262, log())
[2021-03-03 12:10:16]    INFO >> epoch 007:  131512 / 284748 loss=0.006666, ups=43.28, bsz=64, num_updates=1.84e+06, lr=0.000122108, gnorm=0.021, clip=0, train_wall=208, wall=42707 (progress_bar.py:262, log())
[2021-03-03 12:14:13]    INFO >> epoch 007:  141512 / 284748 loss=0.006474, ups=42.1, bsz=64, num_updates=1.85e+06, lr=0.000110721, gnorm=0.021, clip=0, train_wall=214, wall=42944 (progress_bar.py:262, log())
[2021-03-03 12:18:08]    INFO >> epoch 007:  151512 / 284748 loss=0.006419, ups=42.73, bsz=64, num_updates=1.86e+06, lr=9.92536e-05, gnorm=0.021, clip=0, train_wall=211, wall=43178 (progress_bar.py:262, log())
[2021-03-03 12:21:59]    INFO >> epoch 007:  161512 / 284748 loss=0.006218, ups=43.11, bsz=64, num_updates=1.87e+06, lr=8.78435e-05, gnorm=0.021, clip=0, train_wall=209, wall=43410 (progress_bar.py:262, log())
[2021-03-03 12:25:51]    INFO >> epoch 007:  171512 / 284748 loss=0.006034, ups=43.15, bsz=64, num_updates=1.88e+06, lr=7.66298e-05, gnorm=0.021, clip=0, train_wall=209, wall=43642 (progress_bar.py:262, log())
[2021-03-03 12:29:44]    INFO >> epoch 007:  181512 / 284748 loss=0.006071, ups=43.04, bsz=64, num_updates=1.89e+06, lr=6.57489e-05, gnorm=0.021, clip=0, train_wall=209, wall=43874 (progress_bar.py:262, log())
[2021-03-03 12:33:35]    INFO >> epoch 007:  191512 / 284748 loss=0.005821, ups=43.24, bsz=64, num_updates=1.9e+06, lr=5.53332e-05, gnorm=0.02, clip=0, train_wall=208, wall=44106 (progress_bar.py:262, log())
[2021-03-03 12:37:27]    INFO >> epoch 007:  201512 / 284748 loss=0.005755, ups=43.15, bsz=64, num_updates=1.91e+06, lr=4.55093e-05, gnorm=0.02, clip=0, train_wall=209, wall=44337 (progress_bar.py:262, log())
[2021-03-03 12:41:20]    INFO >> epoch 007:  211512 / 284748 loss=0.005778, ups=42.85, bsz=64, num_updates=1.92e+06, lr=3.63966e-05, gnorm=0.02, clip=0, train_wall=210, wall=44571 (progress_bar.py:262, log())
[2021-03-03 12:45:11]    INFO >> epoch 007:  221512 / 284748 loss=0.005711, ups=43.26, bsz=64, num_updates=1.93e+06, lr=2.8106e-05, gnorm=0.02, clip=0, train_wall=208, wall=44802 (progress_bar.py:262, log())
[2021-03-03 12:49:02]    INFO >> epoch 007:  231512 / 284748 loss=0.005599, ups=43.28, bsz=64, num_updates=1.94e+06, lr=2.07383e-05, gnorm=0.02, clip=0, train_wall=208, wall=45033 (progress_bar.py:262, log())
[2021-03-03 12:52:54]    INFO >> epoch 007:  241512 / 284748 loss=0.005459, ups=43.18, bsz=64, num_updates=1.95e+06, lr=1.4383e-05, gnorm=0.02, clip=0, train_wall=209, wall=45265 (progress_bar.py:262, log())
[2021-03-03 12:56:45]    INFO >> epoch 007:  251512 / 284748 loss=0.005428, ups=43.25, bsz=64, num_updates=1.96e+06, lr=9.11751e-06, gnorm=0.02, clip=0, train_wall=208, wall=45496 (progress_bar.py:262, log())
[2021-03-03 13:00:37]    INFO >> epoch 007:  261512 / 284748 loss=0.005512, ups=43.07, bsz=64, num_updates=1.97e+06, lr=5.00579e-06, gnorm=0.02, clip=0, train_wall=209, wall=45728 (progress_bar.py:262, log())
[2021-03-03 13:04:29]    INFO >> epoch 007:  271512 / 284748 loss=0.005464, ups=43.12, bsz=64, num_updates=1.98e+06, lr=2.09785e-06, gnorm=0.02, clip=0, train_wall=209, wall=45960 (progress_bar.py:262, log())
[2021-03-03 13:08:23]    INFO >> epoch 007:  281512 / 284748 loss=0.005457, ups=42.73, bsz=64, num_updates=1.99e+06, lr=4.29058e-07, gnorm=0.02, clip=0, train_wall=211, wall=46194 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 13:09:39]    INFO >> epoch 007 | loss 0.00636 | nll_loss inf | ppl inf | ups 42.92 | bsz 64 | num_updates 1.99324e+06 | lr 1.58202e-07 | gnorm 0.021 | clip 0 | train_wall 5952 | wall 46269 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 13:09:58]    INFO >> epoch 007 | valid on 'valid' subset | acc 0.8077 | mrr 0.8077 | map 0.8077 | ndcg 0.8077 (progress_bar.py:269, print())
[2021-03-03 13:10:00]    INFO >> saved checkpoint /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint7.pt (epoch 7 @ 1993236 updates, score None) (writing took 1.861365 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-03-03 13:12:49]    INFO >> epoch 008:   6764 / 284748 loss=0.005477, ups=37.65, bsz=64, num_updates=2e+06, lr=0.00020798, gnorm=0.02, clip=0, train_wall=209, wall=46460 (progress_bar.py:262, log())
[2021-03-03 13:16:40]    INFO >> epoch 008:  16764 / 284748 loss=0.006528, ups=43.2, bsz=64, num_updates=2.01e+06, lr=0.000207125, gnorm=0.023, clip=0, train_wall=209, wall=46691 (progress_bar.py:262, log())
[2021-03-03 13:20:32]    INFO >> epoch 008:  26764 / 284748 loss=0.006687, ups=43.18, bsz=64, num_updates=2.02e+06, lr=0.000205016, gnorm=0.023, clip=0, train_wall=209, wall=46923 (progress_bar.py:262, log())
[2021-03-03 13:24:23]    INFO >> epoch 008:  36764 / 284748 loss=0.006769, ups=43.27, bsz=64, num_updates=2.03e+06, lr=0.000201679, gnorm=0.023, clip=0, train_wall=208, wall=47154 (progress_bar.py:262, log())
[2021-03-03 13:28:15]    INFO >> epoch 008:  46764 / 284748 loss=0.006748, ups=43.17, bsz=64, num_updates=2.04e+06, lr=0.000197154, gnorm=0.022, clip=0, train_wall=209, wall=47385 (progress_bar.py:262, log())
[2021-03-03 13:32:06]    INFO >> epoch 008:  56764 / 284748 loss=0.006691, ups=43.29, bsz=64, num_updates=2.05e+06, lr=0.000191496, gnorm=0.022, clip=0, train_wall=208, wall=47616 (progress_bar.py:262, log())
[2021-03-03 13:35:57]    INFO >> epoch 008:  66764 / 284748 loss=0.006672, ups=43.28, bsz=64, num_updates=2.06e+06, lr=0.000184774, gnorm=0.022, clip=0, train_wall=208, wall=47847 (progress_bar.py:262, log())
[2021-03-03 13:39:48]    INFO >> epoch 008:  76764 / 284748 loss=0.006565, ups=43.16, bsz=64, num_updates=2.07e+06, lr=0.000177069, gnorm=0.022, clip=0, train_wall=209, wall=48079 (progress_bar.py:262, log())
[2021-03-03 13:43:40]    INFO >> epoch 008:  86764 / 284748 loss=0.006396, ups=43.17, bsz=64, num_updates=2.08e+06, lr=0.000168477, gnorm=0.021, clip=0, train_wall=209, wall=48311 (progress_bar.py:262, log())
[2021-03-03 13:47:31]    INFO >> epoch 008:  96764 / 284748 loss=0.006225, ups=43.26, bsz=64, num_updates=2.09e+06, lr=0.0001591, gnorm=0.021, clip=0, train_wall=209, wall=48542 (progress_bar.py:262, log())
[2021-03-03 13:51:22]    INFO >> epoch 008:  106764 / 284748 loss=0.00616, ups=43.29, bsz=64, num_updates=2.1e+06, lr=0.000149053, gnorm=0.021, clip=0, train_wall=208, wall=48773 (progress_bar.py:262, log())
[2021-03-03 13:55:14]    INFO >> epoch 008:  116764 / 284748 loss=0.005993, ups=43.22, bsz=64, num_updates=2.11e+06, lr=0.000138459, gnorm=0.021, clip=0, train_wall=209, wall=49004 (progress_bar.py:262, log())
[2021-03-03 13:59:05]    INFO >> epoch 008:  126764 / 284748 loss=0.005924, ups=43.22, bsz=64, num_updates=2.12e+06, lr=0.000127445, gnorm=0.021, clip=0, train_wall=209, wall=49236 (progress_bar.py:262, log())
[2021-03-03 14:02:56]    INFO >> epoch 008:  136764 / 284748 loss=0.005698, ups=43.28, bsz=64, num_updates=2.13e+06, lr=0.000116146, gnorm=0.02, clip=0, train_wall=208, wall=49467 (progress_bar.py:262, log())
[2021-03-03 14:06:47]    INFO >> epoch 008:  146764 / 284748 loss=0.005591, ups=43.21, bsz=64, num_updates=2.14e+06, lr=0.0001047, gnorm=0.02, clip=0, train_wall=209, wall=49698 (progress_bar.py:262, log())
[2021-03-03 14:10:39]    INFO >> epoch 008:  156764 / 284748 loss=0.005599, ups=43.25, bsz=64, num_updates=2.15e+06, lr=9.3245e-05, gnorm=0.02, clip=0, train_wall=208, wall=49929 (progress_bar.py:262, log())
[2021-03-03 14:14:30]    INFO >> epoch 008:  166764 / 284748 loss=0.005287, ups=43.29, bsz=64, num_updates=2.16e+06, lr=8.19208e-05, gnorm=0.02, clip=0, train_wall=208, wall=50160 (progress_bar.py:262, log())
[2021-03-03 14:18:21]    INFO >> epoch 008:  176764 / 284748 loss=0.005197, ups=43.27, bsz=64, num_updates=2.17e+06, lr=7.08652e-05, gnorm=0.02, clip=0, train_wall=208, wall=50392 (progress_bar.py:262, log())
[2021-03-03 14:22:12]    INFO >> epoch 008:  186764 / 284748 loss=0.005081, ups=43.35, bsz=64, num_updates=2.18e+06, lr=6.02124e-05, gnorm=0.02, clip=0, train_wall=208, wall=50622 (progress_bar.py:262, log())
[2021-03-03 14:26:03]    INFO >> epoch 008:  196764 / 284748 loss=0.005037, ups=43.26, bsz=64, num_updates=2.19e+06, lr=5.00921e-05, gnorm=0.02, clip=0, train_wall=209, wall=50853 (progress_bar.py:262, log())
[2021-03-03 14:29:54]    INFO >> epoch 008:  206764 / 284748 loss=0.005011, ups=43.28, bsz=64, num_updates=2.2e+06, lr=4.06274e-05, gnorm=0.019, clip=0, train_wall=208, wall=51084 (progress_bar.py:262, log())
[2021-03-03 14:33:45]    INFO >> epoch 008:  216764 / 284748 loss=0.004834, ups=43.27, bsz=64, num_updates=2.21e+06, lr=3.19332e-05, gnorm=0.019, clip=0, train_wall=208, wall=51316 (progress_bar.py:262, log())
[2021-03-03 14:37:36]    INFO >> epoch 008:  226764 / 284748 loss=0.00488, ups=43.25, bsz=64, num_updates=2.22e+06, lr=2.41154e-05, gnorm=0.019, clip=0, train_wall=208, wall=51547 (progress_bar.py:262, log())
[2021-03-03 14:41:27]    INFO >> epoch 008:  236764 / 284748 loss=0.004779, ups=43.29, bsz=64, num_updates=2.23e+06, lr=1.72691e-05, gnorm=0.019, clip=0, train_wall=208, wall=51778 (progress_bar.py:262, log())
[2021-03-03 14:45:18]    INFO >> epoch 008:  246764 / 284748 loss=0.00488, ups=43.33, bsz=64, num_updates=2.24e+06, lr=1.14773e-05, gnorm=0.019, clip=0, train_wall=208, wall=52009 (progress_bar.py:262, log())
[2021-03-03 14:49:09]    INFO >> epoch 008:  256764 / 284748 loss=0.004729, ups=43.26, bsz=64, num_updates=2.25e+06, lr=6.81068e-06, gnorm=0.019, clip=0, train_wall=208, wall=52240 (progress_bar.py:262, log())
[2021-03-03 14:53:01]    INFO >> epoch 008:  266764 / 284748 loss=0.004734, ups=43.16, bsz=64, num_updates=2.26e+06, lr=3.32586e-06, gnorm=0.019, clip=0, train_wall=209, wall=52471 (progress_bar.py:262, log())
[2021-03-03 14:56:52]    INFO >> epoch 008:  276764 / 284748 loss=0.004702, ups=43.19, bsz=64, num_updates=2.27e+06, lr=1.06526e-06, gnorm=0.019, clip=0, train_wall=209, wall=52703 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 14:59:57]    INFO >> epoch 008 | loss 0.00565 | nll_loss inf | ppl inf | ups 43.02 | bsz 64 | num_updates 2.27798e+06 | lr 1.58202e-07 | gnorm 0.021 | clip 0 | train_wall 5937 | wall 52888 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 15:00:16]    INFO >> epoch 008 | valid on 'valid' subset | acc 0.8269 | mrr 0.8269 | map 0.8269 | ndcg 0.8269 (progress_bar.py:269, print())
[2021-03-03 15:00:18]    INFO >> saved checkpoint /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint8.pt (epoch 8 @ 2277984 updates, score None) (writing took 1.740818 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-03-03 15:01:18]    INFO >> epoch 009:   2016 / 284748 loss=0.004669, ups=37.65, bsz=64, num_updates=2.28e+06, lr=5.6356e-08, gnorm=0.019, clip=0, train_wall=208, wall=52969 (progress_bar.py:262, log())
[2021-03-03 15:05:09]    INFO >> epoch 009:  12016 / 284748 loss=0.005211, ups=43.21, bsz=64, num_updates=2.29e+06, lr=0.000207689, gnorm=0.021, clip=0, train_wall=209, wall=53200 (progress_bar.py:262, log())
[2021-03-03 15:09:01]    INFO >> epoch 009:  22016 / 284748 loss=0.005892, ups=43.09, bsz=64, num_updates=2.3e+06, lr=0.000206173, gnorm=0.022, clip=0, train_wall=209, wall=53432 (progress_bar.py:262, log())
[2021-03-03 15:13:07]    INFO >> epoch 009:  32016 / 284748 loss=0.005995, ups=40.7, bsz=64, num_updates=2.31e+06, lr=0.000203414, gnorm=0.022, clip=0, train_wall=221, wall=53678 (progress_bar.py:262, log())
[2021-03-03 15:17:00]    INFO >> epoch 009:  42016 / 284748 loss=0.006017, ups=43.02, bsz=64, num_updates=2.32e+06, lr=0.000199447, gnorm=0.022, clip=0, train_wall=210, wall=53910 (progress_bar.py:262, log())
[2021-03-03 15:20:52]    INFO >> epoch 009:  52016 / 284748 loss=0.005936, ups=43.05, bsz=64, num_updates=2.33e+06, lr=0.000194319, gnorm=0.022, clip=0, train_wall=209, wall=54143 (progress_bar.py:262, log())
[2021-03-03 15:24:44]    INFO >> epoch 009:  62016 / 284748 loss=0.005952, ups=43.13, bsz=64, num_updates=2.34e+06, lr=0.000188093, gnorm=0.022, clip=0, train_wall=209, wall=54374 (progress_bar.py:262, log())
[2021-03-03 15:28:35]    INFO >> epoch 009:  72016 / 284748 loss=0.005792, ups=43.27, bsz=64, num_updates=2.35e+06, lr=0.000180844, gnorm=0.021, clip=0, train_wall=208, wall=54606 (progress_bar.py:262, log())
[2021-03-03 15:32:26]    INFO >> epoch 009:  82016 / 284748 loss=0.005764, ups=43.21, bsz=64, num_updates=2.36e+06, lr=0.000172661, gnorm=0.021, clip=0, train_wall=209, wall=54837 (progress_bar.py:262, log())
[2021-03-03 15:36:18]    INFO >> epoch 009:  92016 / 284748 loss=0.005465, ups=43.18, bsz=64, num_updates=2.37e+06, lr=0.000163643, gnorm=0.021, clip=0, train_wall=209, wall=55069 (progress_bar.py:262, log())
[2021-03-03 15:40:09]    INFO >> epoch 009:  102016 / 284748 loss=0.005495, ups=43.16, bsz=64, num_updates=2.38e+06, lr=0.000153899, gnorm=0.021, clip=0, train_wall=209, wall=55300 (progress_bar.py:262, log())
[2021-03-03 15:44:00]    INFO >> epoch 009:  112016 / 284748 loss=0.005477, ups=43.31, bsz=64, num_updates=2.39e+06, lr=0.000143549, gnorm=0.021, clip=0, train_wall=208, wall=55531 (progress_bar.py:262, log())
[2021-03-03 15:47:51]    INFO >> epoch 009:  122016 / 284748 loss=0.005384, ups=43.32, bsz=64, num_updates=2.4e+06, lr=0.000132718, gnorm=0.02, clip=0, train_wall=208, wall=55762 (progress_bar.py:262, log())
[2021-03-03 15:51:42]    INFO >> epoch 009:  132016 / 284748 loss=0.005247, ups=43.3, bsz=64, num_updates=2.41e+06, lr=0.000121538, gnorm=0.02, clip=0, train_wall=208, wall=55993 (progress_bar.py:262, log())
[2021-03-03 15:55:33]    INFO >> epoch 009:  142016 / 284748 loss=0.005031, ups=43.23, bsz=64, num_updates=2.42e+06, lr=0.000110144, gnorm=0.02, clip=0, train_wall=208, wall=56224 (progress_bar.py:262, log())
[2021-03-03 15:59:25]    INFO >> epoch 009:  152016 / 284748 loss=0.005092, ups=43.13, bsz=64, num_updates=2.43e+06, lr=9.8676e-05, gnorm=0.02, clip=0, train_wall=209, wall=56456 (progress_bar.py:262, log())
[2021-03-03 16:03:17]    INFO >> epoch 009:  162016 / 284748 loss=0.004848, ups=43.16, bsz=64, num_updates=2.44e+06, lr=8.72724e-05, gnorm=0.019, clip=0, train_wall=209, wall=56688 (progress_bar.py:262, log())
[2021-03-03 16:07:08]    INFO >> epoch 009:  172016 / 284748 loss=0.004911, ups=43.24, bsz=64, num_updates=2.45e+06, lr=7.60723e-05, gnorm=0.02, clip=0, train_wall=209, wall=56919 (progress_bar.py:262, log())
[2021-03-03 16:11:00]    INFO >> epoch 009:  182016 / 284748 loss=0.004681, ups=43.14, bsz=64, num_updates=2.46e+06, lr=6.52118e-05, gnorm=0.019, clip=0, train_wall=209, wall=57151 (progress_bar.py:262, log())
[2021-03-03 16:14:52]    INFO >> epoch 009:  192016 / 284748 loss=0.004533, ups=43.06, bsz=64, num_updates=2.47e+06, lr=5.48229e-05, gnorm=0.019, clip=0, train_wall=209, wall=57383 (progress_bar.py:262, log())
[2021-03-03 16:18:44]    INFO >> epoch 009:  202016 / 284748 loss=0.004421, ups=43.23, bsz=64, num_updates=2.48e+06, lr=4.5032e-05, gnorm=0.019, clip=0, train_wall=209, wall=57614 (progress_bar.py:262, log())
[2021-03-03 16:22:35]    INFO >> epoch 009:  212016 / 284748 loss=0.004323, ups=43.28, bsz=64, num_updates=2.49e+06, lr=3.59582e-05, gnorm=0.019, clip=0, train_wall=208, wall=57845 (progress_bar.py:262, log())
[2021-03-03 16:26:26]    INFO >> epoch 009:  222016 / 284748 loss=0.004479, ups=43.22, bsz=64, num_updates=2.5e+06, lr=2.77118e-05, gnorm=0.019, clip=0, train_wall=209, wall=58077 (progress_bar.py:262, log())
[2021-03-03 16:30:17]    INFO >> epoch 009:  232016 / 284748 loss=0.004236, ups=43.26, bsz=64, num_updates=2.51e+06, lr=2.0393e-05, gnorm=0.019, clip=0, train_wall=208, wall=58308 (progress_bar.py:262, log())
[2021-03-03 16:34:09]    INFO >> epoch 009:  242016 / 284748 loss=0.004235, ups=43.17, bsz=64, num_updates=2.52e+06, lr=1.4091e-05, gnorm=0.018, clip=0, train_wall=209, wall=58540 (progress_bar.py:262, log())
[2021-03-03 16:38:01]    INFO >> epoch 009:  252016 / 284748 loss=0.004197, ups=43.15, bsz=64, num_updates=2.53e+06, lr=8.8822e-06, gnorm=0.018, clip=0, train_wall=209, wall=58771 (progress_bar.py:262, log())
[2021-03-03 16:41:52]    INFO >> epoch 009:  262016 / 284748 loss=0.004227, ups=43.24, bsz=64, num_updates=2.54e+06, lr=4.83007e-06, gnorm=0.018, clip=0, train_wall=208, wall=59003 (progress_bar.py:262, log())
[2021-03-03 16:45:43]    INFO >> epoch 009:  272016 / 284748 loss=0.004026, ups=43.18, bsz=64, num_updates=2.55e+06, lr=1.98386e-06, gnorm=0.018, clip=0, train_wall=209, wall=59234 (progress_bar.py:262, log())
[2021-03-03 16:49:35]    INFO >> epoch 009:  282016 / 284748 loss=0.004181, ups=43.24, bsz=64, num_updates=2.56e+06, lr=3.78183e-07, gnorm=0.018, clip=0, train_wall=208, wall=59465 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 16:50:38]    INFO >> epoch 009 | loss 0.005026 | nll_loss inf | ppl inf | ups 42.88 | bsz 64 | num_updates 2.56273e+06 | lr 1.58202e-07 | gnorm 0.02 | clip 0 | train_wall 5954 | wall 59529 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 16:50:58]    INFO >> epoch 009 | valid on 'valid' subset | acc 0.8406 | mrr 0.8406 | map 0.8406 | ndcg 0.8406 (progress_bar.py:269, print())
[2021-03-03 16:51:00]    INFO >> saved checkpoint /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint9.pt (epoch 9 @ 2562732 updates, score None) (writing took 1.932651 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-03-03 16:54:02]    INFO >> epoch 010:   7268 / 284748 loss=0.004349, ups=37.48, bsz=64, num_updates=2.57e+06, lr=0.000207967, gnorm=0.019, clip=0, train_wall=208, wall=59732 (progress_bar.py:262, log())
[2021-03-03 16:57:59]    INFO >> epoch 010:  17268 / 284748 loss=0.005115, ups=42.19, bsz=64, num_updates=2.58e+06, lr=0.000207049, gnorm=0.022, clip=0, train_wall=213, wall=59969 (progress_bar.py:262, log())
[2021-03-03 17:01:53]    INFO >> epoch 010:  27268 / 284748 loss=0.005309, ups=42.63, bsz=64, num_updates=2.59e+06, lr=0.000204877, gnorm=0.022, clip=0, train_wall=211, wall=60204 (progress_bar.py:262, log())
[2021-03-03 17:05:48]    INFO >> epoch 010:  37268 / 284748 loss=0.005336, ups=42.63, bsz=64, num_updates=2.6e+06, lr=0.000201479, gnorm=0.021, clip=0, train_wall=212, wall=60438 (progress_bar.py:262, log())
[2021-03-03 17:09:44]    INFO >> epoch 010:  47268 / 284748 loss=0.005469, ups=42.33, bsz=64, num_updates=2.61e+06, lr=0.000196895, gnorm=0.022, clip=0, train_wall=213, wall=60675 (progress_bar.py:262, log())
[2021-03-03 17:13:36]    INFO >> epoch 010:  57268 / 284748 loss=0.005349, ups=43.17, bsz=64, num_updates=2.62e+06, lr=0.000191182, gnorm=0.021, clip=0, train_wall=209, wall=60906 (progress_bar.py:262, log())
[2021-03-03 17:17:25]    INFO >> epoch 010:  67268 / 284748 loss=0.005236, ups=43.57, bsz=64, num_updates=2.63e+06, lr=0.000184408, gnorm=0.021, clip=0, train_wall=207, wall=61136 (progress_bar.py:262, log())
[2021-03-03 17:21:28]    INFO >> epoch 010:  77268 / 284748 loss=0.005148, ups=41.12, bsz=64, num_updates=2.64e+06, lr=0.000176657, gnorm=0.021, clip=0, train_wall=219, wall=61379 (progress_bar.py:262, log())
[2021-03-03 17:25:20]    INFO >> epoch 010:  87268 / 284748 loss=0.005, ups=43.23, bsz=64, num_updates=2.65e+06, lr=0.000168022, gnorm=0.02, clip=0, train_wall=209, wall=61610 (progress_bar.py:262, log())
[2021-03-03 17:29:11]    INFO >> epoch 010:  97268 / 284748 loss=0.004981, ups=43.19, bsz=64, num_updates=2.66e+06, lr=0.000158609, gnorm=0.02, clip=0, train_wall=209, wall=61842 (progress_bar.py:262, log())
[2021-03-03 17:33:01]    INFO >> epoch 010:  107268 / 284748 loss=0.004916, ups=43.43, bsz=64, num_updates=2.67e+06, lr=0.000148531, gnorm=0.02, clip=0, train_wall=208, wall=62072 (progress_bar.py:262, log())
[2021-03-03 17:36:56]    INFO >> epoch 010:  117268 / 284748 loss=0.004841, ups=42.7, bsz=64, num_updates=2.68e+06, lr=0.000137913, gnorm=0.02, clip=0, train_wall=211, wall=62306 (progress_bar.py:262, log())
[2021-03-03 17:41:03]    INFO >> epoch 010:  127268 / 284748 loss=0.004767, ups=40.48, bsz=64, num_updates=2.69e+06, lr=0.000126881, gnorm=0.02, clip=0, train_wall=223, wall=62553 (progress_bar.py:262, log())
[2021-03-03 17:45:11]    INFO >> epoch 010:  137268 / 284748 loss=0.004656, ups=40.32, bsz=64, num_updates=2.7e+06, lr=0.000115572, gnorm=0.02, clip=0, train_wall=224, wall=62801 (progress_bar.py:262, log())
[2021-03-03 17:49:13]    INFO >> epoch 010:  147268 / 284748 loss=0.004391, ups=41.31, bsz=64, num_updates=2.71e+06, lr=0.000104122, gnorm=0.019, clip=0, train_wall=218, wall=63044 (progress_bar.py:262, log())
[2021-03-03 17:53:06]    INFO >> epoch 010:  157268 / 284748 loss=0.004375, ups=42.93, bsz=64, num_updates=2.72e+06, lr=9.267e-05, gnorm=0.019, clip=0, train_wall=210, wall=63276 (progress_bar.py:262, log())
[2021-03-03 17:56:58]    INFO >> epoch 010:  167268 / 284748 loss=0.004332, ups=43.01, bsz=64, num_updates=2.73e+06, lr=8.13561e-05, gnorm=0.019, clip=0, train_wall=210, wall=63509 (progress_bar.py:262, log())
[2021-03-03 18:00:48]    INFO >> epoch 010:  177268 / 284748 loss=0.004263, ups=43.59, bsz=64, num_updates=2.74e+06, lr=7.03175e-05, gnorm=0.019, clip=0, train_wall=207, wall=63738 (progress_bar.py:262, log())
[2021-03-03 18:04:37]    INFO >> epoch 010:  187268 / 284748 loss=0.004128, ups=43.61, bsz=64, num_updates=2.75e+06, lr=5.96886e-05, gnorm=0.019, clip=0, train_wall=207, wall=63968 (progress_bar.py:262, log())
[2021-03-03 18:08:27]    INFO >> epoch 010:  197268 / 284748 loss=0.00404, ups=43.46, bsz=64, num_updates=2.76e+06, lr=4.95984e-05, gnorm=0.018, clip=0, train_wall=208, wall=64198 (progress_bar.py:262, log())
[2021-03-03 18:12:16]    INFO >> epoch 010:  207268 / 284748 loss=0.003919, ups=43.75, bsz=64, num_updates=2.77e+06, lr=4.01698e-05, gnorm=0.018, clip=0, train_wall=206, wall=64426 (progress_bar.py:262, log())
[2021-03-03 18:16:05]    INFO >> epoch 010:  217268 / 284748 loss=0.003865, ups=43.56, bsz=64, num_updates=2.78e+06, lr=3.15174e-05, gnorm=0.018, clip=0, train_wall=207, wall=64656 (progress_bar.py:262, log())
[2021-03-03 18:19:54]    INFO >> epoch 010:  227268 / 284748 loss=0.003886, ups=43.66, bsz=64, num_updates=2.79e+06, lr=2.37464e-05, gnorm=0.018, clip=0, train_wall=207, wall=64885 (progress_bar.py:262, log())
[2021-03-03 18:23:44]    INFO >> epoch 010:  237268 / 284748 loss=0.003798, ups=43.58, bsz=64, num_updates=2.8e+06, lr=1.69513e-05, gnorm=0.018, clip=0, train_wall=207, wall=65114 (progress_bar.py:262, log())
[2021-03-03 18:27:34]    INFO >> epoch 010:  247268 / 284748 loss=0.003754, ups=43.52, bsz=64, num_updates=2.81e+06, lr=1.12147e-05, gnorm=0.018, clip=0, train_wall=207, wall=65344 (progress_bar.py:262, log())
[2021-03-03 18:31:24]    INFO >> epoch 010:  257268 / 284748 loss=0.003867, ups=43.46, bsz=64, num_updates=2.82e+06, lr=6.60635e-06, gnorm=0.018, clip=0, train_wall=208, wall=65574 (progress_bar.py:262, log())
[2021-03-03 18:35:13]    INFO >> epoch 010:  267268 / 284748 loss=0.003796, ups=43.61, bsz=64, num_updates=2.83e+06, lr=3.18234e-06, gnorm=0.018, clip=0, train_wall=207, wall=65804 (progress_bar.py:262, log())
[2021-03-03 18:39:02]    INFO >> epoch 010:  277268 / 284748 loss=0.003724, ups=43.64, bsz=64, num_updates=2.84e+06, lr=9.84294e-07, gnorm=0.018, clip=0, train_wall=207, wall=66033 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 18:41:53]    INFO >> epoch 010 | loss 0.004503 | nll_loss inf | ppl inf | ups 42.66 | bsz 64 | num_updates 2.84748e+06 | lr 1.58202e-07 | gnorm 0.019 | clip 0 | train_wall 5990 | wall 66204 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 18:42:12]    INFO >> epoch 010 | valid on 'valid' subset | acc 0.8584 | mrr 0.8584 | map 0.8584 | ndcg 0.8584 (progress_bar.py:269, print())
[2021-03-03 18:42:14]    INFO >> saved checkpoint /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint10.pt (epoch 10 @ 2847480 updates, score None) (writing took 1.823071 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-03-03 18:43:25]    INFO >> epoch 011:   2520 / 284748 loss=0.003749, ups=38.05, bsz=64, num_updates=2.85e+06, lr=3.89276e-08, gnorm=0.018, clip=0, train_wall=207, wall=66296 (progress_bar.py:262, log())
[2021-03-03 18:47:15]    INFO >> epoch 011:  12520 / 284748 loss=0.004359, ups=43.55, bsz=64, num_updates=2.86e+06, lr=0.000207642, gnorm=0.02, clip=0, train_wall=207, wall=66525 (progress_bar.py:262, log())
[2021-03-03 18:51:04]    INFO >> epoch 011:  22520 / 284748 loss=0.004883, ups=43.66, bsz=64, num_updates=2.87e+06, lr=0.000206063, gnorm=0.021, clip=0, train_wall=207, wall=66754 (progress_bar.py:262, log())
[2021-03-03 18:54:54]    INFO >> epoch 011:  32520 / 284748 loss=0.005009, ups=43.48, bsz=64, num_updates=2.88e+06, lr=0.000203243, gnorm=0.022, clip=0, train_wall=208, wall=66984 (progress_bar.py:262, log())
[2021-03-03 18:58:43]    INFO >> epoch 011:  42520 / 284748 loss=0.005028, ups=43.51, bsz=64, num_updates=2.89e+06, lr=0.000199216, gnorm=0.021, clip=0, train_wall=208, wall=67214 (progress_bar.py:262, log())
[2021-03-03 19:02:32]    INFO >> epoch 011:  52520 / 284748 loss=0.004977, ups=43.66, bsz=64, num_updates=2.9e+06, lr=0.000194031, gnorm=0.021, clip=0, train_wall=207, wall=67443 (progress_bar.py:262, log())
[2021-03-03 19:06:22]    INFO >> epoch 011:  62520 / 284748 loss=0.004883, ups=43.49, bsz=64, num_updates=2.91e+06, lr=0.000187751, gnorm=0.021, clip=0, train_wall=208, wall=67673 (progress_bar.py:262, log())
[2021-03-03 19:10:12]    INFO >> epoch 011:  72520 / 284748 loss=0.004862, ups=43.63, bsz=64, num_updates=2.92e+06, lr=0.000180453, gnorm=0.021, clip=0, train_wall=207, wall=67902 (progress_bar.py:262, log())
[2021-03-03 19:14:01]    INFO >> epoch 011:  82520 / 284748 loss=0.004832, ups=43.6, bsz=64, num_updates=2.93e+06, lr=0.000172226, gnorm=0.02, clip=0, train_wall=207, wall=68132 (progress_bar.py:262, log())
[2021-03-03 19:17:50]    INFO >> epoch 011:  92520 / 284748 loss=0.004867, ups=43.57, bsz=64, num_updates=2.94e+06, lr=0.000163168, gnorm=0.02, clip=0, train_wall=207, wall=68361 (progress_bar.py:262, log())
[2021-03-03 19:21:40]    INFO >> epoch 011:  102520 / 284748 loss=0.004699, ups=43.58, bsz=64, num_updates=2.95e+06, lr=0.000153391, gnorm=0.02, clip=0, train_wall=207, wall=68591 (progress_bar.py:262, log())
[2021-03-03 19:25:29]    INFO >> epoch 011:  112520 / 284748 loss=0.004521, ups=43.57, bsz=64, num_updates=2.96e+06, lr=0.000143014, gnorm=0.02, clip=0, train_wall=207, wall=68820 (progress_bar.py:262, log())
[2021-03-03 19:29:19]    INFO >> epoch 011:  122520 / 284748 loss=0.004554, ups=43.62, bsz=64, num_updates=2.97e+06, lr=0.000132162, gnorm=0.02, clip=0, train_wall=207, wall=69049 (progress_bar.py:262, log())
[2021-03-03 19:33:08]    INFO >> epoch 011:  132520 / 284748 loss=0.004417, ups=43.63, bsz=64, num_updates=2.98e+06, lr=0.000120968, gnorm=0.019, clip=0, train_wall=207, wall=69279 (progress_bar.py:262, log())
[2021-03-03 19:36:58]    INFO >> epoch 011:  142520 / 284748 loss=0.004305, ups=43.53, bsz=64, num_updates=2.99e+06, lr=0.000109567, gnorm=0.019, clip=0, train_wall=207, wall=69508 (progress_bar.py:262, log())
[2021-03-03 19:40:47]    INFO >> epoch 011:  152520 / 284748 loss=0.004191, ups=43.55, bsz=64, num_updates=3e+06, lr=9.80985e-05, gnorm=0.019, clip=0, train_wall=207, wall=69738 (progress_bar.py:262, log())
[2021-03-03 19:44:37]    INFO >> epoch 011:  162520 / 284748 loss=0.004136, ups=43.55, bsz=64, num_updates=3.01e+06, lr=8.67019e-05, gnorm=0.019, clip=0, train_wall=207, wall=69968 (progress_bar.py:262, log())
[2021-03-03 19:48:27]    INFO >> epoch 011:  172520 / 284748 loss=0.004004, ups=43.49, bsz=64, num_updates=3.02e+06, lr=7.55157e-05, gnorm=0.019, clip=0, train_wall=208, wall=70198 (progress_bar.py:262, log())
[2021-03-03 19:52:16]    INFO >> epoch 011:  182520 / 284748 loss=0.003947, ups=43.66, bsz=64, num_updates=3.03e+06, lr=6.46758e-05, gnorm=0.018, clip=0, train_wall=207, wall=70427 (progress_bar.py:262, log())
[2021-03-03 19:56:05]    INFO >> epoch 011:  192520 / 284748 loss=0.003951, ups=43.55, bsz=64, num_updates=3.04e+06, lr=5.43141e-05, gnorm=0.019, clip=0, train_wall=207, wall=70656 (progress_bar.py:262, log())
[2021-03-03 19:59:55]    INFO >> epoch 011:  202520 / 284748 loss=0.003772, ups=43.6, bsz=64, num_updates=3.05e+06, lr=4.45566e-05, gnorm=0.018, clip=0, train_wall=207, wall=70885 (progress_bar.py:262, log())
[2021-03-03 20:03:44]    INFO >> epoch 011:  212520 / 284748 loss=0.003796, ups=43.57, bsz=64, num_updates=3.06e+06, lr=3.55219e-05, gnorm=0.018, clip=0, train_wall=207, wall=71115 (progress_bar.py:262, log())
[2021-03-03 20:07:34]    INFO >> epoch 011:  222520 / 284748 loss=0.003665, ups=43.59, bsz=64, num_updates=3.07e+06, lr=2.73199e-05, gnorm=0.018, clip=0, train_wall=207, wall=71344 (progress_bar.py:262, log())
[2021-03-03 20:11:23]    INFO >> epoch 011:  232520 / 284748 loss=0.003632, ups=43.65, bsz=64, num_updates=3.08e+06, lr=2.00504e-05, gnorm=0.018, clip=0, train_wall=207, wall=71574 (progress_bar.py:262, log())
[2021-03-03 20:15:12]    INFO >> epoch 011:  242520 / 284748 loss=0.003605, ups=43.56, bsz=64, num_updates=3.09e+06, lr=1.38017e-05, gnorm=0.018, clip=0, train_wall=207, wall=71803 (progress_bar.py:262, log())
[2021-03-03 20:19:02]    INFO >> epoch 011:  252520 / 284748 loss=0.003566, ups=43.52, bsz=64, num_updates=3.1e+06, lr=8.64982e-06, gnorm=0.018, clip=0, train_wall=207, wall=72033 (progress_bar.py:262, log())
[2021-03-03 20:22:51]    INFO >> epoch 011:  262520 / 284748 loss=0.003517, ups=43.61, bsz=64, num_updates=3.11e+06, lr=4.65741e-06, gnorm=0.018, clip=0, train_wall=207, wall=72262 (progress_bar.py:262, log())
[2021-03-03 20:26:41]    INFO >> epoch 011:  272520 / 284748 loss=0.003629, ups=43.54, bsz=64, num_updates=3.12e+06, lr=1.87302e-06, gnorm=0.018, clip=0, train_wall=207, wall=72492 (progress_bar.py:262, log())
[2021-03-03 20:30:31]    INFO >> epoch 011:  282520 / 284748 loss=0.003487, ups=43.47, bsz=64, num_updates=3.13e+06, lr=3.30512e-07, gnorm=0.017, clip=0, train_wall=208, wall=72722 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 20:31:23]    INFO >> epoch 011 | loss 0.004242 | nll_loss inf | ppl inf | ups 43.35 | bsz 64 | num_updates 3.13223e+06 | lr 1.58202e-07 | gnorm 0.019 | clip 0 | train_wall 5901 | wall 72773 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 20:31:42]    INFO >> epoch 011 | valid on 'valid' subset | acc 0.8653 | mrr 0.8653 | map 0.8653 | ndcg 0.8653 (progress_bar.py:269, print())
[2021-03-03 20:31:43]    INFO >> saved checkpoint /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint11.pt (epoch 11 @ 3132228 updates, score None) (writing took 1.823451 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-03-03 20:34:55]    INFO >> epoch 012:   7772 / 284748 loss=0.003586, ups=37.89, bsz=64, num_updates=3.14e+06, lr=0.000207951, gnorm=0.018, clip=0, train_wall=207, wall=72986 (progress_bar.py:262, log())
[2021-03-03 20:38:44]    INFO >> epoch 012:  17772 / 284748 loss=0.004453, ups=43.66, bsz=64, num_updates=3.15e+06, lr=0.000206969, gnorm=0.021, clip=0, train_wall=207, wall=73215 (progress_bar.py:262, log())
[2021-03-03 20:42:34]    INFO >> epoch 012:  27772 / 284748 loss=0.004592, ups=43.53, bsz=64, num_updates=3.16e+06, lr=0.000204735, gnorm=0.021, clip=0, train_wall=207, wall=73445 (progress_bar.py:262, log())
[2021-03-03 20:46:23]    INFO >> epoch 012:  37772 / 284748 loss=0.004682, ups=43.58, bsz=64, num_updates=3.17e+06, lr=0.000201276, gnorm=0.021, clip=0, train_wall=207, wall=73674 (progress_bar.py:262, log())
[2021-03-03 20:50:12]    INFO >> epoch 012:  47772 / 284748 loss=0.004708, ups=43.7, bsz=64, num_updates=3.18e+06, lr=0.000196634, gnorm=0.021, clip=0, train_wall=207, wall=73903 (progress_bar.py:262, log())
[2021-03-03 20:54:02]    INFO >> epoch 012:  57772 / 284748 loss=0.004714, ups=43.51, bsz=64, num_updates=3.19e+06, lr=0.000190865, gnorm=0.021, clip=0, train_wall=207, wall=74133 (progress_bar.py:262, log())
[2021-03-03 20:58:11]    INFO >> epoch 012:  67772 / 284748 loss=0.004596, ups=40.2, bsz=64, num_updates=3.2e+06, lr=0.00018404, gnorm=0.021, clip=0, train_wall=223, wall=74382 (progress_bar.py:262, log())
[2021-03-03 21:02:51]    INFO >> epoch 012:  77772 / 284748 loss=0.004593, ups=35.66, bsz=64, num_updates=3.21e+06, lr=0.000176242, gnorm=0.021, clip=0, train_wall=250, wall=74662 (progress_bar.py:262, log())
[2021-03-03 21:07:28]    INFO >> epoch 012:  87772 / 284748 loss=0.004321, ups=36.16, bsz=64, num_updates=3.22e+06, lr=0.000167565, gnorm=0.02, clip=0, train_wall=246, wall=74939 (progress_bar.py:262, log())
[2021-03-03 21:11:37]    INFO >> epoch 012:  97772 / 284748 loss=0.004434, ups=40.19, bsz=64, num_updates=3.23e+06, lr=0.000158116, gnorm=0.02, clip=0, train_wall=223, wall=75187 (progress_bar.py:262, log())
[2021-03-03 21:17:13]    INFO >> epoch 012:  107772 / 284748 loss=0.004399, ups=29.76, bsz=64, num_updates=3.24e+06, lr=0.000148008, gnorm=0.02, clip=0, train_wall=301, wall=75523 (progress_bar.py:262, log())
[2021-03-03 21:23:47]    INFO >> epoch 012:  117772 / 284748 loss=0.004211, ups=25.35, bsz=64, num_updates=3.25e+06, lr=0.000137365, gnorm=0.019, clip=0, train_wall=353, wall=75918 (progress_bar.py:262, log())
[2021-03-03 21:30:23]    INFO >> epoch 012:  127772 / 284748 loss=0.004104, ups=25.28, bsz=64, num_updates=3.26e+06, lr=0.000126317, gnorm=0.019, clip=0, train_wall=354, wall=76313 (progress_bar.py:262, log())
[2021-03-03 21:36:57]    INFO >> epoch 012:  137772 / 284748 loss=0.003936, ups=25.37, bsz=64, num_updates=3.27e+06, lr=0.000114997, gnorm=0.019, clip=0, train_wall=352, wall=76708 (progress_bar.py:262, log())
[2021-03-03 21:43:30]    INFO >> epoch 012:  147772 / 284748 loss=0.00395, ups=25.41, bsz=64, num_updates=3.28e+06, lr=0.000103543, gnorm=0.019, clip=0, train_wall=352, wall=77101 (progress_bar.py:262, log())
[2021-03-03 21:50:03]    INFO >> epoch 012:  157772 / 284748 loss=0.003812, ups=25.47, bsz=64, num_updates=3.29e+06, lr=9.20953e-05, gnorm=0.018, clip=0, train_wall=351, wall=77494 (progress_bar.py:262, log())
[2021-03-03 21:56:36]    INFO >> epoch 012:  167772 / 284748 loss=0.003679, ups=25.45, bsz=64, num_updates=3.3e+06, lr=8.0792e-05, gnorm=0.018, clip=0, train_wall=351, wall=77887 (progress_bar.py:262, log())
[2021-03-03 22:03:10]    INFO >> epoch 012:  177772 / 284748 loss=0.003572, ups=25.37, bsz=64, num_updates=3.31e+06, lr=6.97709e-05, gnorm=0.018, clip=0, train_wall=352, wall=78281 (progress_bar.py:262, log())
[2021-03-03 22:09:45]    INFO >> epoch 012:  187772 / 284748 loss=0.003621, ups=25.35, bsz=64, num_updates=3.32e+06, lr=5.91661e-05, gnorm=0.018, clip=0, train_wall=353, wall=78675 (progress_bar.py:262, log())
[2021-03-03 22:16:19]    INFO >> epoch 012:  197772 / 284748 loss=0.003411, ups=25.38, bsz=64, num_updates=3.33e+06, lr=4.91064e-05, gnorm=0.018, clip=0, train_wall=352, wall=79069 (progress_bar.py:262, log())
[2021-03-03 22:22:50]    INFO >> epoch 012:  207772 / 284748 loss=0.003475, ups=25.52, bsz=64, num_updates=3.34e+06, lr=3.97143e-05, gnorm=0.018, clip=0, train_wall=350, wall=79461 (progress_bar.py:262, log())
[2021-03-03 22:29:18]    INFO >> epoch 012:  217772 / 284748 loss=0.003342, ups=25.81, bsz=64, num_updates=3.35e+06, lr=3.11038e-05, gnorm=0.017, clip=0, train_wall=346, wall=79849 (progress_bar.py:262, log())
[2021-03-03 22:35:46]    INFO >> epoch 012:  227772 / 284748 loss=0.003356, ups=25.76, bsz=64, num_updates=3.36e+06, lr=2.33798e-05, gnorm=0.017, clip=0, train_wall=347, wall=80237 (progress_bar.py:262, log())
[2021-03-03 22:42:13]    INFO >> epoch 012:  237772 / 284748 loss=0.00329, ups=25.86, bsz=64, num_updates=3.37e+06, lr=1.66362e-05, gnorm=0.017, clip=0, train_wall=346, wall=80624 (progress_bar.py:262, log())
[2021-03-03 22:48:39]    INFO >> epoch 012:  247772 / 284748 loss=0.003304, ups=25.89, bsz=64, num_updates=3.38e+06, lr=1.09549e-05, gnorm=0.017, clip=0, train_wall=345, wall=81010 (progress_bar.py:262, log())
[2021-03-03 22:55:03]    INFO >> epoch 012:  257772 / 284748 loss=0.003137, ups=26.02, bsz=64, num_updates=3.39e+06, lr=6.40503e-06, gnorm=0.017, clip=0, train_wall=343, wall=81394 (progress_bar.py:262, log())
[2021-03-03 23:01:32]    INFO >> epoch 012:  267772 / 284748 loss=0.003185, ups=25.76, bsz=64, num_updates=3.4e+06, lr=3.04194e-06, gnorm=0.017, clip=0, train_wall=347, wall=81782 (progress_bar.py:262, log())
[2021-03-03 23:08:08]    INFO >> epoch 012:  277772 / 284748 loss=0.003174, ups=25.23, bsz=64, num_updates=3.41e+06, lr=9.06512e-07, gnorm=0.017, clip=0, train_wall=354, wall=82179 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 23:12:44]    INFO >> epoch 012 | loss 0.0039 | nll_loss inf | ppl inf | ups 29.41 | bsz 64 | num_updates 3.41698e+06 | lr 1.58202e-07 | gnorm 0.019 | clip 0 | train_wall 8634 | wall 82455 (progress_bar.py:269, print())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
[2021-03-03 23:13:09]    INFO >> epoch 012 | valid on 'valid' subset | acc 0.8787 | mrr 0.8787 | map 0.8787 | ndcg 0.8787 (progress_bar.py:269, print())
[2021-03-03 23:13:11]    INFO >> saved checkpoint /data/yanghe/.ncc/deepcs/retrieval/data-mmap/deepcs/checkpoints/checkpoint12.pt (epoch 12 @ 3416976 updates, score None) (writing took 1.875973 seconds) (checkpoint_utils.py:81, save_checkpoint())
[2021-03-03 23:15:33]    INFO >> epoch 013:   3024 / 284748 loss=0.003129, ups=22.45, bsz=64, num_updates=3.42e+06, lr=2.47137e-08, gnorm=0.017, clip=0, train_wall=354, wall=82624 (progress_bar.py:262, log())
Using backend: pytorch
Using backend: pytorch
Using backend: pytorch
