preprocess:
  no_progress_bar: 0 # action='store_true', help='disable progress bar'
  log_interval: 100
  log_format: ~ # ', default=None, help='log format to use', choices=['json', 'none', 'simple', 'tqdm']
  tensorboard_logdir: '' # metavar='DIR', default='', help='path to save logs for tensorboard, should match --logdir of running tensorboard (default: no tensorboard logging)'
  seed: 1 # default=1, type=int, metavar='N', help='pseudo random number generator seed'
  cpu: 0 # ', action='store_true', help='use CPU instead of CUDA'
  fp16: 0 # ', action='store_true', help='use FP16'
  memory_efficient_fp16: 0 # ', action='store_true', help='use a memory-efficient version of FP16 training; implies --fp16'
  fp16_no_flatten_grads: 0 #', action='store_true', help='don\'t flatten FP16 grads tensor'
  fp16_init_scale: 128 #', default=2 ** 7, type=int, help='default FP16 loss scale'
  fp16_scale_window: ~ #', type=int, help='number of updates before increasing loss scale'
  fp16_scale_tolerance: 0.0 #', default=0.0, type=float, help='pct of updates that can overflow before decreasing the loss scale'
  min_loss_scale: 1e-4 #', default=1e-4, type=float, metavar='D', help='minimum FP16 loss scale, after which training is stopped'
  threshold_loss_scale: ~ #', type=float, help='threshold FP16 loss scale from below'
  user_dir: ~ #, default=None, help='path to a python module containing custom extensions (tasks and/or architectures)'
  empty_cache_freq: 0 #', default=0, type=int, help='how often to clear the PyTorch CUDA cache (0 to disable)'
  all_gather_list_size: 16384 #', default=16384, type=int, help='number of bytes reserved for gathering stats from workers'

  task: summarization # task', metavar='TASK', default="translation", choices=TASK_REGISTRY.keys(), help='task'
  source_lang: code
  target_lang: ~ # docstring #", default=None, metavar="TARGET", help="target language"
  # ~ path for dataset default. e.g.  for code_search_net, ~/flatten = ~/.ncc/code_search_net/flatten
  trainpref: ~/.ncc/code_search_net/flatten/ruby/train #", metavar="FP", default=None, help="train file prefix"
  validpref: ~/.ncc/code_search_net/flatten/ruby/valid #", metavar="FP", default=None, help="comma separated, valid file prefixes"
  testpref:  ~/.ncc/code_search_net/flatten/ruby/test #", metavar="FP", default=None, help="comma separated, test file prefixes"
#  trainpref: ~/.ncc/CodeSearchNet/codebert/data-raw/ruby/docstring_path #", metavar="FP", default=None, help="train file prefix"
#  validpref: ~/.ncc/CodeSearchNet/codebert/data-raw/ruby/docstring_path #", metavar="FP", default=None, help="comma separated, valid file prefixes"
#  testpref:  ~/.ncc/CodeSearchNet/codebert/data-raw/ruby/docstring_path #", metavar="FP", default=None, help="comma separated, test file prefixes"

  align_suffix: ~ # ", metavar="FP", default=None, help="alignment file suffix"
#  destdir: ~/.ncc/CodeSearchNet/codebert/data-mmap/ruby/docstring_path/ #", metavar="DIR", default="data-bin", help="destination dir"
  destdir: ~/.ncc/code_search_net/codebert/data-raw/ruby/code #", metavar="DIR", default="data-bin", help="destination dir"
  thresholdtgt: 0 #", metavar="N", default=0, type=int, help="map words appearing less than threshold times to unknown"
  thresholdsrc: 0 #", metavar="N", default=0, type=int, help="map words appearing less than threshold times to unknown"
  srcdict: ~/.ncc/code_search_net/codebert/data-raw/ruby/codesearchnet_ruby.dict.txt
  tgtdict: ~ # ", metavar="FP", help="reuse given target dictionary"
  src_sp: ~/.ncc/code_search_net/codebert/data-raw/ruby/codesearchnet_ruby.model
  tgt_sp: ~
  nwordstgt: -1 #", metavar="N", default=-1, type=int, help="number of target words to retain"
  nwordssrc: -1 #", metavar="N", default=-1, type=int, help="number of source words to retain"
  alignfile: ~ #", metavar="ALIGN", default=None, help="an alignment file (optional)"
  dataset_impl: raw #', metavar='FORMAT', default='mmap', choices=get_available_dataset_impl(), help='output dataset implementation'
  joined_dictionary: 1 # ", action="store_true", help="Generate joined dictionary"
  only_source: 1 # ", action="store_true", help="Only process the source language"
  padding_factor: 8 #", metavar="N", default=8, type=int, help="Pad dictionary size to be multiple of N"
  workers: 10 # ", metavar="N", default=1, type=int, help="number of parallel workers"
  inserted: 0